{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8bc990-ebc5-46bd-bab4-f4091ce5b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../script/\")\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import importlib\n",
    "from time import time\n",
    "import Functions\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8230662c-fa72-42c7-bf5e-919cbf89080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from Node import Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a876dc",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cce456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    def __init__(self,opset,X,Y,log_odds,p,learning_rate):\n",
    "        self.generation = 0\n",
    "#         self.X_train,self.X_valid,self.y_train,self.y_valid = train_test_split(X,Y,train_size=0.9)\n",
    "        X = X.astype('float64')\n",
    "        self.opset = opset\n",
    "        \n",
    "        self.num_class = len(pd.unique(Y))\n",
    "        self.feature_space = X.shape[1]\n",
    "        \n",
    "        self.vals = X.T\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.log_odds = log_odds\n",
    "        self.p = p\n",
    "        self.residual = self.Y - p\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.count_label = pd.value_counts(Y).reset_index().values\n",
    "        \n",
    "        self.best = (np.inf,None)\n",
    "        self.nodes = [Node(True,index=i) for i in range(self.feature_space)]\n",
    "    \n",
    "    def loss(self,vals,bins,beta):\n",
    "        fitness = []\n",
    "        \n",
    "        for val in vals:\n",
    "            val_max = np.max(val)\n",
    "            val_min = np.min(val)\n",
    "            width = ((val_max - val_min)/bins)\n",
    "\n",
    "            index = ((val - val_min)//width).astype('int32') if width != 0 else np.zeros(val.shape[0])\n",
    "            index = np.where(index >= bins,bins-1,index)\n",
    "            index = np.where(index < 0,0,index)\n",
    "\n",
    "            p_bin = [sum(self.p[index==i]*(1-self.p[index==i]))  for i in range(bins)]\n",
    "            residual_bin = [sum(self.residual[index==i])  for i in range(bins)]\n",
    "\n",
    "            grad_bin = [residual_bin[i]/p_bin[i] if p_bin[i] > 0 else 0 for i in range(bins)]\n",
    "\n",
    "            grads = np.zeros(index.shape[0])\n",
    "            for i in range(bins):\n",
    "                grads[index==i] = grad_bin[i]\n",
    "\n",
    "            log_odds_1 = self.log_odds + self.learning_rate * grads\n",
    "            p_1 = np.exp(log_odds_1)\n",
    "            p_1 = p_1/(1+p_1)\n",
    "\n",
    "            fitness.append(sum((self.Y-p_1)**2))\n",
    "\n",
    "        return fitness\n",
    "    \n",
    "    def dloss(self,vals):\n",
    "        return [0 for i in range(len(vals))]\n",
    "    \n",
    "    def evolve(self,total_size,batch_size,elite_size,bins,beta,verbose):\n",
    "        self.generation += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\tgeneration:\",self.generation)\n",
    "            t = time()\n",
    "        \n",
    "        num_batches = total_size//batch_size\n",
    "        pool = self.nodes\n",
    "\n",
    "        elites_funcs = []\n",
    "        elite_sons = []\n",
    "        elite_vals = []\n",
    "\n",
    "        elites_fitness = []\n",
    "        for j in range(num_batches):\n",
    "\n",
    "            funcs = np.random.choice(list(self.opset.keys()),size=batch_size)\n",
    "            arg_count = [self.opset[func] for func in funcs]\n",
    "            sons = np.random.choice(pool,size = sum(arg_count))\n",
    "            it = iter(sons)\n",
    "            sons = [[next(it) for _ in range(arg_count[i])] for i in range(batch_size)]\n",
    "            vals = [funcs[i]([self.vals[s.index] for s in sons[i]]) for i in range(batch_size)]\n",
    "\n",
    "            vals = np.stack(vals)\n",
    "            # fitness = self.loss(vals,bins,beta)\n",
    "            fitness = self.dloss(vals)\n",
    "            \n",
    "            elites_funcs.extend(funcs)\n",
    "            elite_sons.extend(sons)\n",
    "            elite_vals.extend(vals)\n",
    "            elites_fitness.extend(fitness)\n",
    "\n",
    "            rank = np.argsort(elites_fitness)\n",
    "\n",
    "            elites_funcs = [elites_funcs[index] for index in rank[:elite_size]]\n",
    "            elite_sons = [elite_sons[index] for index in rank[:elite_size]]\n",
    "            elite_vals = [elite_vals[index] for index in rank[:elite_size]]\n",
    "            elites_fitness = [elites_fitness[index] for index in rank[:elite_size]]\n",
    "\n",
    "        for index in range(elite_size):\n",
    "            node = Node(False,\n",
    "                func=elites_funcs[index],\n",
    "                sons=elite_sons[index],\n",
    "                index=len(self.nodes),\n",
    "                fit=elites_fitness[index] \n",
    "            )\n",
    "            \n",
    "            self.nodes.append(node)\n",
    "            self.vals = np.append(self.vals,[elite_vals[index]],axis=0)\n",
    "            \n",
    "            if index == 0:\n",
    "                if self.best[0] > node.fitness:\n",
    "                    val = elite_vals[index]\n",
    "                    val_max = np.max(val)\n",
    "                    val_min = np.min(val)\n",
    "                    width = ((val_max - val_min)/bins)\n",
    "                                      \n",
    "                    self.best = (node.fitness,node,(val_max,val_min,width,bins))\n",
    "            # self.test_param_same(node)\n",
    "      \n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\t\",np.min(elites_fitness))\n",
    "            print(\"\\ttime\",time()-t)\n",
    "        return None\n",
    "\n",
    "    def test_param_same(self,node):\n",
    "        v1 = node.predict(self.X)\n",
    "        v2 = self.vals[node.index]\n",
    "        if np.any(v1!=v2):\n",
    "            print(node.index,v1==v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "008aa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref from 2segp github\n",
    "# Classification dataset names - choose from following datasets \n",
    "\n",
    "CLASS_DATASET_NAMES = ['bcw','heart','iono','parks','sonar']\n",
    "dataset_name = CLASS_DATASET_NAMES[1]\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "Xy = np.genfromtxt('test_data/'+dataset_name+'.csv', delimiter=',')\n",
    "X = Xy[:, :-1]\n",
    "y = Xy[:, -1]   # last column is the label\n",
    "\n",
    "# simple operators\n",
    "\n",
    "boost_num = 1000\n",
    "\n",
    "seed = np.random.randint(9999999)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "init_log_odds = sum(y==1)/y_train.shape[0]    \n",
    "init_p = np.exp(init_log_odds)\n",
    "init_p = init_p/(1+init_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "a98e0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sleep.tsv',delimiter='\\t')\n",
    "X = df.iloc[:,:-1].to_numpy()\n",
    "y = df.iloc[:,-1].to_numpy()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "5385e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "num_feature = 1500\n",
    "\n",
    "eg = Engine(Functions.simple_opset,X_train,y_train,init_log_odds,init_p,0)\n",
    "for i in range(3):\n",
    "    eg.evolve(500,500,500,2,[0,0,0],0)\n",
    "nodes = eg.nodes[60:60+num_feature]\n",
    "# nodes = eg.nodes[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "6431c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     14939\n",
      "           1       1.00      1.00      1.00      6338\n",
      "           2       1.00      1.00      1.00     36884\n",
      "           3       1.00      1.00      1.00      7567\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00      8407\n",
      "\n",
      "    accuracy                           0.89     74135\n",
      "   macro avg       0.67      0.67      0.67     74135\n",
      "weighted avg       0.89      0.89      0.89     74135\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      6420\n",
      "           1       0.22      0.24      0.23      2714\n",
      "           2       0.75      0.74      0.75     15814\n",
      "           3       0.65      0.67      0.66      3265\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00      3560\n",
      "\n",
      "    accuracy                           0.59     31773\n",
      "   macro avg       0.38      0.38      0.38     31773\n",
      "weighted avg       0.60      0.59      0.59     31773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhou/miniconda3/envs/tensor_gp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "num_estimator = 1\n",
    "max_depth = 100\n",
    "ratio = 1\n",
    "\n",
    "vals = np.stack([n.predict(X_train) for n in nodes]).T\n",
    "clfs = [DecisionTreeClassifier(max_depth=max_depth) for i in range(num_estimator)]\n",
    "all_index = [i for i in range(len(nodes))]\n",
    "indexes = [np.random.choice(all_index,size=int(len(all_index)*ratio),replace=False) for i in range(num_estimator)]\n",
    "\n",
    "for i in range(num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    clfs[i].fit(feature_batch,y_train)\n",
    "\n",
    "vals = np.stack([n.predict(X_train) for n in nodes]).T\n",
    "\n",
    "feature_index = indexes[0]\n",
    "feature_batch = vals[:,feature_index]\n",
    "prob = clfs[0].predict_proba(feature_batch)\n",
    "\n",
    "for i in range(1,num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    p = clfs[i].predict_proba(feature_batch)\n",
    "    prob += p\n",
    "    \n",
    "pred = np.argmax(prob,axis=1)\n",
    "print(classification_report(y_train,pred))\n",
    "\n",
    "\n",
    "vals = np.stack([n.predict(X_test) for n in nodes]).T\n",
    "\n",
    "feature_index = indexes[0]\n",
    "feature_batch = vals[:,feature_index]\n",
    "prob = clfs[0].predict_proba(feature_batch)\n",
    "\n",
    "for i in range(1,num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    p = clfs[i].predict_proba(feature_batch)\n",
    "    prob += p\n",
    "    \n",
    "pred = np.argmax(prob,axis=1)\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "3826738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        63\n",
      "         1.0       1.00      1.00      1.00        82\n",
      "\n",
      "    accuracy                           1.00       145\n",
      "   macro avg       1.00      1.00      1.00       145\n",
      "weighted avg       1.00      1.00      1.00       145\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.91      0.90        34\n",
      "         1.0       0.89      0.86      0.88        29\n",
      "\n",
      "    accuracy                           0.89        63\n",
      "   macro avg       0.89      0.89      0.89        63\n",
      "weighted avg       0.89      0.89      0.89        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "num_feature = 1000\n",
    "num_estimator = 100\n",
    "max_depth = 40\n",
    "ratio = 0.1\n",
    "\n",
    "eg = Engine(Functions.simple_opset,X_train,y_train,0,0,0)\n",
    "for i in range(3):\n",
    "    eg.evolve(1000,1000,1000,2,[0,0,0],0)\n",
    "nodes = eg.nodes[60:60+num_feature]\n",
    "# nodes = eg.nodes[0:60]\n",
    "vals = np.stack([n.predict(X_train) for n in nodes]).T\n",
    "\n",
    "clfs = [DecisionTreeClassifier(max_depth=max_depth) for i in range(num_estimator)]\n",
    "all_index = [i for i in range(len(nodes))]\n",
    "indexes = [np.random.choice(all_index,size=int(len(all_index)*ratio),replace=False) for i in range(num_estimator)]\n",
    "\n",
    "for i in range(num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    clfs[i].fit(feature_batch,y_train)\n",
    "    \n",
    "vals = np.stack([n.predict(X_train) for n in nodes]).T\n",
    "\n",
    "feature_index = indexes[0]\n",
    "feature_batch = vals[:,feature_index]\n",
    "prob = clfs[0].predict_proba(feature_batch)\n",
    "\n",
    "for i in range(1,num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    p = clfs[i].predict_proba(feature_batch)\n",
    "    prob += p\n",
    "    \n",
    "pred = np.argmax(prob,axis=1)\n",
    "print(classification_report(y_train,pred))\n",
    "\n",
    "\n",
    "vals = np.stack([n.predict(X_test) for n in nodes]).T\n",
    "\n",
    "feature_index = indexes[0]\n",
    "feature_batch = vals[:,feature_index]\n",
    "prob = clfs[0].predict_proba(feature_batch)\n",
    "\n",
    "for i in range(1,num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    p = clfs[i].predict_proba(feature_batch)\n",
    "    prob += p\n",
    "    \n",
    "pred = np.argmax(prob,axis=1)\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "cece6967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        75\n",
      "         1.0       1.00      1.00      1.00        70\n",
      "\n",
      "    accuracy                           1.00       145\n",
      "   macro avg       1.00      1.00      1.00       145\n",
      "weighted avg       1.00      1.00      1.00       145\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.91      0.85        22\n",
      "         1.0       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.89        63\n",
      "   macro avg       0.87      0.89      0.88        63\n",
      "weighted avg       0.90      0.89      0.89        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "num_feature = 3000\n",
    "num_estimator = 1000\n",
    "max_depth = 5\n",
    "ratio = 0.05\n",
    "\n",
    "eg = Engine(Functions.simple_opset,X_train,y_train,0,0,0)\n",
    "for i in range(3):\n",
    "    eg.evolve(1000,1000,1000,2,[0,0,0],0)\n",
    "nodes = eg.nodes[60:60+num_feature]\n",
    "vals = np.stack([n.predict(X_train) for n in nodes]).T\n",
    "\n",
    "clfs = [DecisionTreeClassifier(max_depth=max_depth) for i in range(num_estimator)]\n",
    "all_index = [i for i in range(len(nodes))]\n",
    "indexes = [np.random.choice(all_index,size=int(len(all_index)*ratio),replace=False) for i in range(num_estimator)]\n",
    "\n",
    "for i in range(num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    clfs[i].fit(feature_batch,y_train)\n",
    "    \n",
    "vals = np.stack([n.predict(X_train) for n in nodes]).T\n",
    "\n",
    "feature_index = indexes[0]\n",
    "feature_batch = vals[:,feature_index]\n",
    "prob = clfs[0].predict_proba(feature_batch)\n",
    "\n",
    "for i in range(1,num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    p = clfs[i].predict_proba(feature_batch)\n",
    "    prob += p\n",
    "    \n",
    "pred = np.argmax(prob,axis=1)\n",
    "print(classification_report(y_train,pred))\n",
    "\n",
    "\n",
    "vals = np.stack([n.predict(X_test) for n in nodes]).T\n",
    "\n",
    "feature_index = indexes[0]\n",
    "feature_batch = vals[:,feature_index]\n",
    "prob = clfs[0].predict_proba(feature_batch)\n",
    "\n",
    "for i in range(1,num_estimator):\n",
    "    feature_index = indexes[i]\n",
    "    feature_batch = vals[:,feature_index]\n",
    "    p = clfs[i].predict_proba(feature_batch)\n",
    "    prob += p\n",
    "    \n",
    "pred = np.argmax(prob,axis=1)\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b511b",
   "metadata": {},
   "source": [
    "# GStackGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8706cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GStackGP():\n",
    "    def __init__(self,X,y,learning_rate):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.stack = []\n",
    "        self.init_log_odds = sum(y==1)/y.shape[0]    \n",
    "        self.init_p = np.exp(self.init_log_odds)\n",
    "        self.init_p = self.init_p/(1+self.init_p)\n",
    "\n",
    "        self.log_odds = np.array([self.init_log_odds for i in range(y.shape[0])])\n",
    "        self.p = np.array([self.init_p for i in range(y.shape[0])])\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def evolve(self):\n",
    "        residual = self.y - self.p\n",
    "        loss = sum(residual**2)\n",
    "        if np.isnan(loss):\n",
    "            print(\"loss is inf\")\n",
    "            return \n",
    "        else:\n",
    "            print(\"Loss:\",loss)\n",
    "        self.E = self.trainGP(self.X,self.y,self.log_odds,self.p,self.learning_rate)\n",
    "        grads,node,node_param,grad_bin = self.gradient(self.E,self.log_odds,self.p,residual)\n",
    "        self.log_odds,self.p = self.update_log_p(grads,self.log_odds,self.p,self.learning_rate)\n",
    "        \n",
    "        self.stack.append((node,node_param,grad_bin,self.learning_rate))\n",
    "        \n",
    "    def trainGP(self,X,y,log_odds,p,learning_rate):\n",
    "        E = Engine(Functions.simple_opset,X,y,log_odds,p,learning_rate=learning_rate)\n",
    "        for i in range(30):\n",
    "            E.evolve(total_size=10,batch_size=10,\n",
    "                                            elite_size=3,bins=2,beta=[0,0,0],verbose=0)\n",
    "        return E\n",
    "    \n",
    "    def gradient(self,E,log_odds,p,residual):\n",
    "        node = E.best[1]\n",
    "        val = E.vals[node.index]\n",
    "        val_max,val_min,width,bins = E.best[2]\n",
    "\n",
    "        index = ((val - val_min)//width).astype('int32') if width != 0 else np.zeros(val.shape[0])\n",
    "        index = np.where(index >= bins,bins-1,index)\n",
    "        index = np.where(index < 0,0,index)\n",
    "\n",
    "        p_bin = [sum(p[index==i]*(1-p[index==i]))  for i in range(bins)]\n",
    "        residual_bin = [sum(residual[index==i])  for i in range(bins)]\n",
    "\n",
    "        grad_bin = [residual_bin[i]/p_bin[i] if p_bin[i] > 0 else 0 for i in range(bins)]\n",
    "\n",
    "        grads = np.zeros(index.shape[0])\n",
    "        for i in range(bins):\n",
    "            grads[index==i] = grad_bin[i]\n",
    "        \n",
    "        return grads,E.best[1],E.best[2],grad_bin\n",
    "    \n",
    "    def update_log_p(self,grads,log_odds,p,learning_rate):\n",
    "        log_odds_1 = log_odds + learning_rate * grads\n",
    "        p_1 = np.exp(log_odds_1)\n",
    "        p_1 = p_1/(1+p_1)\n",
    "\n",
    "        return log_odds_1,p_1\n",
    "    \n",
    "    def predict_prob(self,X):\n",
    "        log_odds = np.array([self.init_log_odds for i in range(X.shape[0])])\n",
    "        p = np.array([self.init_p for i in range(X.shape[0])])\n",
    "\n",
    "        for param in self.stack:\n",
    "            node,node_param,grad_bin,learning_rate = param\n",
    "            \n",
    "            val = node.predict(X)\n",
    "            val_max,val_min,width,bins = node_param\n",
    "\n",
    "            index = ((val - val_min)//width).astype('int32') if width != 0 else np.zeros(val.shape[0])\n",
    "            index = np.where(index >= bins,bins-1,index)\n",
    "            index = np.where(index < 0,0,index)\n",
    "\n",
    "            grads = np.zeros(index.shape[0])\n",
    "            for i in range(bins):\n",
    "                grads[index==i] = grad_bin[i]\n",
    "            \n",
    "            log_odds,p = self.update_log_p(grads,log_odds,p,learning_rate)\n",
    "        return p\n",
    "    \n",
    "    def predict(self,X):\n",
    "        p = self.predict_prob(X)\n",
    "        return p > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54d9f7",
   "metadata": {},
   "source": [
    "# Small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "a6c3592e-5e72-41b0-bc7a-0bff336efeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref from 2segp github\n",
    "# Classification dataset names - choose from following datasets \n",
    "\n",
    "CLASS_DATASET_NAMES = ['bcw','heart','iono','parks','sonar']\n",
    "dataset_name = CLASS_DATASET_NAMES[1]\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "Xy = np.genfromtxt('test_data/'+dataset_name+'.csv', delimiter=',')\n",
    "X = Xy[:, :-1]\n",
    "y = Xy[:, -1]   # last column is the label\n",
    "\n",
    "# simple operators\n",
    "\n",
    "boost_num = 1000\n",
    "\n",
    "seed = np.random.randint(9999999)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "af290e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 51.804279795156624\n",
      "Loss: 41.49343732655127\n",
      "Loss: 35.364911677054465\n",
      "Loss: 30.608233413712934\n",
      "Loss: 26.933726238778295\n",
      "Loss: 24.418807668178303\n",
      "Loss: 22.60074364918166\n",
      "Loss: 21.355526740710058\n",
      "Loss: 20.61758691905583\n",
      "Loss: 19.761808763901715\n",
      "Loss: 19.00762181870527\n",
      "Loss: 18.3760853049526\n",
      "Loss: 17.844167910356358\n",
      "Loss: 17.31187899462187\n",
      "Loss: 16.954704484550035\n",
      "Loss: 16.462023190077847\n",
      "Loss: 16.032591268720093\n",
      "Loss: 15.347517482214052\n",
      "Loss: 15.094576009316786\n",
      "Loss: 14.449836356276966\n",
      "Loss: 14.023387669381993\n",
      "Loss: 13.797046457749367\n",
      "Loss: 13.583234024487355\n",
      "Loss: 13.210008062151442\n",
      "Loss: 12.924368433858413\n",
      "Loss: 12.631305163850666\n",
      "Loss: 12.370304102607061\n",
      "Loss: 12.102887687968417\n",
      "Loss: 11.892988757430643\n",
      "Loss: 11.758818946410488\n",
      "Loss: 11.46132422376725\n",
      "Loss: 11.185953827747927\n",
      "Loss: 10.93268582579669\n",
      "Loss: 10.573290529788636\n",
      "Loss: 10.33621023497809\n",
      "Loss: 10.053000181828377\n",
      "Loss: 9.917385453503924\n",
      "Loss: 9.764144864387012\n",
      "Loss: 9.629331159013224\n",
      "Loss: 9.39583465581837\n",
      "Loss: 9.163947871976047\n",
      "Loss: 8.94009153344425\n",
      "Loss: 8.805934139601751\n",
      "Loss: 8.595918482110077\n",
      "Loss: 8.455930448887887\n",
      "Loss: 8.354840548299133\n",
      "Loss: 8.14197992415085\n",
      "Loss: 7.99372121396032\n",
      "Loss: 7.772226728963816\n",
      "Loss: 7.615440989955393\n",
      "Loss: 7.497665054599981\n",
      "Loss: 7.345058305893927\n",
      "Loss: 7.184694469227919\n",
      "Loss: 7.029173497118122\n",
      "Loss: 6.890507064079416\n",
      "Loss: 6.7132086227687395\n",
      "Loss: 6.602310091944493\n",
      "Loss: 6.478315346225022\n",
      "Loss: 6.352853250790958\n",
      "Loss: 6.232707077745347\n",
      "Loss: 6.128463960080271\n",
      "Loss: 5.995229249480488\n",
      "Loss: 5.890220335908935\n",
      "Loss: 5.786734603439193\n",
      "Loss: 5.608751180815257\n",
      "Loss: 5.5284184497517135\n",
      "Loss: 5.469081290259705\n",
      "Loss: 5.35377606377472\n",
      "Loss: 5.264771326561846\n",
      "Loss: 5.188336468006486\n",
      "Loss: 5.089081571409428\n",
      "Loss: 5.04529658445654\n",
      "Loss: 4.8048790489581155\n",
      "Loss: 4.709004185054741\n",
      "Loss: 4.64794170463035\n",
      "Loss: 4.574200286039807\n",
      "Loss: 4.49483847534674\n",
      "Loss: 4.421755600904804\n",
      "Loss: 4.349781210764021\n",
      "Loss: 4.277319343114283\n",
      "Loss: 4.192875917354144\n",
      "Loss: 4.155902596785162\n",
      "Loss: 4.075939569929718\n",
      "Loss: 3.9850636861692155\n",
      "Loss: 3.911729838556154\n",
      "Loss: 3.8654883249409258\n",
      "Loss: 3.7912121618788834\n",
      "Loss: 3.7364759816791517\n",
      "Loss: 3.5554532477929675\n",
      "Loss: 3.446693028727241\n",
      "Loss: 3.379586083100073\n",
      "Loss: 3.3277412073634145\n",
      "Loss: 3.2997971230193355\n",
      "Loss: 3.2515031658104587\n",
      "Loss: 3.2083687282815156\n",
      "Loss: 3.130976589984514\n",
      "Loss: 3.0828870221117826\n",
      "Loss: 3.045166013720707\n",
      "Loss: 2.986451279439925\n",
      "Loss: 2.953202763267875\n",
      "Loss: 2.8874847552407106\n",
      "Loss: 2.847976231298302\n",
      "Loss: 2.7874602628258187\n",
      "Loss: 2.734518472288051\n",
      "Loss: 2.7112346155506164\n",
      "Loss: 2.635190173854829\n",
      "Loss: 2.5942935386947377\n",
      "Loss: 2.5580122810440136\n",
      "Loss: 2.5110011603854083\n",
      "Loss: 2.479637534074779\n",
      "Loss: 2.429683434659546\n",
      "Loss: 2.3932783938353666\n",
      "Loss: 2.33709029964137\n",
      "Loss: 2.2943231761739247\n",
      "Loss: 2.266467272838171\n",
      "Loss: 2.2175536195998258\n",
      "Loss: 2.17847898197432\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23286/2081485536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23286/3677250805.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23286/3677250805.py\u001b[0m in \u001b[0;36mpredict_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_param\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mval_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/MultBinaryGP/script/Node.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/MultBinaryGP/script/Node.py\u001b[0m in \u001b[0;36mpred_rec\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/MultBinaryGP/script/Functions.py\u001b[0m in \u001b[0;36m_divide\u001b[0;34m(sons)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m26843545\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m26843545\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26843545\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "gsgp = GStackGP(X_train,y_train,0.3)\n",
    "for i in range(1500):\n",
    "    gsgp.evolve()\n",
    "    \n",
    "    if i%1 == 0:\n",
    "        pred = gsgp.predict(X_train)\n",
    "        train_acc.append(accuracy_score(y_train,pred))\n",
    "        pred = gsgp.predict(X_test)\n",
    "        test_acc.append(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "3194b1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2313d02a60>]"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      1.000     1.000     1.000        65\n",
      "         1.0      1.000     1.000     1.000        80\n",
      "\n",
      "    accuracy                          1.000       145\n",
      "   macro avg      1.000     1.000     1.000       145\n",
      "weighted avg      1.000     1.000     1.000       145\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.867     0.812     0.839        32\n",
      "         1.0      0.818     0.871     0.844        31\n",
      "\n",
      "    accuracy                          0.841        63\n",
      "   macro avg      0.842     0.842     0.841        63\n",
      "weighted avg      0.843     0.841     0.841        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = gsgp.predict(X_train)\n",
    "print(classification_report(y_train,pred,digits=3))\n",
    "pred = gsgp.predict(X_test)\n",
    "print(classification_report(y_test,pred,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171da7fb",
   "metadata": {},
   "source": [
    "# Higgs Boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f813da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    print(roc_auc_score(true_y,prob[:,1]))\n",
    "\n",
    "df = pd.read_csv('../data/HIGGS.csv',header=None)\n",
    "X = df.iloc[:,1:].to_numpy()\n",
    "y = df.iloc[:,0].to_numpy().astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=500000)\n",
    "Xs,ys = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25782751",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Xs[:1050000]\n",
    "y_train = ys[:1050000]\n",
    "\n",
    "X_test = Xs[1050000:1210000]\n",
    "y_test = ys[1050000:1210000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8447e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start\n",
      "Loss: 271919.0944547627\n",
      "train complete\n",
      "train auc: 0.5258654810848914 test auc 0.5246813939060924\n",
      "train start\n",
      "Loss: 260271.01078482388\n",
      "train complete\n",
      "train start\n",
      "Loss: 250622.0223460063\n",
      "train complete\n",
      "train start\n",
      "Loss: 249377.3030609782\n",
      "train complete\n",
      "train start\n",
      "Loss: 247674.13267773198\n",
      "train complete\n",
      "train start\n",
      "Loss: 246092.75683943022\n",
      "train complete\n",
      "train start\n",
      "Loss: 244864.77042022234\n",
      "train complete\n",
      "train start\n",
      "Loss: 244109.4138296209\n",
      "train complete\n",
      "train start\n",
      "Loss: 243205.71141343552\n",
      "train complete\n",
      "train start\n",
      "Loss: 242986.57455942972\n",
      "train complete\n",
      "train start\n",
      "Loss: 240457.04838524185\n",
      "train complete\n",
      "train auc: 0.6132988188301514 test auc 0.6120390153127323\n",
      "train start\n",
      "Loss: 239727.16239474286\n",
      "train complete\n",
      "train start\n",
      "Loss: 239158.3263821821\n",
      "train complete\n",
      "train start\n",
      "Loss: 238698.26702003824\n",
      "train complete\n",
      "train start\n",
      "Loss: 234226.41463685918\n",
      "train complete\n",
      "train start\n",
      "Loss: 233400.83277542077\n",
      "train complete\n",
      "train start\n",
      "Loss: 232759.91196994\n",
      "train complete\n",
      "train start\n",
      "Loss: 232479.51549038087\n",
      "train complete\n",
      "train start\n",
      "Loss: 232176.73679356545\n",
      "train complete\n",
      "train start\n",
      "Loss: 231638.6623393377\n",
      "train complete\n",
      "train start\n",
      "Loss: 231429.61727686087\n",
      "train complete\n",
      "train auc: 0.6436750800551445 test auc 0.6439444245431653\n",
      "train start\n",
      "Loss: 231034.80349014094\n",
      "train complete\n",
      "train start\n",
      "Loss: 230733.55105282084\n",
      "train complete\n",
      "train start\n",
      "Loss: 230461.72353156775\n",
      "train complete\n",
      "train start\n",
      "Loss: 230326.97380175695\n",
      "train complete\n",
      "train start\n",
      "Loss: 229803.44036067175\n",
      "train complete\n",
      "train start\n",
      "Loss: 229524.68233934636\n",
      "train complete\n",
      "train start\n",
      "Loss: 229238.437560293\n",
      "train complete\n",
      "train start\n",
      "Loss: 229111.30922442552\n",
      "train complete\n",
      "train start\n",
      "Loss: 228929.80092297817\n",
      "train complete\n",
      "train start\n",
      "Loss: 228719.22775046417\n",
      "train complete\n",
      "train auc: 0.6493980630859355 test auc 0.6495549522843039\n",
      "train start\n",
      "Loss: 228596.51022322164\n",
      "train complete\n",
      "train start\n",
      "Loss: 228408.01658976154\n",
      "train complete\n",
      "train start\n",
      "Loss: 228231.97351817542\n",
      "train complete\n",
      "train start\n",
      "Loss: 227935.82066636786\n",
      "train complete\n",
      "train start\n",
      "Loss: 227784.91547469553\n",
      "train complete\n",
      "train start\n",
      "Loss: 227658.04638886722\n",
      "train complete\n",
      "train start\n",
      "Loss: 227483.67378717038\n",
      "train complete\n",
      "train start\n",
      "Loss: 227301.84126888396\n",
      "train complete\n",
      "train start\n",
      "Loss: 227073.20590920912\n",
      "train complete\n",
      "train start\n",
      "Loss: 226943.4988927691\n",
      "train complete\n",
      "train auc: 0.6538173147351156 test auc 0.6532618358701032\n",
      "train start\n",
      "Loss: 226859.29061143275\n",
      "train complete\n",
      "train start\n",
      "Loss: 226767.51544913504\n",
      "train complete\n",
      "train start\n",
      "Loss: 226638.52588119314\n",
      "train complete\n",
      "train start\n",
      "Loss: 226550.19006891077\n",
      "train complete\n",
      "train start\n",
      "Loss: 226481.33427038472\n",
      "train complete\n",
      "train start\n",
      "Loss: 226378.5476505845\n",
      "train complete\n",
      "train start\n",
      "Loss: 226062.80271739015\n",
      "train complete\n",
      "train start\n",
      "Loss: 225958.80797115105\n",
      "train complete\n",
      "train start\n",
      "Loss: 225704.9438110638\n",
      "train complete\n",
      "train start\n",
      "Loss: 225572.01997949867\n",
      "train complete\n",
      "train auc: 0.6565614302154482 test auc 0.656305261737169\n",
      "train start\n",
      "Loss: 225264.74751350883\n",
      "train complete\n",
      "train start\n",
      "Loss: 225161.9073731425\n",
      "train complete\n",
      "train start\n",
      "Loss: 225080.52964378594\n",
      "train complete\n",
      "train start\n",
      "Loss: 224983.11179495344\n",
      "train complete\n",
      "train start\n",
      "Loss: 224828.83849666157\n",
      "train complete\n",
      "train start\n",
      "Loss: 224767.54486636913\n",
      "train complete\n",
      "train start\n",
      "Loss: 224713.78025881233\n",
      "train complete\n",
      "train start\n",
      "Loss: 224662.0801156653\n",
      "train complete\n",
      "train start\n",
      "Loss: 224583.19755028863\n",
      "train complete\n",
      "train start\n",
      "Loss: 224516.55177448565\n",
      "train complete\n",
      "train auc: 0.6582152004133821 test auc 0.6579249565743212\n",
      "train start\n",
      "Loss: 224421.2575528745\n",
      "train complete\n",
      "train start\n",
      "Loss: 224347.92526861152\n",
      "train complete\n",
      "train start\n",
      "Loss: 224240.41360623014\n",
      "train complete\n",
      "train start\n",
      "Loss: 223735.96525329616\n",
      "train complete\n",
      "train start\n",
      "Loss: 223666.8767613675\n",
      "train complete\n",
      "train start\n",
      "Loss: 223518.1157391032\n",
      "train complete\n",
      "train start\n",
      "Loss: 223470.37311898114\n",
      "train complete\n",
      "train start\n",
      "Loss: 223320.55705766645\n",
      "train complete\n",
      "train start\n",
      "Loss: 223272.70109175885\n",
      "train complete\n",
      "train start\n",
      "Loss: 223207.60556883216\n",
      "train complete\n",
      "train auc: 0.6613213626863199 test auc 0.660116813711289\n",
      "train start\n",
      "Loss: 223127.43246535928\n",
      "train complete\n",
      "train start\n",
      "Loss: 223003.04031848258\n",
      "train complete\n",
      "train start\n",
      "Loss: 222886.88492059486\n",
      "train complete\n",
      "train start\n",
      "Loss: 222788.47295609236\n",
      "train complete\n",
      "train start\n",
      "Loss: 222749.11670402644\n",
      "train complete\n",
      "train start\n",
      "Loss: 222554.82116534695\n",
      "train complete\n",
      "train start\n",
      "Loss: 222463.7667393563\n",
      "train complete\n",
      "train start\n",
      "Loss: 222356.86162142592\n",
      "train complete\n",
      "train start\n",
      "Loss: 222270.51854732115\n",
      "train complete\n",
      "train start\n",
      "Loss: 222154.20343048905\n",
      "train complete\n",
      "train auc: 0.6635383899160323 test auc 0.6622464775206425\n",
      "train start\n",
      "Loss: 222054.79733995674\n",
      "train complete\n",
      "train start\n",
      "Loss: 221937.46118966097\n",
      "train complete\n",
      "train start\n",
      "Loss: 221845.29211969624\n",
      "train complete\n",
      "train start\n",
      "Loss: 221254.05698572454\n",
      "train complete\n",
      "train start\n",
      "Loss: 221209.05917963837\n",
      "train complete\n",
      "train start\n",
      "Loss: 221159.17977658898\n",
      "train complete\n",
      "train start\n",
      "Loss: 221061.9268581068\n",
      "train complete\n",
      "train start\n",
      "Loss: 221018.834567811\n",
      "train complete\n",
      "train start\n",
      "Loss: 220969.8297788874\n",
      "train complete\n",
      "train start\n",
      "Loss: 220884.0713337778\n",
      "train complete\n",
      "train auc: 0.666353789575044 test auc 0.6663355080213031\n",
      "train start\n",
      "Loss: 220827.4379566712\n",
      "train complete\n",
      "train start\n",
      "Loss: 220754.22275922316\n",
      "train complete\n",
      "train start\n",
      "Loss: 220690.77060901118\n",
      "train complete\n",
      "train start\n",
      "Loss: 220637.72363095108\n",
      "train complete\n",
      "train start\n",
      "Loss: 220588.25767402822\n",
      "train complete\n",
      "train start\n",
      "Loss: 220525.54985688257\n",
      "train complete\n",
      "train start\n",
      "Loss: 220472.94557412894\n",
      "train complete\n",
      "train start\n",
      "Loss: 220333.00858530888\n",
      "train complete\n",
      "train start\n",
      "Loss: 220181.8943477724\n",
      "train complete\n",
      "train start\n",
      "Loss: 220044.13286650932\n",
      "train complete\n",
      "train auc: 0.668103252695043 test auc 0.6680409702604613\n",
      "train start\n",
      "Loss: 219876.78168468154\n",
      "train complete\n",
      "train start\n",
      "Loss: 219684.62435149553\n",
      "train complete\n",
      "train start\n",
      "Loss: 219644.3045598593\n",
      "train complete\n",
      "train start\n",
      "Loss: 219453.713231895\n",
      "train complete\n",
      "train start\n",
      "Loss: 219350.12899782904\n",
      "train complete\n",
      "train start\n",
      "Loss: 219211.20477050781\n",
      "train complete\n",
      "train start\n",
      "Loss: 219167.68878939375\n",
      "train complete\n",
      "train start\n",
      "Loss: 219105.52232624585\n",
      "train complete\n",
      "train start\n",
      "Loss: 218956.73711775593\n",
      "train complete\n",
      "train start\n",
      "Loss: 218805.93131845686\n",
      "train complete\n",
      "train auc: 0.6701625265444092 test auc 0.6692936653255734\n",
      "train start\n",
      "Loss: 218755.8025332866\n",
      "train complete\n",
      "train start\n",
      "Loss: 218723.4827247627\n",
      "train complete\n",
      "train start\n",
      "Loss: 218674.00485780454\n",
      "train complete\n",
      "train start\n",
      "Loss: 218627.76066679222\n",
      "train complete\n",
      "train start\n",
      "Loss: 218576.44373180595\n",
      "train complete\n",
      "train start\n",
      "Loss: 218554.0922243508\n",
      "train complete\n",
      "train start\n",
      "Loss: 218305.20598271047\n",
      "train complete\n",
      "train start\n",
      "Loss: 218143.06285583915\n",
      "train complete\n",
      "train start\n",
      "Loss: 218108.53713174467\n",
      "train complete\n",
      "train start\n",
      "Loss: 218068.76589592034\n",
      "train complete\n",
      "train auc: 0.6712143528187084 test auc 0.6705694497439576\n",
      "train start\n",
      "Loss: 218032.23357486242\n",
      "train complete\n",
      "train start\n",
      "Loss: 217962.6258240766\n",
      "train complete\n",
      "train start\n",
      "Loss: 217929.48784676535\n",
      "train complete\n",
      "train start\n",
      "Loss: 217863.40893452038\n",
      "train complete\n",
      "train start\n",
      "Loss: 217803.46283107158\n",
      "train complete\n",
      "train start\n",
      "Loss: 217749.19783123903\n",
      "train complete\n",
      "train start\n",
      "Loss: 217554.97945640897\n",
      "train complete\n",
      "train start\n",
      "Loss: 217502.70200028157\n",
      "train complete\n",
      "train start\n",
      "Loss: 217449.03337794682\n",
      "train complete\n",
      "train start\n",
      "Loss: 217306.93208713093\n",
      "train complete\n",
      "train auc: 0.6733713747050745 test auc 0.6728249763333738\n",
      "train start\n",
      "Loss: 217245.2072041841\n",
      "train complete\n",
      "train start\n",
      "Loss: 217200.22168803835\n",
      "train complete\n",
      "train start\n",
      "Loss: 217139.7142151391\n",
      "train complete\n",
      "train start\n",
      "Loss: 217080.32873329255\n",
      "train complete\n",
      "train start\n",
      "Loss: 217034.44278167366\n",
      "train complete\n",
      "train start\n",
      "Loss: 216984.2843777924\n",
      "train complete\n",
      "train start\n",
      "Loss: 216920.68569015607\n",
      "train complete\n",
      "train start\n",
      "Loss: 216878.62317215605\n",
      "train complete\n",
      "train start\n",
      "Loss: 216790.30905352827\n",
      "train complete\n",
      "train start\n",
      "Loss: 216720.05671222415\n",
      "train complete\n",
      "train auc: 0.6746181955322843 test auc 0.6733139846466702\n",
      "train start\n",
      "Loss: 216687.49935503118\n",
      "train complete\n",
      "train start\n",
      "Loss: 216645.20118172915\n",
      "train complete\n",
      "train start\n",
      "Loss: 216574.83577194397\n",
      "train complete\n",
      "train start\n",
      "Loss: 216534.90166191183\n",
      "train complete\n",
      "train start\n",
      "Loss: 216490.94945578594\n",
      "train complete\n",
      "train start\n",
      "Loss: 216412.4149162302\n",
      "train complete\n",
      "train start\n",
      "Loss: 216380.08441832382\n",
      "train complete\n",
      "train start\n",
      "Loss: 216314.53634791757\n",
      "train complete\n",
      "train start\n",
      "Loss: 216296.53792484794\n",
      "train complete\n",
      "train start\n",
      "Loss: 216240.12559159804\n",
      "train complete\n",
      "train auc: 0.6752672766485888 test auc 0.6740689934786342\n",
      "train start\n",
      "Loss: 216195.68949935192\n",
      "train complete\n",
      "train start\n",
      "Loss: 216100.48479636756\n",
      "train complete\n",
      "train start\n",
      "Loss: 216050.74265334944\n",
      "train complete\n",
      "train start\n",
      "Loss: 216014.93645357018\n",
      "train complete\n",
      "train start\n",
      "Loss: 215941.42284601228\n",
      "train complete\n",
      "train start\n",
      "Loss: 215629.91982670582\n",
      "train complete\n",
      "train start\n",
      "Loss: 215599.20938170375\n",
      "train complete\n",
      "train start\n",
      "Loss: 215558.17662698549\n",
      "train complete\n",
      "train start\n",
      "Loss: 215538.67215759764\n",
      "train complete\n",
      "train start\n",
      "Loss: 215511.70290674566\n",
      "train complete\n",
      "train auc: 0.6766407141634677 test auc 0.6750838034186988\n",
      "train start\n",
      "Loss: 215486.7053626417\n",
      "train complete\n",
      "train start\n",
      "Loss: 215440.95120800228\n",
      "train complete\n",
      "train start\n",
      "Loss: 215397.23168967513\n",
      "train complete\n",
      "train start\n",
      "Loss: 215368.54637950312\n",
      "train complete\n",
      "train start\n",
      "Loss: 215305.36649952925\n",
      "train complete\n",
      "train start\n",
      "Loss: 215273.97830756736\n",
      "train complete\n",
      "train start\n",
      "Loss: 215226.8393098097\n",
      "train complete\n",
      "train start\n",
      "Loss: 215185.91778494156\n",
      "train complete\n",
      "train start\n",
      "Loss: 215148.5316469187\n",
      "train complete\n",
      "train start\n",
      "Loss: 215120.31165083498\n",
      "train complete\n",
      "train auc: 0.6773970382984317 test auc 0.6761953897342905\n",
      "train start\n",
      "Loss: 215086.26064992134\n",
      "train complete\n",
      "train start\n",
      "Loss: 215022.74891771027\n",
      "train complete\n",
      "train start\n",
      "Loss: 215003.31444232038\n",
      "train complete\n",
      "train start\n",
      "Loss: 214963.2084401835\n",
      "train complete\n",
      "train start\n",
      "Loss: 214951.08314477422\n",
      "train complete\n",
      "train start\n",
      "Loss: 214920.5515415534\n",
      "train complete\n",
      "train start\n",
      "Loss: 214886.56280081897\n",
      "train complete\n",
      "train start\n",
      "Loss: 214778.6034450383\n",
      "train complete\n",
      "train start\n",
      "Loss: 214744.4440735656\n",
      "train complete\n",
      "train start\n",
      "Loss: 214688.85858868924\n",
      "train complete\n",
      "train auc: 0.6779983184430831 test auc 0.6771798238175204\n",
      "train start\n",
      "Loss: 214620.00948374945\n",
      "train complete\n",
      "train start\n",
      "Loss: 214548.63806354153\n",
      "train complete\n",
      "train start\n",
      "Loss: 214514.38348584974\n",
      "train complete\n",
      "train start\n",
      "Loss: 214450.05589226895\n",
      "train complete\n",
      "train start\n",
      "Loss: 214404.11185084353\n",
      "train complete\n",
      "train start\n",
      "Loss: 214371.28629347362\n",
      "train complete\n",
      "train start\n",
      "Loss: 214350.63445359145\n",
      "train complete\n",
      "train start\n",
      "Loss: 214234.4312222981\n",
      "train complete\n",
      "train start\n",
      "Loss: 214200.92645576998\n",
      "train complete\n",
      "train start\n",
      "Loss: 214174.41314006623\n",
      "train complete\n",
      "train auc: 0.6790438206285722 test auc 0.6778245515357063\n",
      "train start\n",
      "Loss: 214157.55524902415\n",
      "train complete\n",
      "train start\n",
      "Loss: 214117.30707522065\n",
      "train complete\n",
      "train start\n",
      "Loss: 214074.64611851607\n",
      "train complete\n",
      "train start\n",
      "Loss: 214046.48808878855\n",
      "train complete\n",
      "train start\n",
      "Loss: 213996.00668979425\n",
      "train complete\n",
      "train start\n",
      "Loss: 213952.8181209992\n",
      "train complete\n",
      "train start\n",
      "Loss: 213936.49714794868\n",
      "train complete\n",
      "train start\n",
      "Loss: 213913.08897563006\n",
      "train complete\n",
      "train start\n",
      "Loss: 213856.93292794412\n",
      "train complete\n",
      "train start\n",
      "Loss: 213620.1014329087\n",
      "train complete\n",
      "train auc: 0.6801295273961986 test auc 0.679246204068909\n",
      "train start\n",
      "Loss: 213602.22108235562\n",
      "train complete\n",
      "train start\n",
      "Loss: 213554.13754339895\n",
      "train complete\n",
      "train start\n",
      "Loss: 213533.76598756897\n",
      "train complete\n",
      "train start\n",
      "Loss: 213465.15110806056\n",
      "train complete\n",
      "train start\n",
      "Loss: 213435.63044708222\n",
      "train complete\n",
      "train start\n",
      "Loss: 213385.165483009\n",
      "train complete\n",
      "train start\n",
      "Loss: 213358.59854402998\n",
      "train complete\n",
      "train start\n",
      "Loss: 213327.27958808414\n",
      "train complete\n",
      "train start\n",
      "Loss: 213311.78806216916\n",
      "train complete\n",
      "train start\n",
      "Loss: 213237.4408871351\n",
      "train complete\n",
      "train auc: 0.6814559134172302 test auc 0.6801229704538477\n",
      "train start\n",
      "Loss: 213128.125225967\n",
      "train complete\n",
      "train start\n",
      "Loss: 213097.93650503267\n",
      "train complete\n",
      "train start\n",
      "Loss: 213063.787684399\n",
      "train complete\n",
      "train start\n",
      "Loss: 213020.95464037257\n",
      "train complete\n",
      "train start\n",
      "Loss: 212987.44162562938\n",
      "train complete\n",
      "train start\n",
      "Loss: 212964.12486431983\n",
      "train complete\n",
      "train start\n",
      "Loss: 212934.69857581524\n",
      "train complete\n",
      "train start\n",
      "Loss: 212905.00370153858\n",
      "train complete\n",
      "train start\n",
      "Loss: 212866.583828839\n",
      "train complete\n",
      "train start\n",
      "Loss: 212830.85134321032\n",
      "train complete\n",
      "train auc: 0.6821680827768668 test auc 0.680606617029969\n",
      "train start\n",
      "Loss: 212805.0215335328\n",
      "train complete\n",
      "train start\n",
      "Loss: 212776.82667431337\n",
      "train complete\n",
      "train start\n",
      "Loss: 212750.2156808696\n",
      "train complete\n",
      "train start\n",
      "Loss: 212726.26608224685\n",
      "train complete\n",
      "train start\n",
      "Loss: 212687.74365992678\n",
      "train complete\n",
      "train start\n",
      "Loss: 212658.46744494853\n",
      "train complete\n",
      "train start\n",
      "Loss: 212616.8248587203\n",
      "train complete\n",
      "train start\n",
      "Loss: 212594.7621072393\n",
      "train complete\n",
      "train start\n",
      "Loss: 212536.83592599756\n",
      "train complete\n",
      "train start\n",
      "Loss: 212469.0945472899\n",
      "train complete\n",
      "train auc: 0.6826158873088717 test auc 0.6820480066542014\n",
      "train start\n",
      "Loss: 212388.26271011273\n",
      "train complete\n",
      "train start\n",
      "Loss: 212360.1721423143\n",
      "train complete\n",
      "train start\n",
      "Loss: 212215.94964030726\n",
      "train complete\n",
      "train start\n",
      "Loss: 212173.73584184266\n",
      "train complete\n",
      "train start\n",
      "Loss: 212122.65971848552\n",
      "train complete\n",
      "train start\n",
      "Loss: 212063.1871501513\n",
      "train complete\n",
      "train start\n",
      "Loss: 212033.84943067678\n",
      "train complete\n",
      "train start\n",
      "Loss: 211977.64604903982\n",
      "train complete\n",
      "train start\n",
      "Loss: 211943.370812328\n",
      "train complete\n",
      "train start\n",
      "Loss: 211921.31458663344\n",
      "train complete\n",
      "train auc: 0.6833402715090398 test auc 0.6823997761435513\n",
      "train start\n",
      "Loss: 211891.92096088827\n",
      "train complete\n",
      "train start\n",
      "Loss: 211864.90104114258\n",
      "train complete\n",
      "train start\n",
      "Loss: 211827.50518974397\n",
      "train complete\n",
      "train start\n",
      "Loss: 211801.7867277157\n",
      "train complete\n",
      "train start\n",
      "Loss: 211776.05218664007\n",
      "train complete\n",
      "train start\n",
      "Loss: 211747.9285813969\n",
      "train complete\n",
      "train start\n",
      "Loss: 211723.760539776\n",
      "train complete\n",
      "train start\n",
      "Loss: 211679.45881608131\n",
      "train complete\n",
      "train start\n",
      "Loss: 211649.10999425038\n",
      "train complete\n",
      "train start\n",
      "Loss: 211626.18889759746\n",
      "train complete\n",
      "train auc: 0.6840135841631556 test auc 0.683068245784542\n",
      "train start\n",
      "Loss: 211610.72753814099\n",
      "train complete\n",
      "train start\n",
      "Loss: 211596.16462792084\n",
      "train complete\n",
      "train start\n",
      "Loss: 211560.33961581008\n",
      "train complete\n",
      "train start\n",
      "Loss: 211541.55029988775\n",
      "train complete\n",
      "train start\n",
      "Loss: 211477.1758407421\n",
      "train complete\n",
      "train start\n",
      "Loss: 211396.69629448256\n",
      "train complete\n",
      "train start\n",
      "Loss: 211346.15204606636\n",
      "train complete\n",
      "train start\n",
      "Loss: 211189.9331894192\n",
      "train complete\n",
      "train start\n",
      "Loss: 211158.7967922133\n",
      "train complete\n",
      "train start\n",
      "Loss: 211119.7357972676\n",
      "train complete\n",
      "train auc: 0.6850787003093606 test auc 0.6839835655382799\n",
      "train start\n",
      "Loss: 211092.71885287072\n",
      "train complete\n",
      "train start\n",
      "Loss: 211046.41001780567\n",
      "train complete\n",
      "train start\n",
      "Loss: 211030.93993636584\n",
      "train complete\n",
      "train start\n",
      "Loss: 210994.27753830602\n",
      "train complete\n",
      "train start\n",
      "Loss: 210979.24174674627\n",
      "train complete\n",
      "train start\n",
      "Loss: 210921.02566116006\n",
      "train complete\n",
      "train start\n",
      "Loss: 210886.07025044892\n",
      "train complete\n",
      "train start\n",
      "Loss: 210866.74845961784\n",
      "train complete\n",
      "train start\n",
      "Loss: 210806.78128422124\n",
      "train complete\n",
      "train start\n",
      "Loss: 210777.58420577587\n",
      "train complete\n",
      "train auc: 0.6858402956774915 test auc 0.6844823420017783\n",
      "train start\n",
      "Loss: 210749.03077059865\n",
      "train complete\n",
      "train start\n",
      "Loss: 210737.879787718\n",
      "train complete\n",
      "train start\n",
      "Loss: 210725.1466051344\n",
      "train complete\n",
      "train start\n",
      "Loss: 210712.445442041\n",
      "train complete\n",
      "train start\n",
      "Loss: 210684.56263222935\n",
      "train complete\n",
      "train start\n",
      "Loss: 210660.1362216054\n",
      "train complete\n",
      "train start\n",
      "Loss: 210633.12723856512\n",
      "train complete\n",
      "train start\n",
      "Loss: 210555.11612176054\n",
      "train complete\n",
      "train start\n",
      "Loss: 210531.672066638\n",
      "train complete\n",
      "train start\n",
      "Loss: 210489.78187736726\n",
      "train complete\n",
      "train auc: 0.68620554784639 test auc 0.6846477966146576\n",
      "train start\n",
      "Loss: 210430.74553121236\n",
      "train complete\n",
      "train start\n",
      "Loss: 210403.36472358322\n",
      "train complete\n",
      "train start\n",
      "Loss: 210380.07957054925\n",
      "train complete\n",
      "train start\n",
      "Loss: 210332.8206140509\n",
      "train complete\n",
      "train start\n",
      "Loss: 210315.63826979155\n",
      "train complete\n",
      "train start\n",
      "Loss: 210283.53934500235\n",
      "train complete\n",
      "train start\n",
      "Loss: 210231.17633083428\n",
      "train complete\n",
      "train start\n",
      "Loss: 210204.11711415113\n",
      "train complete\n",
      "train start\n",
      "Loss: 210190.27351971265\n",
      "train complete\n",
      "train start\n",
      "Loss: 210165.54573212084\n",
      "train complete\n",
      "train auc: 0.6870865464354777 test auc 0.6850633343830919\n",
      "train start\n",
      "Loss: 210137.85333187287\n",
      "train complete\n",
      "train start\n",
      "Loss: 210097.01429990222\n",
      "train complete\n",
      "train start\n",
      "Loss: 210077.31185622414\n",
      "train complete\n",
      "train start\n",
      "Loss: 210038.6846940539\n",
      "train complete\n",
      "train start\n",
      "Loss: 210010.56361510325\n",
      "train complete\n",
      "train start\n",
      "Loss: 209992.70913519623\n",
      "train complete\n",
      "train start\n",
      "Loss: 209974.6352949796\n",
      "train complete\n",
      "train start\n",
      "Loss: 209937.20433906416\n",
      "train complete\n",
      "train start\n",
      "Loss: 209921.3177871094\n",
      "train complete\n",
      "train start\n",
      "Loss: 209902.84649653887\n",
      "train complete\n",
      "train auc: 0.6873719000281152 test auc 0.6859587491973865\n",
      "train start\n",
      "Loss: 209891.6237029805\n",
      "train complete\n",
      "train start\n",
      "Loss: 209868.47197757926\n",
      "train complete\n",
      "train start\n",
      "Loss: 209842.0388575896\n",
      "train complete\n",
      "train start\n",
      "Loss: 209800.98943206444\n",
      "train complete\n",
      "train start\n",
      "Loss: 209766.56514541229\n",
      "train complete\n",
      "train start\n",
      "Loss: 209719.33141275813\n",
      "train complete\n",
      "train start\n",
      "Loss: 209697.44194983214\n",
      "train complete\n",
      "train start\n",
      "Loss: 209624.10981623753\n",
      "train complete\n",
      "train start\n",
      "Loss: 209598.37701947536\n",
      "train complete\n",
      "train start\n",
      "Loss: 209391.14979304955\n",
      "train complete\n",
      "train auc: 0.6887273207264919 test auc 0.6872866246650327\n",
      "train start\n",
      "Loss: 209314.45005935675\n",
      "train complete\n",
      "train start\n",
      "Loss: 209297.9405415081\n",
      "train complete\n",
      "train start\n",
      "Loss: 209250.51718684298\n",
      "train complete\n",
      "train start\n",
      "Loss: 209231.48656434388\n",
      "train complete\n",
      "train start\n",
      "Loss: 209212.70686762535\n",
      "train complete\n",
      "train start\n",
      "Loss: 209190.2460776818\n",
      "train complete\n",
      "train start\n",
      "Loss: 209173.53466981472\n",
      "train complete\n",
      "train start\n",
      "Loss: 209156.16894432102\n",
      "train complete\n",
      "train start\n",
      "Loss: 209133.5321036564\n",
      "train complete\n",
      "train start\n",
      "Loss: 209120.12818287377\n",
      "train complete\n",
      "train auc: 0.6894809947705653 test auc 0.6877930076421539\n",
      "train start\n",
      "Loss: 209085.55534415564\n",
      "train complete\n",
      "train start\n",
      "Loss: 209056.712420246\n",
      "train complete\n",
      "train start\n",
      "Loss: 209033.10096185617\n",
      "train complete\n",
      "train start\n",
      "Loss: 209018.06070700882\n",
      "train complete\n",
      "train start\n",
      "Loss: 208714.32531014644\n",
      "train complete\n",
      "train start\n",
      "Loss: 208687.893163762\n",
      "train complete\n",
      "train start\n",
      "Loss: 208656.49022670317\n",
      "train complete\n",
      "train start\n",
      "Loss: 208618.29780430248\n",
      "train complete\n",
      "train start\n",
      "Loss: 208597.12146378562\n",
      "train complete\n",
      "train start\n",
      "Loss: 208575.97373646067\n",
      "train complete\n",
      "train auc: 0.6904194369508995 test auc 0.6884289727319428\n",
      "train start\n",
      "Loss: 208565.6409013413\n",
      "train complete\n",
      "train start\n",
      "Loss: 208553.59740350925\n",
      "train complete\n",
      "train start\n",
      "Loss: 208531.31328503313\n",
      "train complete\n",
      "train start\n",
      "Loss: 208523.71262538177\n",
      "train complete\n",
      "train start\n",
      "Loss: 208512.7330692644\n",
      "train complete\n",
      "train start\n",
      "Loss: 208502.44294798307\n",
      "train complete\n",
      "train start\n",
      "Loss: 208481.15763982394\n",
      "train complete\n",
      "train start\n",
      "Loss: 208446.13421283857\n",
      "train complete\n",
      "train start\n",
      "Loss: 208427.85635769178\n",
      "train complete\n",
      "train start\n",
      "Loss: 208414.99248834717\n",
      "train complete\n",
      "train auc: 0.6906725440629418 test auc 0.6882337948317622\n",
      "train start\n",
      "Loss: 208403.05003737676\n",
      "train complete\n",
      "train start\n",
      "Loss: 208361.82041257533\n",
      "train complete\n",
      "train start\n",
      "Loss: 208347.02322283038\n",
      "train complete\n",
      "train start\n",
      "Loss: 208319.58998732228\n",
      "train complete\n",
      "train start\n",
      "Loss: 208305.25863518214\n",
      "train complete\n",
      "train start\n",
      "Loss: 208294.0677487842\n",
      "train complete\n",
      "train start\n",
      "Loss: 208247.92002998467\n",
      "train complete\n",
      "train start\n",
      "Loss: 208225.0398140863\n",
      "train complete\n",
      "train start\n",
      "Loss: 208182.81214037977\n",
      "train complete\n",
      "train start\n",
      "Loss: 208171.3395632072\n",
      "train complete\n",
      "train auc: 0.6909134399393745 test auc 0.6888231984578779\n",
      "train start\n",
      "Loss: 208147.02596646672\n",
      "train complete\n",
      "train start\n",
      "Loss: 208133.7797305053\n",
      "train complete\n",
      "train start\n",
      "Loss: 208120.88253553197\n",
      "train complete\n",
      "train start\n",
      "Loss: 208093.5530303466\n",
      "train complete\n",
      "train start\n",
      "Loss: 208062.21772603274\n",
      "train complete\n",
      "train start\n",
      "Loss: 208046.29784520183\n",
      "train complete\n",
      "train start\n",
      "Loss: 208028.8314302992\n",
      "train complete\n",
      "train start\n",
      "Loss: 208004.15218012407\n",
      "train complete\n",
      "train start\n",
      "Loss: 207961.2064540798\n",
      "train complete\n",
      "train start\n",
      "Loss: 207926.06319531772\n",
      "train complete\n",
      "train auc: 0.691144665303587 test auc 0.6891271154625787\n",
      "train start\n",
      "Loss: 207912.7195444229\n",
      "train complete\n",
      "train start\n",
      "Loss: 207901.07153522133\n",
      "train complete\n",
      "train start\n",
      "Loss: 207881.9292288086\n",
      "train complete\n",
      "train start\n",
      "Loss: 207872.68125740436\n",
      "train complete\n",
      "train start\n",
      "Loss: 207850.11905068054\n",
      "train complete\n",
      "train start\n",
      "Loss: 207804.5118680365\n",
      "train complete\n",
      "train start\n",
      "Loss: 207776.71591117332\n",
      "train complete\n",
      "train start\n",
      "Loss: 207754.23790950206\n",
      "train complete\n",
      "train start\n",
      "Loss: 207609.4515669777\n",
      "train complete\n",
      "train start\n",
      "Loss: 207455.11452682008\n",
      "train complete\n",
      "train auc: 0.6920069471387315 test auc 0.689308670973751\n",
      "train start\n",
      "Loss: 207441.663745572\n",
      "train complete\n",
      "train start\n",
      "Loss: 207362.5417130718\n",
      "train complete\n",
      "train start\n",
      "Loss: 207348.12450723222\n",
      "train complete\n",
      "train start\n",
      "Loss: 207328.9112441815\n",
      "train complete\n",
      "train start\n",
      "Loss: 207317.04075413328\n",
      "train complete\n",
      "train start\n",
      "Loss: 207303.3910920964\n",
      "train complete\n",
      "train start\n",
      "Loss: 207282.69206042503\n",
      "train complete\n",
      "train start\n",
      "Loss: 207249.72591345865\n",
      "train complete\n",
      "train start\n",
      "Loss: 207214.7111357269\n",
      "train complete\n",
      "train start\n",
      "Loss: 207198.02615222937\n",
      "train complete\n",
      "train auc: 0.6926548970014472 test auc 0.6898439395647826\n",
      "train start\n",
      "Loss: 207176.3207109485\n",
      "train complete\n",
      "train start\n",
      "Loss: 207164.90762745138\n",
      "train complete\n",
      "train start\n",
      "Loss: 207143.83893816164\n",
      "train complete\n",
      "train start\n",
      "Loss: 207127.97292715323\n",
      "train complete\n",
      "train start\n",
      "Loss: 207116.85751899346\n",
      "train complete\n",
      "train start\n",
      "Loss: 207049.24272928046\n",
      "train complete\n",
      "train start\n",
      "Loss: 207026.74677276693\n",
      "train complete\n",
      "train start\n",
      "Loss: 207020.7502930374\n",
      "train complete\n",
      "train start\n",
      "Loss: 207005.89237802546\n",
      "train complete\n",
      "train start\n",
      "Loss: 206976.6789404327\n",
      "train complete\n",
      "train auc: 0.6932927787363518 test auc 0.6907313792264151\n",
      "train start\n",
      "Loss: 206888.80719673226\n",
      "train complete\n",
      "train start\n",
      "Loss: 206877.62674201315\n",
      "train complete\n",
      "train start\n",
      "Loss: 206778.90058352158\n",
      "train complete\n",
      "train start\n",
      "Loss: 206740.05446289084\n",
      "train complete\n",
      "train start\n",
      "Loss: 206721.47215360793\n",
      "train complete\n",
      "train start\n",
      "Loss: 206693.52700687628\n",
      "train complete\n",
      "train start\n",
      "Loss: 206658.23218095294\n",
      "train complete\n",
      "train start\n",
      "Loss: 206613.2100601169\n",
      "train complete\n",
      "train start\n",
      "Loss: 206591.32565539164\n",
      "train complete\n",
      "train start\n",
      "Loss: 206559.5054652558\n",
      "train complete\n",
      "train auc: 0.69436308464085 test auc 0.6920086209455117\n",
      "train start\n",
      "Loss: 206534.67031802944\n",
      "train complete\n",
      "train start\n",
      "Loss: 206516.89699230553\n",
      "train complete\n",
      "train start\n",
      "Loss: 206499.1909520174\n",
      "train complete\n",
      "train start\n",
      "Loss: 206471.25469753298\n",
      "train complete\n",
      "train start\n",
      "Loss: 206456.79500549723\n",
      "train complete\n",
      "train start\n",
      "Loss: 206369.53831009925\n",
      "train complete\n",
      "train start\n",
      "Loss: 206353.40934232413\n",
      "train complete\n",
      "train start\n",
      "Loss: 206322.29375262617\n",
      "train complete\n",
      "train start\n",
      "Loss: 206309.11700609303\n",
      "train complete\n",
      "train start\n",
      "Loss: 206265.37156392215\n",
      "train complete\n",
      "train auc: 0.6945680994562209 test auc 0.6923587982279475\n",
      "train start\n",
      "Loss: 206236.32089493092\n",
      "train complete\n",
      "train start\n",
      "Loss: 206222.1700535905\n",
      "train complete\n",
      "train start\n",
      "Loss: 206214.8566312132\n",
      "train complete\n",
      "train start\n",
      "Loss: 206185.1098058992\n",
      "train complete\n",
      "train start\n",
      "Loss: 206161.8412017966\n",
      "train complete\n",
      "train start\n",
      "Loss: 206044.45099428078\n",
      "train complete\n",
      "train start\n",
      "Loss: 205991.4139218741\n",
      "train complete\n",
      "train start\n",
      "Loss: 205982.91468323607\n",
      "train complete\n",
      "train start\n",
      "Loss: 205968.94378072835\n",
      "train complete\n",
      "train start\n",
      "Loss: 205953.00380341584\n",
      "train complete\n",
      "train auc: 0.695180881395156 test auc 0.6917311267322035\n",
      "train start\n",
      "Loss: 205881.549731783\n",
      "train complete\n",
      "train start\n",
      "Loss: 205867.92140233534\n",
      "train complete\n",
      "train start\n",
      "Loss: 205860.13330924753\n",
      "train complete\n",
      "train start\n",
      "Loss: 205847.02292971956\n",
      "train complete\n",
      "train start\n",
      "Loss: 205813.7290537062\n",
      "train complete\n",
      "train start\n",
      "Loss: 205796.1275748655\n",
      "train complete\n",
      "train start\n",
      "Loss: 205780.81847860094\n",
      "train complete\n",
      "train start\n",
      "Loss: 205751.92446445365\n",
      "train complete\n",
      "train start\n",
      "Loss: 205725.72381653474\n",
      "train complete\n",
      "train start\n",
      "Loss: 205716.6275877205\n",
      "train complete\n",
      "train auc: 0.695675791622282 test auc 0.6923386752424342\n",
      "train start\n",
      "Loss: 205702.72706938715\n",
      "train complete\n",
      "train start\n",
      "Loss: 205676.14710201518\n",
      "train complete\n",
      "train start\n",
      "Loss: 205667.2488088697\n",
      "train complete\n",
      "train start\n",
      "Loss: 205653.4026760739\n",
      "train complete\n",
      "train start\n",
      "Loss: 205644.41365333207\n",
      "train complete\n",
      "train start\n",
      "Loss: 205632.61792932305\n",
      "train complete\n",
      "train start\n",
      "Loss: 205621.68941022866\n",
      "train complete\n",
      "train start\n",
      "Loss: 205567.02215928616\n",
      "train complete\n",
      "train start\n",
      "Loss: 205526.76611690366\n",
      "train complete\n",
      "train start\n",
      "Loss: 205512.92773349173\n",
      "train complete\n",
      "train auc: 0.6959153969002998 test auc 0.6930348241849045\n",
      "train start\n",
      "Loss: 205495.68633645942\n",
      "train complete\n",
      "train start\n",
      "Loss: 205483.39073540753\n",
      "train complete\n",
      "train start\n",
      "Loss: 205466.45327686536\n",
      "train complete\n",
      "train start\n",
      "Loss: 205420.7127573379\n",
      "train complete\n",
      "train start\n",
      "Loss: 205396.9138210422\n",
      "train complete\n",
      "train start\n",
      "Loss: 205382.65549740946\n",
      "train complete\n",
      "train start\n",
      "Loss: 205357.83674935673\n",
      "train complete\n",
      "train start\n",
      "Loss: 205344.86374916855\n",
      "train complete\n",
      "train start\n",
      "Loss: 205307.8113123749\n",
      "train complete\n",
      "train start\n",
      "Loss: 205278.7482350568\n",
      "train complete\n",
      "train auc: 0.6963643926040635 test auc 0.6931861042345551\n",
      "train start\n",
      "Loss: 205261.43148710637\n",
      "train complete\n",
      "train start\n",
      "Loss: 205237.76651961126\n",
      "train complete\n",
      "train start\n",
      "Loss: 205212.36444001686\n",
      "train complete\n",
      "train start\n",
      "Loss: 205196.1670095346\n",
      "train complete\n",
      "train start\n",
      "Loss: 205169.203370402\n",
      "train complete\n",
      "train start\n",
      "Loss: 205142.6279480082\n",
      "train complete\n",
      "train start\n",
      "Loss: 205116.3244089902\n",
      "train complete\n",
      "train start\n",
      "Loss: 205108.841222852\n",
      "train complete\n",
      "train start\n",
      "Loss: 205093.6153069411\n",
      "train complete\n",
      "train start\n",
      "Loss: 205073.95697746932\n",
      "train complete\n",
      "train auc: 0.6968332915726205 test auc 0.6937579151507759\n",
      "train start\n",
      "Loss: 205062.71881869843\n",
      "train complete\n",
      "train start\n",
      "Loss: 205051.8464108831\n",
      "train complete\n",
      "train start\n",
      "Loss: 205040.1643385647\n",
      "train complete\n",
      "train start\n",
      "Loss: 205005.7799240039\n",
      "train complete\n",
      "train start\n",
      "Loss: 204997.08699831733\n",
      "train complete\n",
      "train start\n",
      "Loss: 204983.8676369692\n",
      "train complete\n",
      "train start\n",
      "Loss: 204964.89597366846\n",
      "train complete\n",
      "train start\n",
      "Loss: 204916.80728815758\n",
      "train complete\n",
      "train start\n",
      "Loss: 204864.80503092904\n",
      "train complete\n",
      "train start\n",
      "Loss: 204844.8508029499\n",
      "train complete\n",
      "train auc: 0.6972606611320749 test auc 0.6940920244771303\n",
      "train start\n",
      "Loss: 204833.30467663554\n",
      "train complete\n",
      "train start\n",
      "Loss: 204820.4809710686\n",
      "train complete\n",
      "train start\n",
      "Loss: 204805.19297068898\n",
      "train complete\n",
      "train start\n",
      "Loss: 204787.23898602056\n",
      "train complete\n",
      "train start\n",
      "Loss: 204768.46185661064\n",
      "train complete\n",
      "train start\n",
      "Loss: 204760.69078348193\n",
      "train complete\n",
      "train start\n",
      "Loss: 204737.75433524945\n",
      "train complete\n",
      "train start\n",
      "Loss: 204704.41755390988\n",
      "train complete\n",
      "train start\n",
      "Loss: 204665.30710493165\n",
      "train complete\n",
      "train start\n",
      "Loss: 204654.7160040685\n",
      "train complete\n",
      "train auc: 0.6978221258622824 test auc 0.6945734090213597\n",
      "train start\n",
      "Loss: 204624.56028950922\n",
      "train complete\n",
      "train start\n",
      "Loss: 204605.14306283824\n",
      "train complete\n",
      "train start\n",
      "Loss: 204597.8154096472\n",
      "train complete\n",
      "train start\n",
      "Loss: 204588.25469086913\n",
      "train complete\n",
      "train start\n",
      "Loss: 204563.27572746284\n",
      "train complete\n",
      "train start\n",
      "Loss: 204550.46510685165\n",
      "train complete\n",
      "train start\n",
      "Loss: 204541.55459414446\n",
      "train complete\n",
      "train start\n",
      "Loss: 204532.65241626668\n",
      "train complete\n",
      "train start\n",
      "Loss: 204506.90192120167\n",
      "train complete\n",
      "train start\n",
      "Loss: 204495.7322076322\n",
      "train complete\n",
      "train auc: 0.6982341087234021 test auc 0.6951585066297614\n",
      "train start\n",
      "Loss: 204481.43696171616\n",
      "train complete\n",
      "train start\n",
      "Loss: 204470.85634897774\n",
      "train complete\n",
      "train start\n",
      "Loss: 204461.85969003517\n",
      "train complete\n",
      "train start\n",
      "Loss: 204431.0899496387\n",
      "train complete\n",
      "train start\n",
      "Loss: 204338.96463769107\n",
      "train complete\n",
      "train start\n",
      "Loss: 204317.25968644812\n",
      "train complete\n",
      "train start\n",
      "Loss: 204304.38020368075\n",
      "train complete\n",
      "train start\n",
      "Loss: 204286.94186319213\n",
      "train complete\n",
      "train start\n",
      "Loss: 204263.43784447855\n",
      "train complete\n",
      "train start\n",
      "Loss: 204251.33451024367\n",
      "train complete\n",
      "train auc: 0.698685481053787 test auc 0.6954991165526206\n",
      "train start\n",
      "Loss: 204240.0147123632\n",
      "train complete\n",
      "train start\n",
      "Loss: 204232.08589948376\n",
      "train complete\n",
      "train start\n",
      "Loss: 204208.9026708109\n",
      "train complete\n",
      "train start\n",
      "Loss: 204181.5743696068\n",
      "train complete\n",
      "train start\n",
      "Loss: 204172.68096044322\n",
      "train complete\n",
      "train start\n",
      "Loss: 204136.63440005473\n",
      "train complete\n",
      "train start\n",
      "Loss: 204118.99394431856\n",
      "train complete\n",
      "train start\n",
      "Loss: 204107.14625441478\n",
      "train complete\n",
      "train start\n",
      "Loss: 204089.4635868648\n",
      "train complete\n",
      "train start\n",
      "Loss: 204074.5964029208\n",
      "train complete\n",
      "train auc: 0.6987667180938133 test auc 0.6955662700361592\n",
      "train start\n",
      "Loss: 204062.51201771552\n",
      "train complete\n",
      "train start\n",
      "Loss: 204045.88166111414\n",
      "train complete\n",
      "train start\n",
      "Loss: 204032.10213589188\n",
      "train complete\n",
      "train start\n",
      "Loss: 204019.82587846852\n",
      "train complete\n",
      "train start\n",
      "Loss: 203984.64727168935\n",
      "train complete\n",
      "train start\n",
      "Loss: 203976.46524329312\n",
      "train complete\n",
      "train start\n",
      "Loss: 203951.76687106886\n",
      "train complete\n",
      "train start\n",
      "Loss: 203944.3457945424\n",
      "train complete\n",
      "train start\n",
      "Loss: 203933.7313087252\n",
      "train complete\n",
      "train start\n",
      "Loss: 203923.93990844948\n",
      "train complete\n",
      "train auc: 0.6991179193881744 test auc 0.6960345357366214\n",
      "train start\n",
      "Loss: 203908.48696179004\n",
      "train complete\n",
      "train start\n",
      "Loss: 203892.57097299598\n",
      "train complete\n",
      "train start\n",
      "Loss: 203884.11917871764\n",
      "train complete\n",
      "train start\n",
      "Loss: 203873.33358522548\n",
      "train complete\n",
      "train start\n",
      "Loss: 203854.9962270236\n",
      "train complete\n",
      "train start\n",
      "Loss: 203842.53112900647\n",
      "train complete\n",
      "train start\n",
      "Loss: 203823.61008131903\n",
      "train complete\n",
      "train start\n",
      "Loss: 203804.94886718667\n",
      "train complete\n",
      "train start\n",
      "Loss: 203794.51563485325\n",
      "train complete\n",
      "train start\n",
      "Loss: 203782.2463658702\n",
      "train complete\n",
      "train auc: 0.6993725900349073 test auc 0.6960227236005976\n",
      "train start\n",
      "Loss: 203771.3336403088\n",
      "train complete\n",
      "train start\n",
      "Loss: 203761.01473511435\n",
      "train complete\n",
      "train start\n",
      "Loss: 203742.85564447174\n",
      "train complete\n",
      "train start\n",
      "Loss: 203726.91056559017\n",
      "train complete\n",
      "train start\n",
      "Loss: 203709.27857818748\n",
      "train complete\n",
      "train start\n",
      "Loss: 203697.0883258504\n",
      "train complete\n",
      "train start\n",
      "Loss: 203685.22290340316\n",
      "train complete\n",
      "train start\n",
      "Loss: 203674.26803062865\n",
      "train complete\n",
      "train start\n",
      "Loss: 203664.79692416856\n",
      "train complete\n",
      "train start\n",
      "Loss: 203626.24688951144\n",
      "train complete\n",
      "train auc: 0.6997280033708685 test auc 0.696293398776018\n",
      "train start\n",
      "Loss: 203616.61627835265\n",
      "train complete\n",
      "train start\n",
      "Loss: 203609.11397074026\n",
      "train complete\n",
      "train start\n",
      "Loss: 203592.98240853738\n",
      "train complete\n",
      "train start\n",
      "Loss: 203564.86503705347\n",
      "train complete\n",
      "train start\n",
      "Loss: 203549.88351424664\n",
      "train complete\n",
      "train start\n",
      "Loss: 203536.43590956897\n",
      "train complete\n",
      "train start\n",
      "Loss: 203524.30250580356\n",
      "train complete\n",
      "train start\n",
      "Loss: 203488.51406588947\n",
      "train complete\n",
      "train start\n",
      "Loss: 203446.64124724863\n",
      "train complete\n",
      "train start\n",
      "Loss: 203332.10227433572\n",
      "train complete\n",
      "train auc: 0.7003942908517123 test auc 0.6968823835654379\n",
      "train start\n",
      "Loss: 203310.68629075124\n",
      "train complete\n",
      "train start\n",
      "Loss: 203299.0694546469\n",
      "train complete\n",
      "train start\n",
      "Loss: 203281.8840022416\n",
      "train complete\n",
      "train start\n",
      "Loss: 203262.5877250936\n",
      "train complete\n",
      "train start\n",
      "Loss: 203239.15850434132\n",
      "train complete\n",
      "train start\n",
      "Loss: 203225.14957125075\n",
      "train complete\n",
      "train start\n",
      "Loss: 203216.18993584273\n",
      "train complete\n",
      "train start\n",
      "Loss: 203204.00580211796\n",
      "train complete\n",
      "train start\n",
      "Loss: 203186.2956134634\n",
      "train complete\n",
      "train start\n",
      "Loss: 203175.95452195476\n",
      "train complete\n",
      "train auc: 0.7007817923794298 test auc 0.6968934082257268\n",
      "train start\n",
      "Loss: 203167.41606267996\n",
      "train complete\n",
      "train start\n",
      "Loss: 203085.88537505397\n",
      "train complete\n",
      "train start\n",
      "Loss: 203064.56685549818\n",
      "train complete\n",
      "train start\n",
      "Loss: 203045.4292206911\n",
      "train complete\n",
      "train start\n",
      "Loss: 203035.3910786874\n",
      "train complete\n",
      "train start\n",
      "Loss: 203001.25522866502\n",
      "train complete\n",
      "train start\n",
      "Loss: 202991.1074127916\n",
      "train complete\n",
      "train start\n",
      "Loss: 202984.70180266717\n",
      "train complete\n",
      "train start\n",
      "Loss: 202975.79213608516\n",
      "train complete\n",
      "train start\n",
      "Loss: 202969.45801530126\n",
      "train complete\n",
      "train auc: 0.7012283515384026 test auc 0.6975229731143234\n",
      "train start\n",
      "Loss: 202959.19215867063\n",
      "train complete\n",
      "train start\n",
      "Loss: 202943.92287523267\n",
      "train complete\n",
      "train start\n",
      "Loss: 202933.59644325325\n",
      "train complete\n",
      "train start\n",
      "Loss: 202897.92218367805\n",
      "train complete\n",
      "train start\n",
      "Loss: 202883.76338336032\n",
      "train complete\n",
      "train start\n",
      "Loss: 202876.47601352498\n",
      "train complete\n",
      "train start\n",
      "Loss: 202865.66762575676\n",
      "train complete\n",
      "train start\n",
      "Loss: 202853.7345173362\n",
      "train complete\n",
      "train start\n",
      "Loss: 202842.31561650994\n",
      "train complete\n",
      "train start\n",
      "Loss: 202832.36660399588\n",
      "train complete\n",
      "train auc: 0.7013312631160824 test auc 0.6974306031518229\n",
      "train start\n",
      "Loss: 202825.70191472358\n",
      "train complete\n",
      "train start\n",
      "Loss: 202816.78345960146\n",
      "train complete\n",
      "train start\n",
      "Loss: 202781.73586045066\n",
      "train complete\n",
      "train start\n",
      "Loss: 202774.28123433804\n",
      "train complete\n",
      "train start\n",
      "Loss: 202752.8673177297\n",
      "train complete\n",
      "train start\n",
      "Loss: 202734.69532653067\n",
      "train complete\n",
      "train start\n",
      "Loss: 202724.55811339308\n",
      "train complete\n",
      "train start\n",
      "Loss: 202707.33317970636\n",
      "train complete\n",
      "train start\n",
      "Loss: 202692.58154454015\n",
      "train complete\n",
      "train start\n",
      "Loss: 202676.0679510859\n",
      "train complete\n",
      "train auc: 0.701577551209042 test auc 0.6978105539192165\n",
      "train start\n",
      "Loss: 202663.26011044034\n",
      "train complete\n",
      "train start\n",
      "Loss: 202653.98352589272\n",
      "train complete\n",
      "train start\n",
      "Loss: 202643.19529967444\n",
      "train complete\n",
      "train start\n",
      "Loss: 202627.54556951518\n",
      "train complete\n",
      "train start\n",
      "Loss: 202613.7042395933\n",
      "train complete\n",
      "train start\n",
      "Loss: 202600.2745351621\n",
      "train complete\n",
      "train start\n",
      "Loss: 202590.23199001566\n",
      "train complete\n",
      "train start\n",
      "Loss: 202574.972153733\n",
      "train complete\n",
      "train start\n",
      "Loss: 202569.64175157584\n",
      "train complete\n",
      "train start\n",
      "Loss: 202554.69190006176\n",
      "train complete\n",
      "train auc: 0.7019892600778986 test auc 0.698118877336944\n",
      "train start\n",
      "Loss: 202549.71558520934\n",
      "train complete\n",
      "train start\n",
      "Loss: 202538.12894820236\n",
      "train complete\n",
      "train start\n",
      "Loss: 202527.08956540044\n",
      "train complete\n",
      "train start\n",
      "Loss: 202519.54771126684\n",
      "train complete\n",
      "train start\n",
      "Loss: 202510.26622678715\n",
      "train complete\n",
      "train start\n",
      "Loss: 202498.8493888646\n",
      "train complete\n",
      "train start\n",
      "Loss: 202492.29989757107\n",
      "train complete\n",
      "train start\n",
      "Loss: 202486.62537254722\n",
      "train complete\n",
      "train start\n",
      "Loss: 202478.69766247613\n",
      "train complete\n",
      "train start\n",
      "Loss: 202471.3013572403\n",
      "train complete\n",
      "train auc: 0.7021147649567087 test auc 0.6976792290066655\n",
      "train start\n",
      "Loss: 202462.67242838262\n",
      "train complete\n",
      "train start\n",
      "Loss: 202452.56480377566\n",
      "train complete\n",
      "train start\n",
      "Loss: 202440.56987287695\n",
      "train complete\n",
      "train start\n",
      "Loss: 202433.81163425036\n",
      "train complete\n",
      "train start\n",
      "Loss: 202429.63531146862\n",
      "train complete\n",
      "train start\n",
      "Loss: 202359.34461603226\n",
      "train complete\n",
      "train start\n",
      "Loss: 202345.54790063135\n",
      "train complete\n",
      "train start\n",
      "Loss: 202336.21400840415\n",
      "train complete\n",
      "train start\n",
      "Loss: 202329.10104664546\n",
      "train complete\n",
      "train start\n",
      "Loss: 202324.18947919004\n",
      "train complete\n",
      "train auc: 0.7022610981997115 test auc 0.6986665776913207\n",
      "train start\n",
      "Loss: 202265.4967040536\n",
      "train complete\n",
      "train start\n",
      "Loss: 202242.74227775558\n",
      "train complete\n",
      "train start\n",
      "Loss: 202228.22527027494\n",
      "train complete\n",
      "train start\n",
      "Loss: 202203.33704678644\n",
      "train complete\n",
      "train start\n",
      "Loss: 202191.23752304353\n",
      "train complete\n",
      "train start\n",
      "Loss: 202176.30904997783\n",
      "train complete\n",
      "train start\n",
      "Loss: 202171.60214142365\n",
      "train complete\n",
      "train start\n",
      "Loss: 202160.9135656904\n",
      "train complete\n",
      "train start\n",
      "Loss: 202143.5053771479\n",
      "train complete\n",
      "train start\n",
      "Loss: 202108.39571389792\n",
      "train complete\n",
      "train auc: 0.702798636972953 test auc 0.6990993897282669\n",
      "train start\n",
      "Loss: 202102.22431645737\n",
      "train complete\n",
      "train start\n",
      "Loss: 202094.84590953824\n",
      "train complete\n",
      "train start\n",
      "Loss: 202090.37154151686\n",
      "train complete\n",
      "train start\n",
      "Loss: 202077.88387893751\n",
      "train complete\n",
      "train start\n",
      "Loss: 202064.18252350323\n",
      "train complete\n",
      "train start\n",
      "Loss: 202050.43205197487\n",
      "train complete\n",
      "train start\n",
      "Loss: 202039.19544365935\n",
      "train complete\n",
      "train start\n",
      "Loss: 202034.59956570284\n",
      "train complete\n",
      "train start\n",
      "Loss: 202021.146511392\n",
      "train complete\n",
      "train start\n",
      "Loss: 202009.56712267725\n",
      "train complete\n",
      "train auc: 0.7030749811013522 test auc 0.6992704570666033\n",
      "train start\n",
      "Loss: 201988.70031131833\n",
      "train complete\n",
      "train start\n",
      "Loss: 201984.42644394867\n",
      "train complete\n",
      "train start\n",
      "Loss: 201976.9429725805\n",
      "train complete\n",
      "train start\n",
      "Loss: 201946.1382789463\n",
      "train complete\n",
      "train start\n",
      "Loss: 201934.6783960996\n",
      "train complete\n",
      "train start\n",
      "Loss: 201925.214348822\n",
      "train complete\n",
      "train start\n",
      "Loss: 201911.33954936505\n",
      "train complete\n",
      "train start\n",
      "Loss: 201895.3627022513\n",
      "train complete\n",
      "train start\n",
      "Loss: 201881.46493406457\n",
      "train complete\n",
      "train start\n",
      "Loss: 201873.74817197394\n",
      "train complete\n",
      "train auc: 0.7033387947764964 test auc 0.6996295648258496\n",
      "train start\n",
      "Loss: 201823.12245490495\n",
      "train complete\n",
      "train start\n",
      "Loss: 201809.89548252238\n",
      "train complete\n",
      "train start\n",
      "Loss: 201806.19850061333\n",
      "train complete\n",
      "train start\n",
      "Loss: 201794.94902713055\n",
      "train complete\n",
      "train start\n",
      "Loss: 201773.3821547924\n",
      "train complete\n",
      "train start\n",
      "Loss: 201761.7931680595\n",
      "train complete\n",
      "train start\n",
      "Loss: 201754.5810138735\n",
      "train complete\n",
      "train start\n",
      "Loss: 201735.12407706323\n",
      "train complete\n",
      "train start\n",
      "Loss: 201725.92058539664\n",
      "train complete\n",
      "train start\n",
      "Loss: 201718.19198253166\n",
      "train complete\n",
      "train auc: 0.7034657979918912 test auc 0.699374689362784\n",
      "train start\n",
      "Loss: 201711.4040232104\n",
      "train complete\n",
      "train start\n",
      "Loss: 201691.6954359545\n",
      "train complete\n",
      "train start\n",
      "Loss: 201682.63716514249\n",
      "train complete\n",
      "train start\n",
      "Loss: 201671.6025227511\n",
      "train complete\n",
      "train start\n",
      "Loss: 201662.22826702317\n",
      "train complete\n",
      "train start\n",
      "Loss: 201648.0448986472\n",
      "train complete\n",
      "train start\n",
      "Loss: 201639.751751551\n",
      "train complete\n",
      "train start\n",
      "Loss: 201630.81034300293\n",
      "train complete\n",
      "train start\n",
      "Loss: 201616.90558278986\n",
      "train complete\n",
      "train start\n",
      "Loss: 201608.75864938713\n",
      "train complete\n",
      "train auc: 0.7037020205970075 test auc 0.6997703394472197\n",
      "train start\n",
      "Loss: 201591.82067805028\n",
      "train complete\n",
      "train start\n",
      "Loss: 201582.97234955637\n",
      "train complete\n",
      "train start\n",
      "Loss: 201558.83892243108\n",
      "train complete\n",
      "train start\n",
      "Loss: 201551.68154379987\n",
      "train complete\n",
      "train start\n",
      "Loss: 201545.6728211395\n",
      "train complete\n",
      "train start\n",
      "Loss: 201540.21846026578\n",
      "train complete\n",
      "train start\n",
      "Loss: 201529.51051604652\n",
      "train complete\n",
      "train start\n",
      "Loss: 201523.02113822423\n",
      "train complete\n",
      "train start\n",
      "Loss: 201504.99295053427\n",
      "train complete\n",
      "train start\n",
      "Loss: 201496.48516939438\n",
      "train complete\n",
      "train auc: 0.7038204529651887 test auc 0.699903976589319\n",
      "train start\n",
      "Loss: 201486.19163739224\n",
      "train complete\n",
      "train start\n",
      "Loss: 201476.62452935497\n",
      "train complete\n",
      "train start\n",
      "Loss: 201469.58614284938\n",
      "train complete\n",
      "train start\n",
      "Loss: 201463.12407812453\n",
      "train complete\n",
      "train start\n",
      "Loss: 201454.34482520033\n",
      "train complete\n",
      "train start\n",
      "Loss: 201445.6532450398\n",
      "train complete\n",
      "train start\n",
      "Loss: 201431.05798336296\n",
      "train complete\n",
      "train start\n",
      "Loss: 201424.66365610214\n",
      "train complete\n",
      "train start\n",
      "Loss: 201415.11373474784\n",
      "train complete\n",
      "train start\n",
      "Loss: 201409.06510689278\n",
      "train complete\n",
      "train auc: 0.7039182989358972 test auc 0.6998387494408451\n",
      "train start\n",
      "Loss: 201395.0674274712\n",
      "train complete\n",
      "train start\n",
      "Loss: 201380.06804218155\n",
      "train complete\n",
      "train start\n",
      "Loss: 201369.23765570574\n",
      "train complete\n",
      "train start\n",
      "Loss: 201360.7788562477\n",
      "train complete\n",
      "train start\n",
      "Loss: 201354.77833302593\n",
      "train complete\n",
      "train start\n",
      "Loss: 201348.38267496627\n",
      "train complete\n",
      "train start\n",
      "Loss: 201339.21843975247\n",
      "train complete\n",
      "train start\n",
      "Loss: 201332.8137045965\n",
      "train complete\n",
      "train start\n",
      "Loss: 201318.3714173496\n",
      "train complete\n",
      "train start\n",
      "Loss: 201310.58061563483\n",
      "train complete\n",
      "train auc: 0.7040886438346886 test auc 0.7001357972100017\n",
      "train start\n",
      "Loss: 201300.17980994817\n",
      "train complete\n",
      "train start\n",
      "Loss: 201293.6042486308\n",
      "train complete\n",
      "train start\n",
      "Loss: 201286.9219682573\n",
      "train complete\n",
      "train start\n",
      "Loss: 201278.86051091578\n",
      "train complete\n",
      "train start\n",
      "Loss: 201241.53179548468\n",
      "train complete\n",
      "train start\n",
      "Loss: 201231.98258539062\n",
      "train complete\n",
      "train start\n",
      "Loss: 201221.0295610465\n",
      "train complete\n",
      "train start\n",
      "Loss: 201208.4336063425\n",
      "train complete\n",
      "train start\n",
      "Loss: 201201.77309205977\n",
      "train complete\n",
      "train start\n",
      "Loss: 201192.27619766997\n",
      "train complete\n",
      "train auc: 0.7043508019403966 test auc 0.7002682107842261\n",
      "train start\n",
      "Loss: 201160.075045025\n",
      "train complete\n",
      "train start\n",
      "Loss: 201145.64855637398\n",
      "train complete\n",
      "train start\n",
      "Loss: 201124.28784072187\n",
      "train complete\n",
      "train start\n",
      "Loss: 201108.8412919779\n",
      "train complete\n",
      "train start\n",
      "Loss: 201091.8162908325\n",
      "train complete\n",
      "train start\n",
      "Loss: 201072.40461225645\n",
      "train complete\n",
      "train start\n",
      "Loss: 201060.74758337892\n",
      "train complete\n",
      "train start\n",
      "Loss: 201048.94244456876\n",
      "train complete\n",
      "train start\n",
      "Loss: 201043.03791003276\n",
      "train complete\n",
      "train start\n",
      "Loss: 201038.04211981007\n",
      "train complete\n",
      "train auc: 0.7044271742477048 test auc 0.7003582685171782\n",
      "train start\n",
      "Loss: 201024.4711078651\n",
      "train complete\n",
      "train start\n",
      "Loss: 201011.17339165797\n",
      "train complete\n",
      "train start\n",
      "Loss: 201002.1986744183\n",
      "train complete\n",
      "train start\n",
      "Loss: 200991.73699174725\n",
      "train complete\n",
      "train start\n",
      "Loss: 200982.94622608332\n",
      "train complete\n",
      "train start\n",
      "Loss: 200976.67549971715\n",
      "train complete\n",
      "train start\n",
      "Loss: 200970.80052686087\n",
      "train complete\n",
      "train start\n",
      "Loss: 200963.94453657285\n",
      "train complete\n",
      "train start\n",
      "Loss: 200952.24403040347\n",
      "train complete\n",
      "train start\n",
      "Loss: 200947.35105428088\n",
      "train complete\n",
      "train auc: 0.7047246643739613 test auc 0.7008277405300709\n",
      "train start\n",
      "Loss: 200938.81116564892\n",
      "train complete\n",
      "train start\n",
      "Loss: 200932.807588211\n",
      "train complete\n",
      "train start\n",
      "Loss: 200924.0869800231\n",
      "train complete\n",
      "train start\n",
      "Loss: 200919.3561958629\n",
      "train complete\n",
      "train start\n",
      "Loss: 200915.71834606107\n",
      "train complete\n",
      "train start\n",
      "Loss: 200908.80678422816\n",
      "train complete\n",
      "train start\n",
      "Loss: 200903.57184269087\n",
      "train complete\n",
      "train start\n",
      "Loss: 200895.30615783972\n",
      "train complete\n",
      "train start\n",
      "Loss: 200889.2265224266\n",
      "train complete\n",
      "train start\n",
      "Loss: 200844.19154770413\n",
      "train complete\n",
      "train auc: 0.7049530776624895 test auc 0.7004029538434433\n",
      "train start\n",
      "Loss: 200823.3293906961\n",
      "train complete\n",
      "train start\n",
      "Loss: 200808.0186518758\n",
      "train complete\n",
      "train start\n",
      "Loss: 200796.2393332184\n",
      "train complete\n",
      "train start\n",
      "Loss: 200781.0871993989\n",
      "train complete\n",
      "train start\n",
      "Loss: 200769.47271709167\n",
      "train complete\n",
      "train start\n",
      "Loss: 200761.99468685628\n",
      "train complete\n",
      "train start\n",
      "Loss: 200758.1005977791\n",
      "train complete\n",
      "train start\n",
      "Loss: 200734.70488594007\n",
      "train complete\n",
      "train start\n",
      "Loss: 200727.22228552325\n",
      "train complete\n",
      "train start\n",
      "Loss: 200717.69652268727\n",
      "train complete\n",
      "train auc: 0.7051682252757838 test auc 0.7009556472959558\n",
      "train start\n",
      "Loss: 200708.6762798316\n",
      "train complete\n",
      "train start\n",
      "Loss: 200703.3552355813\n",
      "train complete\n",
      "train start\n",
      "Loss: 200698.4426769276\n",
      "train complete\n",
      "train start\n",
      "Loss: 200693.67017549934\n",
      "train complete\n",
      "train start\n",
      "Loss: 200672.92211977663\n",
      "train complete\n",
      "train start\n",
      "Loss: 200667.1289505553\n",
      "train complete\n",
      "train start\n",
      "Loss: 200660.05273888732\n",
      "train complete\n",
      "train start\n",
      "Loss: 200649.7570377838\n",
      "train complete\n",
      "train start\n",
      "Loss: 200645.0554052702\n",
      "train complete\n",
      "train start\n",
      "Loss: 200638.75213897674\n",
      "train complete\n",
      "train auc: 0.7053475549141478 test auc 0.7008363525654987\n",
      "train start\n",
      "Loss: 200626.2092754946\n",
      "train complete\n",
      "train start\n",
      "Loss: 200612.89127251954\n",
      "train complete\n",
      "train start\n",
      "Loss: 200606.15300441085\n",
      "train complete\n",
      "train start\n",
      "Loss: 200599.10445244066\n",
      "train complete\n",
      "train start\n",
      "Loss: 200588.91202983807\n",
      "train complete\n",
      "train start\n",
      "Loss: 200584.56070193066\n",
      "train complete\n",
      "train start\n",
      "Loss: 200575.60239149397\n",
      "train complete\n",
      "train start\n",
      "Loss: 200553.14500684798\n",
      "train complete\n",
      "train start\n",
      "Loss: 200547.74045326564\n",
      "train complete\n",
      "train start\n",
      "Loss: 200543.69834837617\n",
      "train complete\n",
      "train auc: 0.7054425877531847 test auc 0.7009740274815854\n",
      "train start\n",
      "Loss: 200526.28828523378\n",
      "train complete\n",
      "train start\n",
      "Loss: 200520.73957532804\n",
      "train complete\n",
      "train start\n",
      "Loss: 200516.15319728572\n",
      "train complete\n",
      "train start\n",
      "Loss: 200508.8834471151\n",
      "train complete\n",
      "train start\n",
      "Loss: 200480.32673848438\n",
      "train complete\n",
      "train start\n",
      "Loss: 200472.41807548018\n",
      "train complete\n",
      "train start\n",
      "Loss: 200467.93993106033\n",
      "train complete\n",
      "train start\n",
      "Loss: 200459.12788201318\n",
      "train complete\n",
      "train start\n",
      "Loss: 200450.23078698866\n",
      "train complete\n",
      "train start\n",
      "Loss: 200445.33139441823\n",
      "train complete\n",
      "train auc: 0.7056010748625621 test auc 0.7012054292655724\n",
      "train start\n",
      "Loss: 200438.2105449777\n",
      "train complete\n",
      "train start\n",
      "Loss: 200430.8623953656\n",
      "train complete\n",
      "train start\n",
      "Loss: 200423.33212025109\n",
      "train complete\n",
      "train start\n",
      "Loss: 200418.1858716854\n",
      "train complete\n",
      "train start\n",
      "Loss: 200413.5413624703\n",
      "train complete\n",
      "train start\n",
      "Loss: 200403.69643503084\n",
      "train complete\n",
      "train start\n",
      "Loss: 200394.99270233652\n",
      "train complete\n",
      "train start\n",
      "Loss: 200369.91147410977\n",
      "train complete\n",
      "train start\n",
      "Loss: 200366.01639719086\n",
      "train complete\n",
      "train start\n",
      "Loss: 200356.96124649508\n",
      "train complete\n",
      "train auc: 0.7056875163536686 test auc 0.7012500643941811\n",
      "train start\n",
      "Loss: 200350.154639855\n",
      "train complete\n",
      "train start\n",
      "Loss: 200343.8626617327\n",
      "train complete\n",
      "train start\n",
      "Loss: 200337.19003107763\n",
      "train complete\n",
      "train start\n",
      "Loss: 200334.1242295572\n",
      "train complete\n",
      "train start\n",
      "Loss: 200321.91327183854\n",
      "train complete\n",
      "train start\n",
      "Loss: 200313.23985318936\n",
      "train complete\n",
      "train start\n",
      "Loss: 200309.40980006728\n",
      "train complete\n",
      "train start\n",
      "Loss: 200303.82748399547\n",
      "train complete\n",
      "train start\n",
      "Loss: 200294.00673848612\n",
      "train complete\n",
      "train start\n",
      "Loss: 200288.24228976964\n",
      "train complete\n",
      "train auc: 0.7059131685998243 test auc 0.7014211317325175\n",
      "train start\n",
      "Loss: 200285.58908588972\n",
      "train complete\n",
      "train start\n",
      "Loss: 200282.6193152745\n",
      "train complete\n",
      "train start\n",
      "Loss: 200267.24791076285\n",
      "train complete\n",
      "train start\n",
      "Loss: 200259.897847285\n",
      "train complete\n",
      "train start\n",
      "Loss: 200255.28820006977\n",
      "train complete\n",
      "train start\n",
      "Loss: 200248.82166678333\n",
      "train complete\n",
      "train start\n",
      "Loss: 200242.20939460382\n",
      "train complete\n",
      "train start\n",
      "Loss: 200236.56860989283\n",
      "train complete\n",
      "train start\n",
      "Loss: 200228.51387430308\n",
      "train complete\n",
      "train start\n",
      "Loss: 200221.97617092327\n",
      "train complete\n",
      "train auc: 0.7059521862341518 test auc 0.7016573587662269\n",
      "train start\n",
      "Loss: 200212.88315453875\n",
      "train complete\n",
      "train start\n",
      "Loss: 200202.8008335424\n",
      "train complete\n",
      "train start\n",
      "Loss: 200184.82975533992\n",
      "train complete\n",
      "train start\n",
      "Loss: 200178.61595074183\n",
      "train complete\n",
      "train start\n",
      "Loss: 200172.53386643168\n",
      "train complete\n",
      "train start\n",
      "Loss: 200166.18763749662\n",
      "train complete\n",
      "train start\n",
      "Loss: 200158.15634892258\n",
      "train complete\n",
      "train start\n",
      "Loss: 200153.7719310801\n",
      "train complete\n",
      "train start\n",
      "Loss: 200143.91644479884\n",
      "train complete\n",
      "train start\n",
      "Loss: 200138.30402917558\n",
      "train complete\n",
      "train auc: 0.7060745108982941 test auc 0.7017509852388144\n",
      "train start\n",
      "Loss: 200128.88267575103\n",
      "train complete\n",
      "train start\n",
      "Loss: 200122.52932288725\n",
      "train complete\n",
      "train start\n",
      "Loss: 200115.25983872332\n",
      "train complete\n",
      "train start\n",
      "Loss: 200110.39926772422\n",
      "train complete\n",
      "train start\n",
      "Loss: 200093.96329905995\n",
      "train complete\n",
      "train start\n",
      "Loss: 200088.215649821\n",
      "train complete\n",
      "train start\n",
      "Loss: 200074.0482153915\n",
      "train complete\n",
      "train start\n",
      "Loss: 200063.40969390344\n",
      "train complete\n",
      "train start\n",
      "Loss: 200061.03361874013\n",
      "train complete\n",
      "train start\n",
      "Loss: 200037.27677716943\n",
      "train complete\n",
      "train auc: 0.7063291347356637 test auc 0.7024470337859718\n",
      "train start\n",
      "Loss: 200027.98751732067\n",
      "train complete\n",
      "train start\n",
      "Loss: 200022.9431760165\n",
      "train complete\n",
      "train start\n",
      "Loss: 200017.41372474603\n",
      "train complete\n",
      "train start\n",
      "Loss: 200010.51166517052\n",
      "train complete\n",
      "train start\n",
      "Loss: 200007.53107812398\n",
      "train complete\n",
      "train start\n",
      "Loss: 199998.60064932317\n",
      "train complete\n",
      "train start\n",
      "Loss: 199994.1354006487\n",
      "train complete\n",
      "train start\n",
      "Loss: 199986.13185160363\n",
      "train complete\n",
      "train start\n",
      "Loss: 199979.37606052074\n",
      "train complete\n",
      "train start\n",
      "Loss: 199974.4210606732\n",
      "train complete\n",
      "train auc: 0.7063035931547081 test auc 0.7022459247189511\n",
      "train start\n",
      "Loss: 199969.48395752467\n",
      "train complete\n",
      "train start\n",
      "Loss: 199960.0842288854\n",
      "train complete\n",
      "train start\n",
      "Loss: 199956.65935513054\n",
      "train complete\n",
      "train start\n",
      "Loss: 199951.58131008505\n",
      "train complete\n",
      "train start\n",
      "Loss: 199944.9341706586\n",
      "train complete\n",
      "train start\n",
      "Loss: 199940.0772399344\n",
      "train complete\n",
      "train start\n",
      "Loss: 199934.78257338007\n",
      "train complete\n",
      "train start\n",
      "Loss: 199930.55327702963\n",
      "train complete\n",
      "train start\n",
      "Loss: 199926.81486754032\n",
      "train complete\n",
      "train start\n",
      "Loss: 199921.22292182242\n",
      "train complete\n",
      "train auc: 0.7064941879005715 test auc 0.7021423795031925\n",
      "train start\n",
      "Loss: 199905.94681589107\n",
      "train complete\n",
      "train start\n",
      "Loss: 199898.90365237926\n",
      "train complete\n",
      "train start\n",
      "Loss: 199894.1416768653\n",
      "train complete\n",
      "train start\n",
      "Loss: 199889.39908002908\n",
      "train complete\n",
      "train start\n",
      "Loss: 199885.06035045316\n",
      "train complete\n",
      "train start\n",
      "Loss: 199879.65346358754\n",
      "train complete\n",
      "train start\n",
      "Loss: 199857.30387758015\n",
      "train complete\n",
      "train start\n",
      "Loss: 199850.9324125766\n",
      "train complete\n",
      "train start\n",
      "Loss: 199847.28894116136\n",
      "train complete\n",
      "train start\n",
      "Loss: 199840.4801213257\n",
      "train complete\n",
      "train auc: 0.7065328084473015 test auc 0.7023825611710208\n",
      "train start\n",
      "Loss: 199835.9955667662\n",
      "train complete\n",
      "train start\n",
      "Loss: 199828.88310609933\n",
      "train complete\n",
      "train start\n",
      "Loss: 199824.82429941697\n",
      "train complete\n",
      "train start\n",
      "Loss: 199816.16589206413\n",
      "train complete\n",
      "train start\n",
      "Loss: 199812.7546795686\n",
      "train complete\n",
      "train start\n",
      "Loss: 199804.8385392963\n",
      "train complete\n",
      "train start\n",
      "Loss: 199796.64169305682\n",
      "train complete\n",
      "train start\n",
      "Loss: 199778.20683015496\n",
      "train complete\n",
      "train start\n",
      "Loss: 199771.5809518837\n",
      "train complete\n",
      "train start\n",
      "Loss: 199756.17915317928\n",
      "train complete\n",
      "train auc: 0.7066298681627192 test auc 0.7028744009458745\n",
      "train start\n",
      "Loss: 199745.71614519297\n",
      "train complete\n",
      "train start\n",
      "Loss: 199738.21589638924\n",
      "train complete\n",
      "train start\n",
      "Loss: 199731.92428211885\n",
      "train complete\n",
      "train start\n",
      "Loss: 199725.5427170533\n",
      "train complete\n",
      "train start\n",
      "Loss: 199714.99263779089\n",
      "train complete\n",
      "train start\n",
      "Loss: 199705.90616411707\n",
      "train complete\n",
      "train start\n",
      "Loss: 199698.88296980652\n",
      "train complete\n",
      "train start\n",
      "Loss: 199683.95627776854\n",
      "train complete\n",
      "train start\n",
      "Loss: 199665.1438280873\n",
      "train complete\n",
      "train start\n",
      "Loss: 199659.93375369968\n",
      "train complete\n",
      "train auc: 0.706724577560162 test auc 0.7027543187396823\n",
      "train start\n",
      "Loss: 199650.03508856904\n",
      "train complete\n",
      "train start\n",
      "Loss: 199634.30962668627\n",
      "train complete\n",
      "train start\n",
      "Loss: 199628.22438322203\n",
      "train complete\n",
      "train start\n",
      "Loss: 199611.73851826173\n",
      "train complete\n",
      "train start\n",
      "Loss: 199604.66079936977\n",
      "train complete\n",
      "train start\n",
      "Loss: 199596.29265355098\n",
      "train complete\n",
      "train start\n",
      "Loss: 199590.2812658298\n",
      "train complete\n",
      "train start\n",
      "Loss: 199575.3639932662\n",
      "train complete\n",
      "train start\n",
      "Loss: 199569.55006077903\n",
      "train complete\n",
      "train start\n",
      "Loss: 199557.7134863731\n",
      "train complete\n",
      "train auc: 0.7069141599238221 test auc 0.7027316325363389\n",
      "train start\n",
      "Loss: 199552.3914708725\n",
      "train complete\n",
      "train start\n",
      "Loss: 199546.55845298193\n",
      "train complete\n",
      "train start\n",
      "Loss: 199541.09376135198\n",
      "train complete\n",
      "train start\n",
      "Loss: 199519.62847750285\n",
      "train complete\n",
      "train start\n",
      "Loss: 199513.7649006011\n",
      "train complete\n",
      "train start\n",
      "Loss: 199506.02345964027\n",
      "train complete\n",
      "train start\n",
      "Loss: 199487.39967981292\n",
      "train complete\n",
      "train start\n",
      "Loss: 199483.67023606933\n",
      "train complete\n",
      "train start\n",
      "Loss: 199468.073740553\n",
      "train complete\n",
      "train start\n",
      "Loss: 199463.45162952942\n",
      "train complete\n",
      "train auc: 0.7072824513513046 test auc 0.7029364107383078\n",
      "train start\n",
      "Loss: 199455.66359453302\n",
      "train complete\n",
      "train start\n",
      "Loss: 199445.6597582946\n",
      "train complete\n",
      "train start\n",
      "Loss: 199440.4336513343\n",
      "train complete\n",
      "train start\n",
      "Loss: 199436.2886219279\n",
      "train complete\n",
      "train start\n",
      "Loss: 199432.1150101355\n",
      "train complete\n",
      "train start\n",
      "Loss: 199428.74394721194\n",
      "train complete\n",
      "train start\n",
      "Loss: 199421.66484807\n",
      "train complete\n",
      "train start\n",
      "Loss: 199402.04920757518\n",
      "train complete\n",
      "train start\n",
      "Loss: 199397.07557185227\n",
      "train complete\n",
      "train start\n",
      "Loss: 199392.58268678232\n",
      "train complete\n",
      "train auc: 0.7074999703112843 test auc 0.7029982699377719\n",
      "train start\n",
      "Loss: 199385.00391921223\n",
      "train complete\n",
      "train start\n",
      "Loss: 199380.55986646147\n",
      "train complete\n",
      "train start\n",
      "Loss: 199376.14338192975\n",
      "train complete\n",
      "train start\n",
      "Loss: 199366.91761439588\n",
      "train complete\n",
      "train start\n",
      "Loss: 199362.85577717563\n",
      "train complete\n",
      "train start\n",
      "Loss: 199344.80583902518\n",
      "train complete\n",
      "train start\n",
      "Loss: 199337.93053840983\n",
      "train complete\n",
      "train start\n",
      "Loss: 199331.03392418128\n",
      "train complete\n",
      "train start\n",
      "Loss: 199326.76227078668\n",
      "train complete\n",
      "train start\n",
      "Loss: 199320.3048482916\n",
      "train complete\n",
      "train auc: 0.707570958849963 test auc 0.7034670548702426\n",
      "train start\n",
      "Loss: 199313.67127348806\n",
      "train complete\n",
      "train start\n",
      "Loss: 199309.0948919635\n",
      "train complete\n",
      "train start\n",
      "Loss: 199301.28284286478\n",
      "train complete\n",
      "train start\n",
      "Loss: 199295.69871821243\n",
      "train complete\n",
      "train start\n",
      "Loss: 199289.9133159815\n",
      "train complete\n",
      "train start\n",
      "Loss: 199285.71103230678\n",
      "train complete\n",
      "train start\n",
      "Loss: 199278.65635373897\n",
      "train complete\n",
      "train start\n",
      "Loss: 199274.08443881708\n",
      "train complete\n",
      "train start\n",
      "Loss: 199265.97192724247\n",
      "train complete\n",
      "train start\n",
      "Loss: 199216.5588112346\n",
      "train complete\n",
      "train auc: 0.7077701196910975 test auc 0.7034396940101459\n",
      "train start\n",
      "Loss: 199191.84157792054\n",
      "train complete\n",
      "train start\n",
      "Loss: 199184.5577024024\n",
      "train complete\n",
      "train start\n",
      "Loss: 199173.59611252954\n",
      "train complete\n",
      "train start\n",
      "Loss: 199158.9012743817\n",
      "train complete\n",
      "train start\n",
      "Loss: 199153.72008529116\n",
      "train complete\n",
      "train start\n",
      "Loss: 199148.4659705144\n",
      "train complete\n",
      "train start\n",
      "Loss: 199135.99322679598\n",
      "train complete\n",
      "train start\n",
      "Loss: 199129.67450631774\n",
      "train complete\n",
      "train start\n",
      "Loss: 199121.9796320636\n",
      "train complete\n",
      "train start\n",
      "Loss: 199115.51593818047\n",
      "train complete\n",
      "train auc: 0.7079750177470573 test auc 0.7038003767208619\n",
      "train start\n",
      "Loss: 199111.3762397015\n",
      "train complete\n",
      "train start\n",
      "Loss: 199107.98987141348\n",
      "train complete\n",
      "train start\n",
      "Loss: 199098.7714770182\n",
      "train complete\n",
      "train start\n",
      "Loss: 199094.00656270978\n",
      "train complete\n",
      "train start\n",
      "Loss: 199091.08949968615\n",
      "train complete\n",
      "train start\n",
      "Loss: 199086.5700299122\n",
      "train complete\n",
      "train start\n",
      "Loss: 199080.32642051578\n",
      "train complete\n",
      "train start\n",
      "Loss: 199077.18507797323\n",
      "train complete\n",
      "train start\n",
      "Loss: 199072.56953240233\n",
      "train complete\n",
      "train start\n",
      "Loss: 199068.30565265025\n",
      "train complete\n",
      "train auc: 0.7080703009096787 test auc 0.7036587142284442\n",
      "train start\n",
      "Loss: 199058.27347203458\n",
      "train complete\n",
      "train start\n",
      "Loss: 199050.24061604394\n",
      "train complete\n",
      "train start\n",
      "Loss: 199044.64710706368\n",
      "train complete\n",
      "train start\n",
      "Loss: 199040.2794221772\n",
      "train complete\n",
      "train start\n",
      "Loss: 199029.5859854396\n",
      "train complete\n",
      "train start\n",
      "Loss: 199023.6367628958\n",
      "train complete\n",
      "train start\n",
      "Loss: 199011.1775125026\n",
      "train complete\n",
      "train start\n",
      "Loss: 198994.52433520756\n",
      "train complete\n",
      "train start\n",
      "Loss: 198988.6515982087\n",
      "train complete\n",
      "train start\n",
      "Loss: 198977.5464429015\n",
      "train complete\n",
      "train auc: 0.7081617885630267 test auc 0.7039611566769879\n",
      "train start\n",
      "Loss: 198970.96824787415\n",
      "train complete\n",
      "train start\n",
      "Loss: 198963.2775548016\n",
      "train complete\n",
      "train start\n",
      "Loss: 198958.41793757633\n",
      "train complete\n",
      "train start\n",
      "Loss: 198953.65229369907\n",
      "train complete\n",
      "train start\n",
      "Loss: 198945.7686999146\n",
      "train complete\n",
      "train start\n",
      "Loss: 198935.01471915707\n",
      "train complete\n",
      "train start\n",
      "Loss: 198866.11319141832\n",
      "train complete\n",
      "train start\n",
      "Loss: 198858.03844932505\n",
      "train complete\n",
      "train start\n",
      "Loss: 198843.53325272055\n",
      "train complete\n",
      "train start\n",
      "Loss: 198828.47579793766\n",
      "train complete\n",
      "train auc: 0.7086021708341624 test auc 0.7039338460145478\n",
      "train start\n",
      "Loss: 198818.59509560504\n",
      "train complete\n",
      "train start\n",
      "Loss: 198813.33351987228\n",
      "train complete\n",
      "train start\n",
      "Loss: 198807.88321856057\n",
      "train complete\n",
      "train start\n",
      "Loss: 198799.39148795907\n",
      "train complete\n",
      "train start\n",
      "Loss: 198784.72911265877\n",
      "train complete\n",
      "train start\n",
      "Loss: 198778.31329757677\n",
      "train complete\n",
      "train start\n",
      "Loss: 198770.78046411116\n",
      "train complete\n",
      "train start\n",
      "Loss: 198758.55074109277\n",
      "train complete\n",
      "train start\n",
      "Loss: 198754.1291976198\n",
      "train complete\n",
      "train start\n",
      "Loss: 198731.0408838327\n",
      "train complete\n",
      "train auc: 0.7087429230392044 test auc 0.7037784779932532\n",
      "train start\n",
      "Loss: 198723.50655123964\n",
      "train complete\n",
      "train start\n",
      "Loss: 198718.0820963336\n",
      "train complete\n",
      "train start\n",
      "Loss: 198710.40133618424\n",
      "train complete\n",
      "train start\n",
      "Loss: 198703.00191650543\n",
      "train complete\n",
      "train start\n",
      "Loss: 198697.6479132874\n",
      "train complete\n",
      "train start\n",
      "Loss: 198675.79544113876\n",
      "train complete\n",
      "train start\n",
      "Loss: 198670.56525494118\n",
      "train complete\n",
      "train start\n",
      "Loss: 198665.50995831346\n",
      "train complete\n",
      "train start\n",
      "Loss: 198660.73790381468\n",
      "train complete\n",
      "train start\n",
      "Loss: 198654.24935582455\n",
      "train complete\n",
      "train auc: 0.7086992074088952 test auc 0.7042561934025345\n",
      "train start\n",
      "Loss: 198648.9956940746\n",
      "train complete\n",
      "train start\n",
      "Loss: 198643.3378285251\n",
      "train complete\n",
      "train start\n",
      "Loss: 198640.13616180423\n",
      "train complete\n",
      "train start\n",
      "Loss: 198631.6373137695\n",
      "train complete\n",
      "train start\n",
      "Loss: 198608.61112258912\n",
      "train complete\n",
      "train start\n",
      "Loss: 198482.92096768646\n",
      "train complete\n",
      "train start\n",
      "Loss: 198470.71741256418\n",
      "train complete\n",
      "train start\n",
      "Loss: 198464.450068652\n",
      "train complete\n",
      "train start\n",
      "Loss: 198458.29157292718\n",
      "train complete\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "gsgp = GStackGP(X_train,y_train,1)\n",
    "for i in range(1000):\n",
    "    print(\"train start\")\n",
    "    gsgp.evolve()\n",
    "    print(\"train complete\")\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        pred = gsgp.predict(X_train)\n",
    "        train_acc.append(roc_auc_score(y_train,pred))\n",
    "        pred = gsgp.predict(X_test)\n",
    "        test_acc.append(roc_auc_score(y_test,pred))\n",
    "        print(\"train auc:\",train_acc[-1],\"test auc\",test_acc[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae26b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5258654810848914,\n",
       "  0.6132988188301514,\n",
       "  0.6436750800551445,\n",
       "  0.6493980630859355,\n",
       "  0.6538173147351156,\n",
       "  0.6565614302154482,\n",
       "  0.6582152004133821,\n",
       "  0.6613213626863199,\n",
       "  0.6635383899160323,\n",
       "  0.666353789575044,\n",
       "  0.668103252695043,\n",
       "  0.6701625265444092,\n",
       "  0.6712143528187084,\n",
       "  0.6733713747050745,\n",
       "  0.6746181955322843,\n",
       "  0.6752672766485888,\n",
       "  0.6766407141634677,\n",
       "  0.6773970382984317,\n",
       "  0.6779983184430831,\n",
       "  0.6790438206285722,\n",
       "  0.6801295273961986,\n",
       "  0.6814559134172302,\n",
       "  0.6821680827768668,\n",
       "  0.6826158873088717,\n",
       "  0.6833402715090398,\n",
       "  0.6840135841631556,\n",
       "  0.6850787003093606,\n",
       "  0.6858402956774915,\n",
       "  0.68620554784639,\n",
       "  0.6870865464354777,\n",
       "  0.6873719000281152,\n",
       "  0.6887273207264919,\n",
       "  0.6894809947705653,\n",
       "  0.6904194369508995,\n",
       "  0.6906725440629418,\n",
       "  0.6909134399393745,\n",
       "  0.691144665303587,\n",
       "  0.6920069471387315,\n",
       "  0.6926548970014472,\n",
       "  0.6932927787363518,\n",
       "  0.69436308464085,\n",
       "  0.6945680994562209,\n",
       "  0.695180881395156,\n",
       "  0.695675791622282,\n",
       "  0.6959153969002998,\n",
       "  0.6963643926040635,\n",
       "  0.6968332915726205,\n",
       "  0.6972606611320749,\n",
       "  0.6978221258622824,\n",
       "  0.6982341087234021,\n",
       "  0.698685481053787,\n",
       "  0.6987667180938133,\n",
       "  0.6991179193881744,\n",
       "  0.6993725900349073,\n",
       "  0.6997280033708685,\n",
       "  0.7003942908517123,\n",
       "  0.7007817923794298,\n",
       "  0.7012283515384026,\n",
       "  0.7013312631160824,\n",
       "  0.701577551209042,\n",
       "  0.7019892600778986,\n",
       "  0.7021147649567087,\n",
       "  0.7022610981997115,\n",
       "  0.702798636972953,\n",
       "  0.7030749811013522,\n",
       "  0.7033387947764964,\n",
       "  0.7034657979918912,\n",
       "  0.7037020205970075,\n",
       "  0.7038204529651887,\n",
       "  0.7039182989358972,\n",
       "  0.7040886438346886,\n",
       "  0.7043508019403966,\n",
       "  0.7044271742477048,\n",
       "  0.7047246643739613,\n",
       "  0.7049530776624895,\n",
       "  0.7051682252757838,\n",
       "  0.7053475549141478,\n",
       "  0.7054425877531847,\n",
       "  0.7056010748625621,\n",
       "  0.7056875163536686,\n",
       "  0.7059131685998243,\n",
       "  0.7059521862341518,\n",
       "  0.7060745108982941,\n",
       "  0.7063291347356637,\n",
       "  0.7063035931547081,\n",
       "  0.7064941879005715,\n",
       "  0.7065328084473015,\n",
       "  0.7066298681627192,\n",
       "  0.706724577560162,\n",
       "  0.7069141599238221,\n",
       "  0.7072824513513046,\n",
       "  0.7074999703112843,\n",
       "  0.707570958849963,\n",
       "  0.7077701196910975,\n",
       "  0.7079750177470573,\n",
       "  0.7080703009096787,\n",
       "  0.7081617885630267,\n",
       "  0.7086021708341624,\n",
       "  0.7087429230392044,\n",
       "  0.7086992074088952],\n",
       " [0.5246813939060924,\n",
       "  0.6120390153127323,\n",
       "  0.6439444245431653,\n",
       "  0.6495549522843039,\n",
       "  0.6532618358701032,\n",
       "  0.656305261737169,\n",
       "  0.6579249565743212,\n",
       "  0.660116813711289,\n",
       "  0.6622464775206425,\n",
       "  0.6663355080213031,\n",
       "  0.6680409702604613,\n",
       "  0.6692936653255734,\n",
       "  0.6705694497439576,\n",
       "  0.6728249763333738,\n",
       "  0.6733139846466702,\n",
       "  0.6740689934786342,\n",
       "  0.6750838034186988,\n",
       "  0.6761953897342905,\n",
       "  0.6771798238175204,\n",
       "  0.6778245515357063,\n",
       "  0.679246204068909,\n",
       "  0.6801229704538477,\n",
       "  0.680606617029969,\n",
       "  0.6820480066542014,\n",
       "  0.6823997761435513,\n",
       "  0.683068245784542,\n",
       "  0.6839835655382799,\n",
       "  0.6844823420017783,\n",
       "  0.6846477966146576,\n",
       "  0.6850633343830919,\n",
       "  0.6859587491973865,\n",
       "  0.6872866246650327,\n",
       "  0.6877930076421539,\n",
       "  0.6884289727319428,\n",
       "  0.6882337948317622,\n",
       "  0.6888231984578779,\n",
       "  0.6891271154625787,\n",
       "  0.689308670973751,\n",
       "  0.6898439395647826,\n",
       "  0.6907313792264151,\n",
       "  0.6920086209455117,\n",
       "  0.6923587982279475,\n",
       "  0.6917311267322035,\n",
       "  0.6923386752424342,\n",
       "  0.6930348241849045,\n",
       "  0.6931861042345551,\n",
       "  0.6937579151507759,\n",
       "  0.6940920244771303,\n",
       "  0.6945734090213597,\n",
       "  0.6951585066297614,\n",
       "  0.6954991165526206,\n",
       "  0.6955662700361592,\n",
       "  0.6960345357366214,\n",
       "  0.6960227236005976,\n",
       "  0.696293398776018,\n",
       "  0.6968823835654379,\n",
       "  0.6968934082257268,\n",
       "  0.6975229731143234,\n",
       "  0.6974306031518229,\n",
       "  0.6978105539192165,\n",
       "  0.698118877336944,\n",
       "  0.6976792290066655,\n",
       "  0.6986665776913207,\n",
       "  0.6990993897282669,\n",
       "  0.6992704570666033,\n",
       "  0.6996295648258496,\n",
       "  0.699374689362784,\n",
       "  0.6997703394472197,\n",
       "  0.699903976589319,\n",
       "  0.6998387494408451,\n",
       "  0.7001357972100017,\n",
       "  0.7002682107842261,\n",
       "  0.7003582685171782,\n",
       "  0.7008277405300709,\n",
       "  0.7004029538434433,\n",
       "  0.7009556472959558,\n",
       "  0.7008363525654987,\n",
       "  0.7009740274815854,\n",
       "  0.7012054292655724,\n",
       "  0.7012500643941811,\n",
       "  0.7014211317325175,\n",
       "  0.7016573587662269,\n",
       "  0.7017509852388144,\n",
       "  0.7024470337859718,\n",
       "  0.7022459247189511,\n",
       "  0.7021423795031925,\n",
       "  0.7023825611710208,\n",
       "  0.7028744009458745,\n",
       "  0.7027543187396823,\n",
       "  0.7027316325363389,\n",
       "  0.7029364107383078,\n",
       "  0.7029982699377719,\n",
       "  0.7034670548702426,\n",
       "  0.7034396940101459,\n",
       "  0.7038003767208619,\n",
       "  0.7036587142284442,\n",
       "  0.7039611566769879,\n",
       "  0.7039338460145478,\n",
       "  0.7037784779932532,\n",
       "  0.7042561934025345])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc,test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_gp",
   "language": "python",
   "name": "tensor_gp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "896880215cde5e09b41efabef2e239b6cc79dad31f917f21992b8447ccaaac59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
