{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8bc990-ebc5-46bd-bab4-f4091ce5b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../script/\")\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import importlib\n",
    "from time import time\n",
    "import Functions\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8230662c-fa72-42c7-bf5e-919cbf89080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from Node import Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbad47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    print(roc_auc_score(true_y,prob[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57308354",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44cce456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    def __init__(self,opset,X,Y,Y_one_hot,residual,log_odds,p,learning_rate,max_depth):\n",
    "        self.generation = 0\n",
    "        \n",
    "        X = X.astype('float64')\n",
    "        self.opset = opset\n",
    "        \n",
    "        self.feature_space = X.shape[1]\n",
    "        \n",
    "        self.vals = X.T\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Y_one_hot = Y_one_hot\n",
    "\n",
    "        self.residual = residual\n",
    "        self.log_odds = log_odds\n",
    "        self.p = p\n",
    "        self.mask = np.ones(p.shape[0])\n",
    "        self.sample_weights = np.sum(np.abs(np.array(residual)),axis=1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.best = (np.inf,None)\n",
    "        self.nodes = [Node(True,index=i) for i in range(self.feature_space)]\n",
    "    \n",
    "    \n",
    "    def calculate_decision(self,val):\n",
    "\n",
    "        clf = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "        clf.fit(val,self.Y,sample_weight=self.sample_weights)\n",
    "        \n",
    "        bins = len(clf.tree_.threshold)\n",
    "        index = clf.apply(val)\n",
    "        \n",
    "        residual_bin = np.stack([np.sum(self.residual[index==i],axis=0) for i in range(bins)])\n",
    "        p_bin = np.stack([np.sum(np.multiply(self.p[index==i],1-self.p[index==i]),axis=0) for i in range(bins)])\n",
    "\n",
    "        # grad_bin = np.divide(residual_bin,p_bin,where=p_bin!=0,out=np.zeros(p_bin.shape))\n",
    "        addon = 1/val.shape[0]\n",
    "        p_bin[p_bin < addon] = addon\n",
    "        grad_bin = np.divide(residual_bin,p_bin) #TO-DO is 1e-5 enough?\n",
    "        \n",
    "        # loss = -np.sum(clf.tree_.n_node_samples@np.abs(grad_bin))\n",
    "\n",
    "        index = clf.apply(val)\n",
    "        grads = np.zeros((val.shape[0],grad_bin.shape[1]))\n",
    "        for i in range(len(grad_bin)):\n",
    "            grads[index==i] = grad_bin[i]\n",
    "        log_odds,p = update_log_p(grads,self.log_odds,self.p,self.mask,self.learning_rate)\n",
    "\n",
    "        loss = np.sum(np.power(self.Y_one_hot - p,2))\n",
    "\n",
    "        return loss,clf,(bins,grad_bin)\n",
    "    \n",
    "    def calculate_fitness(self,vals,beta):\n",
    "        fitness = []\n",
    "        clfs = []\n",
    "        for val in vals:\n",
    "            loss,clf,clf_param = self.calculate_decision(np.array([val]).T)\n",
    "            \n",
    "            fitness.append(loss)\n",
    "            clfs.append((clf,clf_param))\n",
    "\n",
    "        return fitness,clfs\n",
    "    \n",
    "\n",
    "    def evolve(self,total_size,batch_size,elite_size,beta,verbose):\n",
    "        self.generation += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\tgeneration:\",self.generation)\n",
    "            t = time()\n",
    "        \n",
    "        num_batches = total_size//batch_size\n",
    "        pool = self.nodes\n",
    "\n",
    "        elites = []\n",
    "        elites_fitness = []\n",
    "        for j in range(num_batches):\n",
    "\n",
    "            funcs = np.random.choice(list(self.opset.keys()),size=batch_size)\n",
    "            arg_count = [self.opset[func] for func in funcs]\n",
    "            sons = np.random.choice(pool,size = sum(arg_count))\n",
    "            it = iter(sons)\n",
    "            sons = [[next(it) for _ in range(arg_count[i])] for i in range(batch_size)]\n",
    "            vals = [funcs[i]([self.vals[s.index] for s in sons[i]]) for i in range(batch_size)]\n",
    "\n",
    "            vals = np.stack(vals)\n",
    "            fitness,clfs = self.calculate_fitness(vals,beta)\n",
    "            \n",
    "            elites.extend([(funcs[i],sons[i],vals[i],clfs[i]) for i in range(batch_size)])\n",
    "            elites_fitness.extend(fitness)\n",
    "\n",
    "            rank = np.argsort(elites_fitness)\n",
    "\n",
    "            elits = [elites[index] for index in rank[:elite_size]]\n",
    "            elites_fitness = [elites_fitness[index] for index in rank[:elite_size]]\n",
    "\n",
    "        for index in range(elite_size):\n",
    "            func,son,val,clf = elits[index]\n",
    "\n",
    "            node = Node(False,\n",
    "                func=func,\n",
    "                sons=son,\n",
    "                index=len(self.nodes),\n",
    "                fit=elites_fitness[index]\n",
    "            )\n",
    "            node.clf = clf\n",
    "\n",
    "            if index == 0:\n",
    "                if self.best[0] > node.fitness:\n",
    "                    self.best = (node.fitness,node)\n",
    "            self.nodes.append(node)\n",
    "            self.vals = np.append(self.vals,[val],axis=0)\n",
    "            self.test_param_same(node)\n",
    "      \n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\t\",np.max(elites_fitness))\n",
    "            print(\"\\ttime\",time()-t)\n",
    "        return None\n",
    "\n",
    "    def test_param_same(self,node):\n",
    "        v1 = node.predict(self.X)\n",
    "        v2 = self.vals[node.index]\n",
    "        if np.any(v1!=v2):\n",
    "            print(node.index,v1==v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19b7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    print(roc_auc_score(true_y,prob[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524befc",
   "metadata": {},
   "source": [
    "# Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3154e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8661125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y,yt):\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = le.fit_transform(y)\n",
    "    y_one_hot = ohe.fit_transform(y_train.reshape(-1,1))\n",
    "    \n",
    "    y_test = le.transform(yt)\n",
    "    yt_one_hot = ohe.transform(y_test.reshape(-1,1))\n",
    "    \n",
    "    return y_train,y_test,y_one_hot,yt_one_hot\n",
    "\n",
    "def initial(y_one_hot):\n",
    "    init_log_odds = np.sum(y_one_hot,axis=0)/(y_one_hot.shape[0] - np.sum(y_one_hot,axis=0))\n",
    "    init_log_odds = np.array(np.log(init_log_odds))\n",
    "\n",
    "    init_p = np.exp(init_log_odds)\n",
    "    init_p = np.array(init_p/(1+init_p))\n",
    "\n",
    "    return init_log_odds,init_p\n",
    "\n",
    "def initial_first_bin(init_log_odds,init_p,X):\n",
    "    log_odds = np.repeat(init_log_odds,X.shape[0],0)\n",
    "    p = np.repeat(init_p,X.shape[0],0)\n",
    "    \n",
    "    return log_odds,p\n",
    "\n",
    "def gradient(node,val):\n",
    "    val = np.array([val]).T\n",
    "\n",
    "    clf,clf_param = node.clf\n",
    "    bins,grad_bin = clf_param\n",
    "    index = clf.apply(val)\n",
    "\n",
    "    grads = np.zeros((val.shape[0],grad_bin.shape[1]))\n",
    "    for i in range(bins):\n",
    "        grads[index==i] = grad_bin[i]\n",
    "        \n",
    "    return grads\n",
    "\n",
    "def update_log_p(grads,log_odds,p,mask,learning_rate):\n",
    "    log_odds_1 = log_odds + grads * learning_rate\n",
    "    # log_odds_1 = log_odds + mask.reshape(-1,1)*grads * learning_rate\n",
    "    \n",
    "    p_1 = np.exp(log_odds_1,where=log_odds_1 < 64,out=np.zeros(log_odds_1.shape)+64)\n",
    "    p_1 = np.divide(p_1,(1+p_1))\n",
    "    \n",
    "    # p_1 = np.exp(log_odds_1)\n",
    "    # p_1 = np.divide(p_1,(1+p_1))\n",
    "\n",
    "    return log_odds_1,p_1\n",
    "\n",
    "def predict_single_node(node,X,log_odds,p,learning_rate):\n",
    "    mask = np.sum(np.power(p,2),axis=1) < 1\n",
    "\n",
    "    val = node.predict(X)\n",
    "    grads = gradient(node,val)\n",
    "    \n",
    "    log_odds,p = update_log_p(grads,log_odds,p,mask,learning_rate)\n",
    "\n",
    "    return log_odds,p\n",
    "\n",
    "def predict(X,init_log_odds,init_p,learning_rate,stack):\n",
    "    log_odds,p = initial_first_bin(init_log_odds,init_p,X)\n",
    "\n",
    "    for node in stack:\n",
    "        log_odds,p = predict_single_node(node,X,log_odds,p,learning_rate) \n",
    "        \n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c114de92",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cda82e45",
   "metadata": {},
   "source": [
    "## Small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab0e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref from 2segp github\n",
    "# Classification dataset names - choose from following datasets \n",
    "\n",
    "CLASS_DATASET_NAMES = ['bcw','heart','iono','parks','sonar']\n",
    "dataset_name = CLASS_DATASET_NAMES[1]\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "Xy = np.genfromtxt('test_data/'+dataset_name+'.csv', delimiter=',')\n",
    "X = Xy[:, :-1]\n",
    "y = Xy[:, -1]   # last column is the label\n",
    "\n",
    "# simple operators\n",
    "\n",
    "boost_num = 1000\n",
    "\n",
    "seed = np.random.randint(9999999)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "525ae59b",
   "metadata": {},
   "source": [
    "## Cover Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/covtype.data',header=None)\n",
    "data_X = df.iloc[:,:-1].to_numpy()\n",
    "data_y = df.iloc[:,-1].to_numpy().astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64916108",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data_X,data_y,test_size=0.9)\n",
    "y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bc3d686",
   "metadata": {},
   "source": [
    "## Higgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b2201c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[375], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data/HIGGS.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m      2\u001b[0m data_X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m      3\u001b[0m data_y \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/HIGGS.csv',header=None)\n",
    "data_X = df.iloc[:,1:].to_numpy()\n",
    "data_y = df.iloc[:,0].to_numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test2,y_train,y_test2 = train_test_split(data_X,data_y,test_size=500000)\n",
    "Xs,ys = shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70f67fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (240935742.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[53], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \\\\\\\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "i = 105000\n",
    "j = 10500\n",
    "\n",
    "X_train = Xs[:i]\n",
    "y_train = ys[:i]\n",
    "\n",
    "X_test = Xs[i:i+j]\n",
    "y_test = ys[i:i+j]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "764aa5a4",
   "metadata": {},
   "source": [
    "## kdd CUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6bdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/kddcup.tsv\",delimiter='\\t')\n",
    "X = df.iloc[:,:-1].to_numpy().astype(\"float\")\n",
    "y = df.iloc[:,-1].to_numpy().astype(\"int\")\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.5,stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c37c68",
   "metadata": {},
   "source": [
    "# Experiments model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313510eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_engine(eg):\n",
    "    log_odds = eg.log_odds\n",
    "    p = eg.p\n",
    "    residual = eg.residual\n",
    "    learning_rate = eg.learning_rate\n",
    "                \n",
    "    for j in range(1):\n",
    "            eg.evolve(10,10,1,[0,0,0],0)    \n",
    "            \n",
    "    node = eg.best[1]\n",
    "    val = eg.vals[node.index]\n",
    "    grads = gradient(node,val)\n",
    "\n",
    "    # masked_grads = np.zeros(log_odds.shape)\n",
    "    # masked_grads[mask] = grads_slim\n",
    "    # log_odds,p = update_log_p(masked_grads,log_odds,p,mask,learning_rate)\n",
    "\n",
    "    return node,grads\n",
    "\n",
    "def update_engine(X,y,y_one_hot,log_odds,p,residual,learning_rate,max_depth,prev_eg=None):\n",
    "    # eg = Engine(Functions.simple_opset,\n",
    "    #             X[mask],\n",
    "    #             y[mask],\n",
    "    #             residual[mask],\n",
    "    #             log_odds[mask],\n",
    "    #             p[mask],\n",
    "    #             learning_rate,max_depth)\n",
    "    \n",
    "    if prev_eg == None:\n",
    "        eg = Engine(Functions.simple_opset,\n",
    "            X,\n",
    "            y,\n",
    "            y_one_hot,\n",
    "            residual,\n",
    "            log_odds,\n",
    "            p,\n",
    "            learning_rate,max_depth)\n",
    "    else:\n",
    "        eg = prev_eg\n",
    "        \n",
    "        eg.residual = residual\n",
    "        eg.log_odds = log_odds\n",
    "        eg.p = p\n",
    "        eg.learning_rate = learning_rate\n",
    "        eg.max_depth = max_depth\n",
    "\n",
    "        eg.best = (np.inf,None)\n",
    "\n",
    "    return eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3605bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :train prob 0.7843447633699041 loss: 93835.34915253335 best fit 93835.34915253335\n",
      "0 :test prob  0.7843002307598883 loss: 93838.59513600747\n",
      "1 :train prob 0.9786041050969596 loss: 46980.54950239844 best fit 46980.54950239844\n",
      "1 :test prob  0.9786810250597142 loss: 46973.97749144679\n",
      "2 :train prob 0.9767053965426501 loss: 34600.27374257843 best fit 34600.27374257843\n",
      "2 :test prob  0.9768916238208979 loss: 34579.09483311415\n",
      "3 :train prob 0.9768025586008664 loss: 28469.638458042988 best fit 28469.638458042988\n",
      "3 :test prob  0.9770009311363912 loss: 28393.343523776643\n",
      "4 :train prob 0.9768066070199587 loss: 22297.382103393975 best fit 22297.382103393975\n",
      "4 :test prob  0.977021173231853 loss: 22237.064844647277\n",
      "5 :train prob 0.97703331848913 loss: 15723.890773999354 best fit 15723.890773999354\n",
      "5 :test prob  0.9773612404356099 loss: 15736.61415942083\n",
      "6 :train prob 0.9763734261770779 loss: 14049.169395347035 best fit 14049.169395347035\n",
      "6 :test prob  0.9766041860653415 loss: 14074.623347772284\n",
      "7 :train prob 0.97703331848913 loss: 12711.159090393972 best fit 12711.159090393972\n",
      "7 :test prob  0.9773612404356099 loss: 12733.98314376098\n",
      "8 :train prob 0.9813165458888304 loss: 11216.288270635627 best fit 11216.288270635627\n",
      "8 :test prob  0.9816039836443868 loss: 11254.173924086752\n",
      "9 :train prob 0.9814865794907088 loss: 9359.264910625407 best fit 9359.264910625407\n",
      "9 :test prob  0.9816728067689567 loss: 9376.34913868086\n",
      "10 :train prob 0.9828266062102748 loss: 8768.217202869926 best fit 8768.217202869926\n",
      "10 :test prob  0.982875187239383 loss: 8780.160416044188\n",
      "11 :train prob 0.9831626249949394 loss: 8148.120703828299 best fit 8148.120703828299\n",
      "11 :test prob  0.9832759807295252 loss: 8145.754810775058\n",
      "12 :train prob 0.9835229342941582 loss: 7687.679397014057 best fit 7687.679397014057\n",
      "12 :test prob  0.9836200963523744 loss: 7683.679004695625\n",
      "13 :train prob 0.9835310311323429 loss: 7497.404273129295 best fit 7497.404273129295\n",
      "13 :test prob  0.9836241447714668 loss: 7492.062342979387\n",
      "14 :train prob 0.9835269827132505 loss: 6841.569323942132 best fit 6841.569323942132\n",
      "14 :test prob  0.9836362900287438 loss: 6844.387554381717\n",
      "15 :train prob 0.9834541111695883 loss: 6523.40343531111 best fit 6523.40343531111\n",
      "15 :test prob  0.9835472248087122 loss: 6523.476616008042\n",
      "16 :train prob 0.9834581595886807 loss: 6399.487405433705 best fit 6399.487405433705\n",
      "16 :test prob  0.9835715153232663 loss: 6399.037261628439\n",
      "17 :train prob 0.9837132099914984 loss: 6277.97687080799 best fit 6277.97687080799\n",
      "17 :test prob  0.9838184688878993 loss: 6282.306452896243\n",
      "18 :train prob 0.9874134650419011 loss: 5178.884859971296 best fit 5178.884859971296\n",
      "18 :test prob  0.9875308691955791 loss: 5197.071138943754\n",
      "19 :train prob 0.9879316626857212 loss: 4878.642630521344 best fit 4878.642630521344\n",
      "19 :test prob  0.9879154690093519 loss: 4905.246304391012\n",
      "20 :train prob 0.9886239423505121 loss: 4736.076879549366 best fit 4736.076879549366\n",
      "20 :test prob  0.9885470223877576 loss: 4763.084600784348\n",
      "21 :train prob 0.9886644265414356 loss: 4754.930886915018 best fit 4754.930886915018\n",
      "21 :test prob  0.988587506578681 loss: 4783.433051120367\n",
      "22 :train prob 0.9906036192866685 loss: 4314.952973687382 best fit 4314.952973687382\n",
      "22 :test prob  0.9905469414193757 loss: 4337.772515607674\n",
      "23 :train prob 0.9896036597708595 loss: 4152.503759766249 best fit 4152.503759766249\n",
      "23 :test prob  0.9895105461317355 loss: 4172.325500670934\n",
      "24 :train prob 0.9904700214566212 loss: 3986.9031924088567 best fit 3986.9031924088567\n",
      "24 :test prob  0.9903688109793126 loss: 4003.8204484808757\n",
      "25 :train prob 0.9907210234403465 loss: 3923.7739949419156 best fit 3923.7739949419156\n",
      "25 :test prob  0.9905914740293915 loss: 3940.467081221956\n",
      "26 :train prob 0.9907169750212542 loss: 3845.2582589267968 best fit 3845.2582589267968\n",
      "26 :test prob  0.9905914740293915 loss: 3860.254070646821\n",
      "27 :train prob 0.9908100886603781 loss: 3672.688589918176 best fit 3672.688589918176\n",
      "27 :test prob  0.9907169750212542 loss: 3678.507660927386\n",
      "28 :train prob 0.990668393992146 loss: 3519.95491705826 best fit 3519.95491705826\n",
      "28 :test prob  0.9906886360876078 loss: 3522.6867478320687\n",
      "29 :train prob 0.9929112181693048 loss: 3432.9376897471393 best fit 3432.9376897471393\n",
      "29 :test prob  0.9929395571029513 loss: 3433.6583149452604\n",
      "30 :train prob 0.9933686895267398 loss: 3353.0656306480164 best fit 3353.0656306480164\n",
      "30 :test prob  0.9933727379458321 loss: 3353.498816859669\n",
      "31 :train prob 0.9936196915104651 loss: 3241.7264239917395 best fit 3241.7264239917395\n",
      "31 :test prob  0.9936358851868345 loss: 3243.2908052978305\n",
      "32 :train prob 0.9924132626209465 loss: 3182.8753101471275 best fit 3182.8753101471275\n",
      "32 :test prob  0.9923444394963766 loss: 3187.868473048395\n",
      "33 :train prob 0.9937532893405125 loss: 3059.5223505310996 best fit 3059.5223505310996\n",
      "33 :test prob  0.9937168535686814 loss: 3062.6078942773547\n",
      "34 :train prob 0.9937856766932512 loss: 2994.9000642807146 best fit 2994.9000642807146\n",
      "34 :test prob  0.9937532893405125 loss: 3003.022822416869\n",
      "35 :train prob 0.9942026638597627 loss: 2873.957320945721 best fit 2873.957320945721\n",
      "35 :test prob  0.9941986154406705 loss: 2879.249450364646\n",
      "36 :train prob 0.9942269543743169 loss: 2839.1500140333683 best fit 2839.1500140333683\n",
      "36 :test prob  0.9942067122788552 loss: 2843.7053378352343\n",
      "37 :train prob 0.9954779158738513 loss: 2703.0152456681913 best fit 2703.0152456681913\n",
      "37 :test prob  0.9954941095502207 loss: 2714.1689238207805\n",
      "38 :train prob 0.9955345937411441 loss: 2606.0713202158477 best fit 2606.0713202158477\n",
      "38 :test prob  0.9955831747702523 loss: 2615.178555565358\n",
      "39 :train prob 0.9955791263511599 loss: 2587.332793566921 best fit 2587.332793566921\n",
      "39 :test prob  0.9956398526375451 loss: 2595.8030118884835\n",
      "40 :train prob 0.9955912716084369 loss: 2554.129012255807 best fit 2554.129012255807\n",
      "40 :test prob  0.9956398526375451 loss: 2561.4648236477656\n",
      "41 :train prob 0.9956803368284685 loss: 2512.837487084343 best fit 2512.837487084343\n",
      "41 :test prob  0.9957167726002996 loss: 2523.6639387188634\n",
      "42 :train prob 0.995720821019392 loss: 2458.1204431450706 best fit 2458.1204431450706\n",
      "42 :test prob  0.9957532083721307 loss: 2473.6355664081875\n",
      "43 :train prob 0.9957086757621149 loss: 2346.638044879654 best fit 2346.638044879654\n",
      "43 :test prob  0.9957653536294078 loss: 2361.4412417239796\n",
      "44 :train prob 0.9957410631148537 loss: 2217.7205901441307 best fit 2217.7205901441307\n",
      "44 :test prob  0.9957896441439618 loss: 2235.8343219416174\n",
      "45 :train prob 0.995979919841302 loss: 2108.82135916405 best fit 2108.82135916405\n",
      "45 :test prob  0.9960325492895025 loss: 2128.7470960638016\n",
      "46 :train prob 0.99583822517307 loss: 2043.811709998922 best fit 2043.811709998922\n",
      "46 :test prob  0.9958868062021781 loss: 2066.0157074272124\n",
      "47 :train prob 0.9958544188494393 loss: 1960.2456860831257 best fit 1960.2456860831257\n",
      "47 :test prob  0.9959070482976398 loss: 1981.1218946319575\n",
      "48 :train prob 0.9960244524513178 loss: 1910.599875038014 best fit 1910.599875038014\n",
      "48 :test prob  0.9960932755758877 loss: 1932.2625266770992\n",
      "49 :train prob 0.9962066313104733 loss: 1876.067578157932 best fit 1876.067578157932\n",
      "49 :test prob  0.9962471155013967 loss: 1899.0507777122061\n",
      "50 :train prob 0.9962187765677503 loss: 1841.6837098058409 best fit 1841.6837098058409\n",
      "50 :test prob  0.9962875996923202 loss: 1870.5431560563138\n",
      "51 :train prob 0.9961985344722886 loss: 1807.0678409321147 best fit 1807.0678409321147\n",
      "51 :test prob  0.9962714060159508 loss: 1834.6181980978783\n",
      "52 :train prob 0.9965507469333226 loss: 1746.170395777371 best fit 1746.170395777371\n",
      "52 :test prob  0.9965143111614915 loss: 1780.5865670375538\n",
      "53 :train prob 0.9965507469333226 loss: 1672.6671772559187 best fit 1672.6671772559187\n",
      "53 :test prob  0.9965062143233068 loss: 1710.0784492924602\n",
      "54 :train prob 0.9966802963442776 loss: 1645.9862346497905 best fit 1645.9862346497905\n",
      "54 :test prob  0.9965750374478766 loss: 1685.4529198754833\n",
      "55 :train prob 0.996720780535201 loss: 1635.4204563631813 best fit 1635.4204563631813\n",
      "55 :test prob  0.9966155216388001 loss: 1672.7193400428291\n",
      "56 :train prob 0.996708635277924 loss: 1584.0577673690834 best fit 1584.0577673690834\n",
      "56 :test prob  0.9966236184769848 loss: 1621.2936662341065\n",
      "57 :train prob 0.9966883931824623 loss: 1565.9428253924948 best fit 1565.9428253924948\n",
      "57 :test prob  0.996603376381523 loss: 1603.8969821115736\n",
      "58 :train prob 0.9967329257924781 loss: 1543.5013453319107 best fit 1543.5013453319107\n",
      "58 :test prob  0.9966479089915388 loss: 1587.6350016599188\n",
      "59 :train prob 0.9967491194688474 loss: 1523.4770646170198 best fit 1523.4770646170198\n",
      "59 :test prob  0.9966560058297235 loss: 1568.231622318473\n",
      "60 :train prob 0.9967612647261245 loss: 1515.5722509427658 best fit 1515.5722509427658\n",
      "60 :test prob  0.9966721995060929 loss: 1557.6203727043803\n",
      "61 :train prob 0.9967653131452168 loss: 1477.6265506066807 best fit 1477.6265506066807\n",
      "61 :test prob  0.9966883931824623 loss: 1521.8691578763123\n",
      "62 :train prob 0.9967410226306628 loss: 1448.795715295753 best fit 1448.795715295753\n",
      "62 :test prob  0.9966600542488159 loss: 1488.8352611121554\n",
      "63 :train prob 0.9970284603862192 loss: 1386.0848489481218 best fit 1386.0848489481218\n",
      "63 :test prob  0.9969920246143881 loss: 1418.4814179265318\n",
      "64 :train prob 0.9970487024816809 loss: 1319.073250074694 best fit 1319.073250074694\n",
      "64 :test prob  0.9970203635480345 loss: 1338.0980686061068\n",
      "65 :train prob 0.9970122667098498 loss: 1273.0150508031684 best fit 1273.0150508031684\n",
      "65 :test prob  0.9969920246143881 loss: 1287.5686969703677\n",
      "66 :train prob 0.9970648961580503 loss: 1249.425605657304 best fit 1249.425605657304\n",
      "66 :test prob  0.9970648961580503 loss: 1265.345271465702\n",
      "67 :train prob 0.9970001214525728 loss: 1228.7110914945747 best fit 1228.7110914945747\n",
      "67 :test prob  0.9970203635480345 loss: 1243.7293382410394\n",
      "68 :train prob 0.9969636856807417 loss: 1202.315194768111 best fit 1202.315194768111\n",
      "68 :test prob  0.9969879761952958 loss: 1216.4587286327746\n",
      "69 :train prob 0.9971580097971742 loss: 1190.0619180970511 best fit 1190.0619180970511\n",
      "69 :test prob  0.9971256224444355 loss: 1208.4416078481727\n",
      "70 :train prob 0.9971701550544513 loss: 1153.873374375294 best fit 1153.873374375294\n",
      "70 :test prob  0.9971256224444355 loss: 1172.9334770282048\n",
      "71 :train prob 0.9972106392453747 loss: 1130.309971736873 best fit 1130.309971736873\n",
      "71 :test prob  0.9971984939880977 loss: 1151.5168426374823\n",
      "72 :train prob 0.9980729525120441 loss: 1020.0045288988103 best fit 1020.0045288988103\n",
      "72 :test prob  0.9980162746447512 loss: 1053.7424359763752\n",
      "73 :train prob 0.9981012914456905 loss: 1011.6146470018916 best fit 1011.6146470018916\n",
      "73 :test prob  0.998036516740213 loss: 1047.0138380880765\n",
      "74 :train prob 0.9980770009311364 loss: 991.2024847886622 best fit 991.2024847886622\n",
      "74 :test prob  0.9980243714829359 loss: 1024.9933903888284\n",
      "75 :train prob 0.9980850977693211 loss: 983.3421704984337 best fit 983.3421704984337\n",
      "75 :test prob  0.998036516740213 loss: 1014.7028261606187\n",
      "76 :train prob 0.9980810493502288 loss: 971.0681347740126 best fit 971.0681347740126\n",
      "76 :test prob  0.9980243714829359 loss: 1001.5686376607654\n",
      "77 :train prob 0.9980648556738594 loss: 962.5595575405529 best fit 962.5595575405529\n",
      "77 :test prob  0.9980243714829359 loss: 993.8135629969668\n",
      "78 :train prob 0.9980810493502288 loss: 946.2516256818066 best fit 946.2516256818066\n",
      "78 :test prob  0.9980000809683819 loss: 976.4619439592628\n",
      "79 :train prob 0.9982065503420914 loss: 928.5617271623959 best fit 928.5617271623959\n",
      "79 :test prob  0.9981336787984292 loss: 959.3140830147132\n",
      "80 :train prob 0.9982146471802761 loss: 918.7682187384988 best fit 918.7682187384988\n",
      "80 :test prob  0.9981498724747986 loss: 948.6113206609782\n",
      "81 :train prob 0.9982348892757378 loss: 916.0776295137791 best fit 916.0776295137791\n",
      "81 :test prob  0.9981579693129833 loss: 946.8199931762979\n",
      "82 :train prob 0.9982389376948302 loss: 907.0163106722732 best fit 907.0163106722732\n",
      "82 :test prob  0.9981620177320756 loss: 937.5084021394279\n",
      "83 :train prob 0.9982996639812154 loss: 893.9669409783791 best fit 893.9669409783791\n",
      "83 :test prob  0.9982105987611838 loss: 923.604297597661\n",
      "84 :train prob 0.9982996639812154 loss: 880.0154422689425 best fit 880.0154422689425\n",
      "84 :test prob  0.9981944050848144 loss: 911.0476443902097\n",
      "85 :train prob 0.998283470304846 loss: 865.5411712171594 best fit 865.5411712171594\n",
      "85 :test prob  0.9981701145702603 loss: 896.6895781074433\n",
      "86 :train prob 0.9983806323630622 loss: 846.7245600508539 best fit 846.7245600508539\n",
      "86 :test prob  0.9983239544957694 loss: 877.4534669155339\n",
      "87 :train prob 0.9983765839439699 loss: 838.5385928645293 best fit 838.5385928645293\n",
      "87 :test prob  0.9983360997530465 loss: 870.0943793673589\n",
      "88 :train prob 0.998413019715801 loss: 817.3026343511212 best fit 817.3026343511212\n",
      "88 :test prob  0.9983603902676005 loss: 849.4061341709479\n",
      "89 :train prob 0.9984413586494474 loss: 814.7171491871692 best fit 814.7171491871692\n",
      "89 :test prob  0.9983887292012469 loss: 845.4796048838024\n",
      "90 :train prob 0.9984494554876321 loss: 809.2490538759663 best fit 809.2490538759663\n",
      "90 :test prob  0.9984089712967086 loss: 835.5357776299331\n",
      "91 :train prob 0.9984777944212785 loss: 799.383314454246 best fit 799.383314454246\n",
      "91 :test prob  0.9984616007449091 loss: 826.6603967898791\n",
      "92 :train prob 0.9985951985749565 loss: 780.2464532510919 best fit 780.2464532510919\n",
      "92 :test prob  0.9985182786122019 loss: 813.4988796947492\n",
      "93 :train prob 0.9986235375086029 loss: 770.4702973958755 best fit 770.4702973958755\n",
      "93 :test prob  0.9985223270312943 loss: 804.7877429162415\n",
      "94 :train prob 0.9986518764422493 loss: 758.1318517700975 best fit 758.1318517700975\n",
      "94 :test prob  0.9985466175458484 loss: 795.8663775563521\n",
      "95 :train prob 0.9986397311849723 loss: 749.7120848886839 best fit 749.7120848886839\n",
      "95 :test prob  0.9985749564794948 loss: 787.1685119564628\n",
      "96 :train prob 0.9985790048985871 loss: 731.7330063455313 best fit 731.7330063455313\n",
      "96 :test prob  0.9985182786122019 loss: 767.4464584294643\n",
      "97 :train prob 0.998542569126756 loss: 715.148399933838 best fit 715.148399933838\n",
      "97 :test prob  0.9984858912594632 loss: 752.9417205785259\n",
      "98 :train prob 0.9985466175458484 loss: 707.8683827773926 best fit 707.8683827773926\n",
      "98 :test prob  0.9984939880976479 loss: 752.21014232205\n",
      "99 :train prob 0.9985587628031254 loss: 699.703432924856 best fit 699.703432924856\n",
      "99 :test prob  0.9985020849358326 loss: 743.7387822581127\n",
      "100 :train prob 0.9986073438322335 loss: 694.9895184418167 best fit 694.9895184418167\n",
      "100 :test prob  0.9985344722885713 loss: 739.7261084752727\n",
      "101 :train prob 0.9985830533176795 loss: 686.6698014873521 best fit 686.6698014873521\n",
      "101 :test prob  0.9985223270312943 loss: 728.6005746024524\n",
      "102 :train prob 0.9986113922513259 loss: 669.8821789023643 best fit 669.8821789023643\n",
      "102 :test prob  0.9985223270312943 loss: 718.7037067355216\n",
      "103 :train prob 0.9986761669568034 loss: 667.0343482751647 best fit 667.0343482751647\n",
      "103 :test prob  0.9985506659649407 loss: 717.4252844702756\n",
      "104 :train prob 0.9988623942350512 loss: 644.8661845416524 best fit 644.8661845416524\n",
      "104 :test prob  0.9987206995668192 loss: 701.5048500928242\n",
      "105 :train prob 0.9988664426541436 loss: 634.4798622313825 best fit 634.4798622313825\n",
      "105 :test prob  0.9987166511477268 loss: 690.5172840492532\n",
      "106 :train prob 0.9988785879114206 loss: 626.0444298839265 best fit 626.0444298839265\n",
      "106 :test prob  0.9987328448240962 loss: 682.5316054289614\n",
      "107 :train prob 0.9988542973968665 loss: 623.0158702576355 best fit 623.0158702576355\n",
      "107 :test prob  0.9987166511477268 loss: 679.2343988559317\n",
      "108 :train prob 0.9988907331686976 loss: 611.6486255059204 best fit 611.6486255059204\n",
      "108 :test prob  0.9987692805959273 loss: 665.4200958560633\n",
      "109 :train prob 0.9989150236832517 loss: 605.9338708047179 best fit 605.9338708047179\n",
      "109 :test prob  0.9987976195295737 loss: 659.500067632935\n",
      "110 :train prob 0.9989393141978058 loss: 599.7473730609353 best fit 599.7473730609353\n",
      "110 :test prob  0.9988138132059431 loss: 653.5476339405353\n",
      "111 :train prob 0.9989676531314522 loss: 595.4995148893727 best fit 595.4995148893727\n",
      "111 :test prob  0.9988340553014048 loss: 650.5050453804622\n",
      "112 :train prob 0.9989514594550828 loss: 589.7444982684498 best fit 589.7444982684498\n",
      "112 :test prob  0.9988138132059431 loss: 646.8223681344326\n",
      "113 :train prob 0.9990000404841909 loss: 585.3058156624198 best fit 585.3058156624198\n",
      "113 :test prob  0.9988623942350512 loss: 644.3859679848681\n",
      "114 :train prob 0.9989919436460062 loss: 581.7573747860764 best fit 581.7573747860764\n",
      "114 :test prob  0.9988542973968665 loss: 639.791101927214\n",
      "115 :train prob 0.999012185741468 loss: 574.3806833144264 best fit 574.3806833144264\n",
      "115 :test prob  0.9988664426541436 loss: 631.9764745616498\n",
      "116 :train prob 0.9990202825796527 loss: 567.0922948190484 best fit 567.0922948190484\n",
      "116 :test prob  0.9988664426541436 loss: 624.137336471084\n",
      "117 :train prob 0.9990324278369297 loss: 564.9810271909092 best fit 564.9810271909092\n",
      "117 :test prob  0.9988664426541436 loss: 623.8121977579533\n",
      "118 :train prob 0.9990324278369297 loss: 559.0983717339622 best fit 559.0983717339622\n",
      "118 :test prob  0.9988704910732359 loss: 616.9148849562118\n",
      "119 :train prob 0.999036476256022 loss: 547.21118785536 best fit 547.21118785536\n",
      "119 :test prob  0.9988623942350512 loss: 605.5889315967706\n",
      "120 :train prob 0.9990405246751144 loss: 537.7503749230227 best fit 537.7503749230227\n",
      "120 :test prob  0.9988623942350512 loss: 591.2655428037044\n",
      "121 :train prob 0.9990445730942067 loss: 536.5434261546585 best fit 536.5434261546585\n",
      "121 :test prob  0.9988785879114206 loss: 591.0099623102385\n",
      "122 :train prob 0.9989878952269139 loss: 534.7291130305305 best fit 534.7291130305305\n",
      "122 :test prob  0.9988583458159589 loss: 588.8378750153158\n",
      "123 :train prob 0.9990486215132991 loss: 519.7184770377847 best fit 519.7184770377847\n",
      "123 :test prob  0.9989028784259747 loss: 576.2587471372971\n",
      "124 :train prob 0.9990729120278531 loss: 513.531054966489 best fit 513.531054966489\n",
      "124 :test prob  0.9989231205214364 loss: 569.1251696182497\n",
      "125 :train prob 0.9991174446378689 loss: 508.01519843636004 best fit 508.01519843636004\n",
      "125 :test prob  0.9989555078741751 loss: 563.3587396180736\n",
      "126 :train prob 0.9991174446378689 loss: 499.3496110925239 best fit 499.3496110925239\n",
      "126 :test prob  0.9989555078741751 loss: 553.8847725896971\n",
      "127 :train prob 0.9991093477996842 loss: 495.4057101059333 best fit 495.4057101059333\n",
      "127 :test prob  0.9989555078741751 loss: 551.4196940275913\n",
      "128 :train prob 0.9991093477996842 loss: 486.47230241191164 best fit 486.47230241191164\n",
      "128 :test prob  0.9989555078741751 loss: 542.8746518355205\n",
      "129 :train prob 0.9991093477996842 loss: 472.0460573761574 best fit 472.0460573761574\n",
      "129 :test prob  0.9989393141978058 loss: 528.933027121526\n",
      "130 :train prob 0.999129589895146 loss: 468.7834967956717 best fit 468.7834967956717\n",
      "130 :test prob  0.9989757499696369 loss: 524.7222090483583\n",
      "131 :train prob 0.9991214930569613 loss: 463.0525325952804 best fit 463.0525325952804\n",
      "131 :test prob  0.9989555078741751 loss: 519.1232666471692\n",
      "132 :train prob 0.9991538804097 loss: 458.19608891274083 best fit 458.19608891274083\n",
      "132 :test prob  0.9989878952269139 loss: 515.159726523806\n",
      "133 :train prob 0.9991498319906077 loss: 444.8496036331676 best fit 444.8496036331676\n",
      "133 :test prob  0.9989595562932675 loss: 503.1770781580968\n",
      "134 :train prob 0.9992874782397474 loss: 436.3969041659052 best fit 436.3969041659052\n",
      "134 :test prob  0.9991255414760536 loss: 495.1053951908224\n",
      "135 :train prob 0.9993036719161168 loss: 433.48376550541155 best fit 433.48376550541155\n",
      "135 :test prob  0.999129589895146 loss: 491.7463481019213\n",
      "136 :train prob 0.9992753329824704 loss: 430.0516725527664 best fit 430.0516725527664\n",
      "136 :test prob  0.9991214930569613 loss: 487.640710537502\n",
      "137 :train prob 0.9992793814015627 loss: 418.15407986496 best fit 418.15407986496\n",
      "137 :test prob  0.9991012509614995 loss: 477.4718717695338\n",
      "138 :train prob 0.9993077203352091 loss: 412.9115966596667 best fit 412.9115966596667\n",
      "138 :test prob  0.999129589895146 loss: 472.6284047477203\n",
      "139 :train prob 0.9993077203352091 loss: 408.82433306613734 best fit 408.82433306613734\n",
      "139 :test prob  0.9991336383142383 loss: 469.1890408756041\n",
      "140 :train prob 0.9993563013643172 loss: 403.768883489015 best fit 403.768883489015\n",
      "140 :test prob  0.9991781709242541 loss: 460.46836727026607\n",
      "141 :train prob 0.9993805918788713 loss: 399.7780210009329 best fit 399.7780210009329\n",
      "141 :test prob  0.9991943646006235 loss: 456.9720519179521\n",
      "142 :train prob 0.999388688717056 loss: 397.77361868379916 best fit 397.77361868379916\n",
      "142 :test prob  0.9991984130197158 loss: 455.4925964676248\n",
      "143 :train prob 0.999400833974333 loss: 392.830726132992 best fit 392.830726132992\n",
      "143 :test prob  0.9992105582769928 loss: 449.6935226576872\n",
      "144 :train prob 0.999376543459779 loss: 389.13544225424295 best fit 389.13544225424295\n",
      "144 :test prob  0.9991903161815311 loss: 446.5678596678869\n",
      "145 :train prob 0.9994129792316101 loss: 380.53021912954364 best fit 380.53021912954364\n",
      "145 :test prob  0.9992186551151775 loss: 439.45287691483753\n",
      "146 :train prob 0.9994129792316101 loss: 377.25373809942624 best fit 377.25373809942624\n",
      "146 :test prob  0.9992267519533622 loss: 437.0371497711482\n",
      "147 :train prob 0.9994210760697948 loss: 375.21560552328265 best fit 375.21560552328265\n",
      "147 :test prob  0.9992388972106393 loss: 432.56530002980935\n",
      "148 :train prob 0.9994089308125177 loss: 372.40152648309515 best fit 372.40152648309515\n",
      "148 :test prob  0.9992267519533622 loss: 430.93194454826744\n",
      "149 :train prob 0.9994089308125177 loss: 369.97146464051525 best fit 369.97146464051525\n",
      "149 :test prob  0.9992388972106393 loss: 427.7381764473129\n",
      "150 :train prob 0.9994210760697948 loss: 368.02766249300964 best fit 368.02766249300964\n",
      "150 :test prob  0.9992510424679163 loss: 425.2133137173588\n",
      "151 :train prob 0.9994210760697948 loss: 365.64330799108706 best fit 365.64330799108706\n",
      "151 :test prob  0.9992388972106393 loss: 423.06405185425206\n",
      "152 :train prob 0.9994089308125177 loss: 364.7148168856336 best fit 364.7148168856336\n",
      "152 :test prob  0.9992227035342699 loss: 422.1437721610716\n",
      "153 :train prob 0.999388688717056 loss: 357.5084636751679 best fit 357.5084636751679\n",
      "153 :test prob  0.9992308003724546 loss: 413.285044780527\n",
      "154 :train prob 0.9994048823934254 loss: 356.4554602082456 best fit 356.4554602082456\n",
      "154 :test prob  0.9992308003724546 loss: 411.85336859256324\n",
      "155 :train prob 0.9994210760697948 loss: 354.65710341294033 best fit 354.65710341294033\n",
      "155 :test prob  0.9992429456297316 loss: 410.4751376500756\n",
      "156 :train prob 0.9994210760697948 loss: 352.63904017994145 best fit 352.63904017994145\n",
      "156 :test prob  0.9992429456297316 loss: 408.14904785022173\n",
      "157 :train prob 0.9994129792316101 loss: 348.63047673081235 best fit 348.63047673081235\n",
      "157 :test prob  0.9992388972106393 loss: 403.93815940412725\n",
      "158 :train prob 0.9994089308125177 loss: 347.39058499368514 best fit 347.39058499368514\n",
      "158 :test prob  0.9992388972106393 loss: 403.74504897666725\n",
      "159 :train prob 0.999400833974333 loss: 341.3254574863264 best fit 341.3254574863264\n",
      "159 :test prob  0.9992227035342699 loss: 401.79574992102005\n",
      "160 :train prob 0.9994332213270718 loss: 338.329965045007 best fit 338.329965045007\n",
      "160 :test prob  0.9992550908870086 loss: 398.62361763211186\n",
      "161 :train prob 0.9994372697461641 loss: 334.8244158538741 best fit 334.8244158538741\n",
      "161 :test prob  0.9992429456297316 loss: 395.19557870519367\n",
      "162 :train prob 0.9994453665843488 loss: 333.4058008290794 best fit 333.4058008290794\n",
      "162 :test prob  0.999259139306101 loss: 392.62638705962115\n",
      "163 :train prob 0.9994534634225335 loss: 331.5919849161621 best fit 331.5919849161621\n",
      "163 :test prob  0.999271284563378 loss: 390.1993137311231\n",
      "164 :train prob 0.9994534634225335 loss: 330.83608336364847 best fit 330.83608336364847\n",
      "164 :test prob  0.9992672361442857 loss: 389.6755439296596\n",
      "165 :train prob 0.9994575118416258 loss: 329.13546123447264 best fit 329.13546123447264\n",
      "165 :test prob  0.999271284563378 loss: 388.87497860011655\n",
      "166 :train prob 0.9994656086798105 loss: 327.31268689812885 best fit 327.31268689812885\n",
      "166 :test prob  0.9992672361442857 loss: 388.3256489011998\n",
      "167 :train prob 0.9994656086798105 loss: 326.4242214370519 best fit 326.4242214370519\n",
      "167 :test prob  0.9992672361442857 loss: 388.551805030083\n",
      "168 :train prob 0.9994777539370876 loss: 325.32419856215057 best fit 325.32419856215057\n",
      "168 :test prob  0.9993077203352091 loss: 388.3922656525075\n",
      "169 :train prob 0.9994777539370876 loss: 323.66297068513455 best fit 323.66297068513455\n",
      "169 :test prob  0.9992874782397474 loss: 386.48327672075914\n",
      "170 :train prob 0.9994818023561799 loss: 322.72373606841467 best fit 322.72373606841467\n",
      "170 :test prob  0.9992915266588397 loss: 386.2848107548351\n",
      "171 :train prob 0.9994818023561799 loss: 319.2283178975142 best fit 319.2283178975142\n",
      "171 :test prob  0.9992915266588397 loss: 382.12904750921257\n",
      "172 :train prob 0.9994818023561799 loss: 317.56565214565325 best fit 317.56565214565325\n",
      "172 :test prob  0.9992915266588397 loss: 381.27290077975715\n",
      "173 :train prob 0.9994818023561799 loss: 315.8715415730842 best fit 315.8715415730842\n",
      "173 :test prob  0.9993036719161168 loss: 380.0731599058801\n",
      "174 :train prob 0.9994939476134569 loss: 312.19073930584597 best fit 312.19073930584597\n",
      "174 :test prob  0.9993077203352091 loss: 377.7254821034294\n",
      "175 :train prob 0.9995020444516416 loss: 311.4045770329115 best fit 311.4045770329115\n",
      "175 :test prob  0.9992996234970244 loss: 378.4133408628882\n",
      "176 :train prob 0.999506092870734 loss: 307.93677187180765 best fit 307.93677187180765\n",
      "176 :test prob  0.9993036719161168 loss: 374.9067811867787\n",
      "177 :train prob 0.9995020444516416 loss: 306.2884496712911 best fit 306.2884496712911\n",
      "177 :test prob  0.9992955750779321 loss: 371.8423298722187\n",
      "178 :train prob 0.9995020444516416 loss: 303.6852305524556 best fit 303.6852305524556\n",
      "178 :test prob  0.9992955750779321 loss: 369.688118586551\n",
      "179 :train prob 0.999506092870734 loss: 298.69693829613277 best fit 298.69693829613277\n",
      "179 :test prob  0.9992996234970244 loss: 366.18103707063\n",
      "180 :train prob 0.999506092870734 loss: 297.58563049580766 best fit 297.58563049580766\n",
      "180 :test prob  0.9992955750779321 loss: 364.3501548876963\n",
      "181 :train prob 0.9994939476134569 loss: 295.88690077294865 best fit 295.88690077294865\n",
      "181 :test prob  0.9993077203352091 loss: 361.42659328831525\n",
      "182 :train prob 0.9994979960325493 loss: 293.31889752521283 best fit 293.31889752521283\n",
      "182 :test prob  0.9993158171733938 loss: 358.7843741121221\n",
      "183 :train prob 0.999506092870734 loss: 289.99194217422183 best fit 289.99194217422183\n",
      "183 :test prob  0.999283429820655 loss: 358.14798103519314\n",
      "184 :train prob 0.9995141897089187 loss: 286.82964237727015 best fit 286.82964237727015\n",
      "184 :test prob  0.9992996234970244 loss: 358.1522183009124\n",
      "185 :train prob 0.9995101412898263 loss: 285.7225404914204 best fit 285.7225404914204\n",
      "185 :test prob  0.999283429820655 loss: 357.46755213916236\n",
      "186 :train prob 0.9995141897089187 loss: 284.82066988830866 best fit 284.82066988830866\n",
      "186 :test prob  0.999283429820655 loss: 356.01752302002234\n",
      "187 :train prob 0.999518238128011 loss: 282.2615014551928 best fit 282.2615014551928\n",
      "187 :test prob  0.9992955750779321 loss: 354.47456643871817\n",
      "188 :train prob 0.9995222865471034 loss: 279.7272816977958 best fit 279.7272816977958\n",
      "188 :test prob  0.9993036719161168 loss: 352.2753627817044\n",
      "189 :train prob 0.9995263349661957 loss: 278.692381471839 best fit 278.692381471839\n",
      "189 :test prob  0.9993036719161168 loss: 351.78710730727965\n",
      "190 :train prob 0.9995506254807498 loss: 277.24931187874256 best fit 277.24931187874256\n",
      "190 :test prob  0.9993077203352091 loss: 350.90477229403973\n",
      "191 :train prob 0.9995506254807498 loss: 276.6799215372278 best fit 276.6799215372278\n",
      "191 :test prob  0.9993279624306708 loss: 351.07525984286497\n",
      "192 :train prob 0.9995506254807498 loss: 276.2538639013058 best fit 276.2538639013058\n",
      "192 :test prob  0.9993279624306708 loss: 350.36439787508135\n",
      "193 :train prob 0.9995263349661957 loss: 274.6340286031089 best fit 274.6340286031089\n",
      "193 :test prob  0.9993158171733938 loss: 347.69080633757176\n",
      "194 :train prob 0.999530383385288 loss: 273.9356016947557 best fit 273.9356016947557\n",
      "194 :test prob  0.9993117687543015 loss: 347.17323960513806\n",
      "195 :train prob 0.9995546738998421 loss: 272.494386863764 best fit 272.494386863764\n",
      "195 :test prob  0.9993198655924861 loss: 345.64140012762863\n",
      "196 :train prob 0.9995546738998421 loss: 269.91609277986265 best fit 269.91609277986265\n",
      "196 :test prob  0.9993158171733938 loss: 343.3200790358852\n",
      "197 :train prob 0.9995587223189345 loss: 268.9840716460848 best fit 268.9840716460848\n",
      "197 :test prob  0.9993158171733938 loss: 343.2626990185486\n",
      "198 :train prob 0.9995587223189345 loss: 267.5096603639996 best fit 267.5096603639996\n",
      "198 :test prob  0.9993239140115785 loss: 340.68146731797657\n",
      "199 :train prob 0.9995506254807498 loss: 264.225041237253 best fit 264.225041237253\n",
      "199 :test prob  0.9993158171733938 loss: 337.545309920789\n",
      "200 :train prob 0.9995506254807498 loss: 263.48098678775085 best fit 263.48098678775085\n",
      "200 :test prob  0.9993198655924861 loss: 337.8984054770485\n",
      "201 :train prob 0.9995546738998421 loss: 262.0473222895574 best fit 262.0473222895574\n",
      "201 :test prob  0.9993279624306708 loss: 337.22598154771157\n",
      "202 :train prob 0.9995546738998421 loss: 261.41119968636474 best fit 261.41119968636474\n",
      "202 :test prob  0.9993239140115785 loss: 336.76388913454736\n",
      "203 :train prob 0.9995749159953038 loss: 258.31967050963664 best fit 258.31967050963664\n",
      "203 :test prob  0.9993522529452249 loss: 330.693254463808\n",
      "204 :train prob 0.9995749159953038 loss: 257.490312411737 best fit 257.490312411737\n",
      "204 :test prob  0.9993522529452249 loss: 329.93399402536556\n",
      "205 :train prob 0.9995668191571191 loss: 257.42940387185797 best fit 257.42940387185797\n",
      "205 :test prob  0.9993522529452249 loss: 330.4071576062489\n",
      "206 :train prob 0.9995668191571191 loss: 256.163815981347 best fit 256.163815981347\n",
      "206 :test prob  0.9993522529452249 loss: 329.2322430153533\n",
      "207 :train prob 0.9995627707380268 loss: 255.59279686865193 best fit 255.59279686865193\n",
      "207 :test prob  0.9993563013643172 loss: 327.9705506057552\n",
      "208 :train prob 0.9995627707380268 loss: 254.0302920930452 best fit 254.0302920930452\n",
      "208 :test prob  0.9993805918788713 loss: 324.28376425508293\n",
      "209 :train prob 0.9995627707380268 loss: 253.61538837282413 best fit 253.61538837282413\n",
      "209 :test prob  0.9993805918788713 loss: 323.8482993747899\n",
      "210 :train prob 0.9995668191571191 loss: 253.18385029397808 best fit 253.18385029397808\n",
      "210 :test prob  0.9993805918788713 loss: 323.29682785551955\n",
      "211 :train prob 0.9995749159953038 loss: 252.38378125841115 best fit 252.38378125841115\n",
      "211 :test prob  0.9993724950406866 loss: 322.8305447559605\n",
      "212 :train prob 0.9995749159953038 loss: 251.29013175433067 best fit 251.29013175433067\n",
      "212 :test prob  0.999376543459779 loss: 321.95743496043735\n",
      "213 :train prob 0.9995749159953038 loss: 250.7989140457382 best fit 250.7989140457382\n",
      "213 :test prob  0.999376543459779 loss: 321.4560052295807\n",
      "214 :train prob 0.9995789644143962 loss: 250.23166284542535 best fit 250.23166284542535\n",
      "214 :test prob  0.9993805918788713 loss: 320.26981272892556\n",
      "215 :train prob 0.9995911096716732 loss: 248.67277302837888 best fit 248.67277302837888\n",
      "215 :test prob  0.9993805918788713 loss: 318.74797242868124\n",
      "216 :train prob 0.9995911096716732 loss: 247.85201344496355 best fit 247.85201344496355\n",
      "216 :test prob  0.999376543459779 loss: 318.8535250013546\n",
      "217 :train prob 0.9995911096716732 loss: 247.40322167029316 best fit 247.40322167029316\n",
      "217 :test prob  0.9993805918788713 loss: 318.84291362516296\n",
      "218 :train prob 0.9995911096716732 loss: 246.93092260459827 best fit 246.93092260459827\n",
      "218 :test prob  0.9993805918788713 loss: 318.6044216746422\n",
      "219 :train prob 0.9995911096716732 loss: 246.4163551154885 best fit 246.4163551154885\n",
      "219 :test prob  0.999376543459779 loss: 318.9763026594348\n",
      "220 :train prob 0.9996073033480426 loss: 244.28684579362306 best fit 244.28684579362306\n",
      "220 :test prob  0.9993805918788713 loss: 317.665606003372\n",
      "221 :train prob 0.9996073033480426 loss: 244.12894259807328 best fit 244.12894259807328\n",
      "221 :test prob  0.9993805918788713 loss: 317.70457182108737\n",
      "222 :train prob 0.9996154001862273 loss: 240.98540567314984 best fit 240.98540567314984\n",
      "222 :test prob  0.9993967855552407 loss: 316.0493043442888\n",
      "223 :train prob 0.9996113517671349 loss: 240.04500945851888 best fit 240.04500945851888\n",
      "223 :test prob  0.999400833974333 loss: 313.9823913626286\n",
      "224 :train prob 0.9996113517671349 loss: 238.15568968605857 best fit 238.15568968605857\n",
      "224 :test prob  0.999400833974333 loss: 310.8865233154565\n",
      "225 :train prob 0.9996154001862273 loss: 237.46196848949464 best fit 237.46196848949464\n",
      "225 :test prob  0.999400833974333 loss: 310.67627751699547\n",
      "226 :train prob 0.999623497024412 loss: 236.16136871087704 best fit 236.16136871087704\n",
      "226 :test prob  0.999400833974333 loss: 310.4693286369991\n",
      "227 :train prob 0.9996275454435043 loss: 234.84703358366122 best fit 234.84703358366122\n",
      "227 :test prob  0.9993967855552407 loss: 310.414323273924\n",
      "228 :train prob 0.9996275454435043 loss: 234.46476274981327 best fit 234.46476274981327\n",
      "228 :test prob  0.9993927371361483 loss: 310.0524495328456\n",
      "229 :train prob 0.999623497024412 loss: 233.76732908187333 best fit 233.76732908187333\n",
      "229 :test prob  0.999400833974333 loss: 308.86190932638937\n",
      "230 :train prob 0.999635642281689 loss: 233.3496172390359 best fit 233.3496172390359\n",
      "230 :test prob  0.9994048823934254 loss: 308.6384506301244\n",
      "231 :train prob 0.9996315938625967 loss: 233.32961720987024 best fit 233.32961720987024\n",
      "231 :test prob  0.9993927371361483 loss: 307.749950888315\n",
      "232 :train prob 0.9996315938625967 loss: 232.76098152019122 best fit 232.76098152019122\n",
      "232 :test prob  0.999400833974333 loss: 305.9268798980591\n",
      "233 :train prob 0.9996315938625967 loss: 231.72080565710826 best fit 231.72080565710826\n",
      "233 :test prob  0.9994170276507024 loss: 304.36242809472327\n",
      "234 :train prob 0.9996396907007813 loss: 230.54421147343243 best fit 230.54421147343243\n",
      "234 :test prob  0.9994129792316101 loss: 304.47203286777875\n",
      "235 :train prob 0.9996396907007813 loss: 229.66112591317113 best fit 229.66112591317113\n",
      "235 :test prob  0.9994170276507024 loss: 303.3363846363854\n",
      "236 :train prob 0.999647787538966 loss: 228.2735029827332 best fit 228.2735029827332\n",
      "236 :test prob  0.9994372697461641 loss: 302.4898039836054\n",
      "237 :train prob 0.9996437391198737 loss: 227.63441812549374 best fit 227.63441812549374\n",
      "237 :test prob  0.9994413181652565 loss: 302.3692300550738\n",
      "238 :train prob 0.9996437391198737 loss: 227.21878167495885 best fit 227.21878167495885\n",
      "238 :test prob  0.9994291729079794 loss: 302.24236590357253\n",
      "239 :train prob 0.9996437391198737 loss: 226.65280656553915 best fit 226.65280656553915\n",
      "239 :test prob  0.9994332213270718 loss: 301.6528316996134\n",
      "240 :train prob 0.999647787538966 loss: 225.98866264737066 best fit 225.98866264737066\n",
      "240 :test prob  0.9994251244888871 loss: 300.05115154714895\n",
      "241 :train prob 0.999647787538966 loss: 222.6387619568281 best fit 222.6387619568281\n",
      "241 :test prob  0.9994210760697948 loss: 297.1548083938933\n",
      "242 :train prob 0.999647787538966 loss: 221.4948675941548 best fit 221.4948675941548\n",
      "242 :test prob  0.9994251244888871 loss: 296.1483958370997\n",
      "243 :train prob 0.999647787538966 loss: 220.6627370977823 best fit 220.6627370977823\n",
      "243 :test prob  0.9994210760697948 loss: 294.57910856679865\n",
      "244 :train prob 0.999647787538966 loss: 219.12067817345084 best fit 219.12067817345084\n",
      "244 :test prob  0.9994251244888871 loss: 293.22489341271904\n",
      "245 :train prob 0.999647787538966 loss: 218.59914400081183 best fit 218.59914400081183\n",
      "245 :test prob  0.9994251244888871 loss: 292.9854254293416\n",
      "246 :train prob 0.9996518359580584 loss: 217.49317279072102 best fit 217.49317279072102\n",
      "246 :test prob  0.9994291729079794 loss: 291.6961477848552\n",
      "247 :train prob 0.9996518359580584 loss: 217.16831952477798 best fit 217.16831952477798\n",
      "247 :test prob  0.9994291729079794 loss: 291.8847728617627\n",
      "248 :train prob 0.9996518359580584 loss: 216.88266044584716 best fit 216.88266044584716\n",
      "248 :test prob  0.9994332213270718 loss: 291.6618261239111\n",
      "249 :train prob 0.9996518359580584 loss: 216.10021315526518 best fit 216.10021315526518\n",
      "249 :test prob  0.9994210760697948 loss: 291.45587008394887\n",
      "250 :train prob 0.999647787538966 loss: 215.51694212736425 best fit 215.51694212736425\n",
      "250 :test prob  0.9994251244888871 loss: 290.7752632140288\n",
      "251 :train prob 0.999647787538966 loss: 214.93228998569992 best fit 214.93228998569992\n",
      "251 :test prob  0.9994251244888871 loss: 290.8075302930643\n",
      "252 :train prob 0.9996518359580584 loss: 214.85530647961056 best fit 214.85530647961056\n",
      "252 :test prob  0.9994372697461641 loss: 290.4726377864742\n",
      "253 :train prob 0.999647787538966 loss: 214.53615170457496 best fit 214.53615170457496\n",
      "253 :test prob  0.9994372697461641 loss: 289.3804541420925\n",
      "254 :train prob 0.999647787538966 loss: 214.15943920987246 best fit 214.15943920987246\n",
      "254 :test prob  0.9994372697461641 loss: 288.83570200470086\n",
      "255 :train prob 0.9996518359580584 loss: 213.77012115360915 best fit 213.77012115360915\n",
      "255 :test prob  0.9994413181652565 loss: 287.86163003972945\n",
      "256 :train prob 0.9996599327962431 loss: 209.09652789767813 best fit 209.09652789767813\n",
      "256 :test prob  0.9994534634225335 loss: 284.2780545832818\n",
      "257 :train prob 0.9996599327962431 loss: 208.38061102256398 best fit 208.38061102256398\n",
      "257 :test prob  0.9994615602607182 loss: 283.494746625056\n",
      "258 :train prob 0.9996558843771507 loss: 207.76587710985984 best fit 207.76587710985984\n",
      "258 :test prob  0.9994575118416258 loss: 283.5722117833806\n",
      "259 :train prob 0.9996599327962431 loss: 207.37362263013932 best fit 207.37362263013932\n",
      "259 :test prob  0.9994656086798105 loss: 283.14264177148584\n",
      "260 :train prob 0.9996639812153354 loss: 206.38771141904633 best fit 206.38771141904633\n",
      "260 :test prob  0.9994656086798105 loss: 284.0992670647425\n",
      "261 :train prob 0.9996639812153354 loss: 205.84414733507725 best fit 205.84414733507725\n",
      "261 :test prob  0.9994656086798105 loss: 283.45767913230634\n",
      "262 :train prob 0.9996639812153354 loss: 205.7141291172193 best fit 205.7141291172193\n",
      "262 :test prob  0.9994656086798105 loss: 283.6412851370827\n",
      "263 :train prob 0.9996639812153354 loss: 205.25225993779415 best fit 205.25225993779415\n",
      "263 :test prob  0.9994656086798105 loss: 282.75625320615217\n",
      "264 :train prob 0.9996599327962431 loss: 204.99296153426695 best fit 204.99296153426695\n",
      "264 :test prob  0.9994615602607182 loss: 282.7785926132207\n",
      "265 :train prob 0.9996599327962431 loss: 204.86258845557487 best fit 204.86258845557487\n",
      "265 :test prob  0.9994615602607182 loss: 283.01937421960207\n",
      "266 :train prob 0.9996639812153354 loss: 204.18612548483566 best fit 204.18612548483566\n",
      "266 :test prob  0.9994656086798105 loss: 282.3352783038865\n",
      "267 :train prob 0.9996639812153354 loss: 203.8666943040366 best fit 203.8666943040366\n",
      "267 :test prob  0.9994656086798105 loss: 282.0864109030173\n",
      "268 :train prob 0.9996720780535201 loss: 202.98083079840836 best fit 202.98083079840836\n",
      "268 :test prob  0.9994656086798105 loss: 283.1781434697212\n",
      "269 :train prob 0.9996761264726124 loss: 202.76933140726712 best fit 202.76933140726712\n",
      "269 :test prob  0.9994777539370876 loss: 283.78941930225125\n",
      "270 :train prob 0.9996761264726124 loss: 202.4969275158677 best fit 202.4969275158677\n",
      "270 :test prob  0.9994777539370876 loss: 283.33067742787836\n",
      "271 :train prob 0.9996720780535201 loss: 199.18636659196835 best fit 199.18636659196835\n",
      "271 :test prob  0.9994818023561799 loss: 280.0745502149437\n",
      "272 :train prob 0.9996720780535201 loss: 198.94303131028383 best fit 198.94303131028383\n",
      "272 :test prob  0.9994777539370876 loss: 279.90473947061207\n",
      "273 :train prob 0.9996720780535201 loss: 198.64013459191716 best fit 198.64013459191716\n",
      "273 :test prob  0.9994777539370876 loss: 280.21072408123985\n",
      "274 :train prob 0.9996761264726124 loss: 198.1219280559659 best fit 198.1219280559659\n",
      "274 :test prob  0.9994615602607182 loss: 279.7234640433886\n",
      "275 :train prob 0.9996761264726124 loss: 197.7735186006062 best fit 197.7735186006062\n",
      "275 :test prob  0.9994615602607182 loss: 279.31159181075805\n",
      "276 :train prob 0.9996761264726124 loss: 197.32947348652593 best fit 197.32947348652593\n",
      "276 :test prob  0.9994656086798105 loss: 278.43032790584783\n",
      "277 :train prob 0.9996720780535201 loss: 196.08834355767488 best fit 196.08834355767488\n",
      "277 :test prob  0.9994696570989029 loss: 278.1966484085216\n",
      "278 :train prob 0.9996720780535201 loss: 194.90893498606462 best fit 194.90893498606462\n",
      "278 :test prob  0.9994939476134569 loss: 276.05029389008007\n",
      "279 :train prob 0.9996720780535201 loss: 194.6317483615216 best fit 194.6317483615216\n",
      "279 :test prob  0.9994939476134569 loss: 275.2936490568368\n",
      "280 :train prob 0.9996680296344278 loss: 194.18335874523126 best fit 194.18335874523126\n",
      "280 :test prob  0.9994898991943646 loss: 274.71500355632344\n",
      "281 :train prob 0.9996680296344278 loss: 193.80932252336186 best fit 193.80932252336186\n",
      "281 :test prob  0.9994858507752723 loss: 274.1748373874153\n",
      "282 :train prob 0.9996680296344278 loss: 194.01294761065697 best fit 194.01294761065697\n",
      "282 :test prob  0.9994858507752723 loss: 274.6354073052962\n",
      "283 :train prob 0.9996720780535201 loss: 194.04478995565916 best fit 194.04478995565916\n",
      "283 :test prob  0.9994818023561799 loss: 275.3905399678071\n",
      "284 :train prob 0.9996761264726124 loss: 194.1780174102856 best fit 194.1780174102856\n",
      "284 :test prob  0.9994858507752723 loss: 275.41708268892637\n",
      "285 :train prob 0.9996801748917048 loss: 193.9633183567408 best fit 193.9633183567408\n",
      "285 :test prob  0.9994858507752723 loss: 275.3747684162908\n",
      "286 :train prob 0.9996801748917048 loss: 194.05887513691067 best fit 194.05887513691067\n",
      "286 :test prob  0.9994777539370876 loss: 275.562531465978\n",
      "287 :train prob 0.9996801748917048 loss: 193.15066448786536 best fit 193.15066448786536\n",
      "287 :test prob  0.9994818023561799 loss: 274.8809781658526\n",
      "288 :train prob 0.9996801748917048 loss: 192.65154096818048 best fit 192.65154096818048\n",
      "288 :test prob  0.9994818023561799 loss: 274.9526714954187\n",
      "289 :train prob 0.9996801748917048 loss: 192.2416854708112 best fit 192.2416854708112\n",
      "289 :test prob  0.9994858507752723 loss: 274.0929978391563\n",
      "290 :train prob 0.9996801748917048 loss: 191.92961891826204 best fit 191.92961891826204\n",
      "290 :test prob  0.9994818023561799 loss: 274.3855847695237\n",
      "291 :train prob 0.9996801748917048 loss: 191.94663204470928 best fit 191.94663204470928\n",
      "291 :test prob  0.9994858507752723 loss: 274.97648467778737\n",
      "292 :train prob 0.9996801748917048 loss: 191.3745317568293 best fit 191.3745317568293\n",
      "292 :test prob  0.9994858507752723 loss: 273.5979913506748\n",
      "293 :train prob 0.9996801748917048 loss: 189.8539906208838 best fit 189.8539906208838\n",
      "293 :test prob  0.9994939476134569 loss: 273.1288095342367\n",
      "294 :train prob 0.9996801748917048 loss: 188.91788942570847 best fit 188.91788942570847\n",
      "294 :test prob  0.9994939476134569 loss: 271.9769489055996\n",
      "295 :train prob 0.9996801748917048 loss: 189.0188667856825 best fit 189.0188667856825\n",
      "295 :test prob  0.9995020444516416 loss: 271.50163199688905\n",
      "296 :train prob 0.9996801748917048 loss: 188.7091293800127 best fit 188.7091293800127\n",
      "296 :test prob  0.9994979960325493 loss: 271.7515908252268\n",
      "297 :train prob 0.9996801748917048 loss: 187.50324601196635 best fit 187.50324601196635\n",
      "297 :test prob  0.9994979960325493 loss: 270.728857920016\n",
      "298 :train prob 0.9996801748917048 loss: 187.0131366484632 best fit 187.0131366484632\n",
      "298 :test prob  0.9994979960325493 loss: 270.2437567071205\n",
      "299 :train prob 0.9996761264726124 loss: 186.84074340385325 best fit 186.84074340385325\n",
      "299 :test prob  0.9994979960325493 loss: 269.20305267087\n",
      "300 :train prob 0.9996680296344278 loss: 185.604757389103 best fit 185.604757389103\n",
      "300 :test prob  0.9995020444516416 loss: 267.90012618610047\n",
      "301 :train prob 0.9996720780535201 loss: 185.32348065912748 best fit 185.32348065912748\n",
      "301 :test prob  0.9995020444516416 loss: 267.83143166336237\n",
      "302 :train prob 0.9996720780535201 loss: 184.34822458582107 best fit 184.34822458582107\n",
      "302 :test prob  0.9994979960325493 loss: 267.6152514884548\n",
      "303 :train prob 0.9996720780535201 loss: 183.83154156710128 best fit 183.83154156710128\n",
      "303 :test prob  0.9994979960325493 loss: 267.1849277769784\n",
      "304 :train prob 0.9996761264726124 loss: 183.5185123096896 best fit 183.5185123096896\n",
      "304 :test prob  0.9994939476134569 loss: 266.62856647077325\n",
      "305 :train prob 0.9996801748917048 loss: 183.1633293473991 best fit 183.1633293473991\n",
      "305 :test prob  0.9994939476134569 loss: 266.9229931241653\n",
      "306 :train prob 0.9996761264726124 loss: 182.2043484493409 best fit 182.2043484493409\n",
      "306 :test prob  0.9994979960325493 loss: 265.44275114527585\n",
      "307 :train prob 0.9996761264726124 loss: 181.89991724034846 best fit 181.89991724034846\n",
      "307 :test prob  0.9994979960325493 loss: 265.1605110566986\n",
      "308 :train prob 0.9996761264726124 loss: 181.6366583230602 best fit 181.6366583230602\n",
      "308 :test prob  0.9994979960325493 loss: 265.2081951878536\n",
      "309 :train prob 0.9996842233107971 loss: 181.44766297911946 best fit 181.44766297911946\n",
      "309 :test prob  0.999506092870734 loss: 265.127469589529\n",
      "310 :train prob 0.9996761264726124 loss: 181.1169394398826 best fit 181.1169394398826\n",
      "310 :test prob  0.9995020444516416 loss: 264.97305773086634\n",
      "311 :train prob 0.9996761264726124 loss: 179.95638536618844 best fit 179.95638536618844\n",
      "311 :test prob  0.9995020444516416 loss: 264.89670444625295\n",
      "312 :train prob 0.9996761264726124 loss: 179.68689806789004 best fit 179.68689806789004\n",
      "312 :test prob  0.9995020444516416 loss: 264.7410879111547\n",
      "313 :train prob 0.9996761264726124 loss: 179.25357216607833 best fit 179.25357216607833\n",
      "313 :test prob  0.9995020444516416 loss: 264.5162330543246\n",
      "314 :train prob 0.9996761264726124 loss: 174.74599997051882 best fit 174.74599997051882\n",
      "314 :test prob  0.9994939476134569 loss: 262.1512321165842\n",
      "315 :train prob 0.9996761264726124 loss: 174.40343782974998 best fit 174.40343782974998\n",
      "315 :test prob  0.9994979960325493 loss: 261.87722162841413\n",
      "316 :train prob 0.9996761264726124 loss: 173.32961992135333 best fit 173.32961992135333\n",
      "316 :test prob  0.9994939476134569 loss: 261.80417397931956\n",
      "317 :train prob 0.9996842233107971 loss: 173.14357972226284 best fit 173.14357972226284\n",
      "317 :test prob  0.9994858507752723 loss: 261.72637670631394\n",
      "318 :train prob 0.9996842233107971 loss: 172.335440963007 best fit 172.335440963007\n",
      "318 :test prob  0.9994939476134569 loss: 261.54368058341066\n",
      "319 :train prob 0.9996842233107971 loss: 172.0360435920222 best fit 172.0360435920222\n",
      "319 :test prob  0.9994939476134569 loss: 261.61962947965503\n",
      "320 :train prob 0.9996842233107971 loss: 172.03877909459712 best fit 172.03877909459712\n",
      "320 :test prob  0.9994858507752723 loss: 261.8478540919766\n",
      "321 :train prob 0.9996842233107971 loss: 171.19166208246216 best fit 171.19166208246216\n",
      "321 :test prob  0.9994898991943646 loss: 261.55841559941905\n",
      "322 :train prob 0.9996842233107971 loss: 170.87310468899878 best fit 170.87310468899878\n",
      "322 :test prob  0.9994898991943646 loss: 260.6539697955619\n",
      "323 :train prob 0.9996801748917048 loss: 170.74875865206835 best fit 170.74875865206835\n",
      "323 :test prob  0.9994858507752723 loss: 260.5964003039962\n",
      "324 :train prob 0.9996801748917048 loss: 170.13725326489907 best fit 170.13725326489907\n",
      "324 :test prob  0.9994898991943646 loss: 259.85737151083686\n",
      "325 :train prob 0.9996801748917048 loss: 169.85847746855217 best fit 169.85847746855217\n",
      "325 :test prob  0.9994898991943646 loss: 259.0440149887015\n",
      "326 :train prob 0.9996801748917048 loss: 169.6958828229097 best fit 169.6958828229097\n",
      "326 :test prob  0.9994898991943646 loss: 258.57738124474935\n",
      "327 :train prob 0.9996801748917048 loss: 169.3192753628511 best fit 169.3192753628511\n",
      "327 :test prob  0.9994979960325493 loss: 258.5092471794949\n",
      "328 :train prob 0.9996801748917048 loss: 169.074407745823 best fit 169.074407745823\n",
      "328 :test prob  0.9994898991943646 loss: 258.1973443965289\n",
      "329 :train prob 0.9996801748917048 loss: 168.6505465541867 best fit 168.6505465541867\n",
      "329 :test prob  0.9994898991943646 loss: 257.5858600198285\n",
      "330 :train prob 0.9996761264726124 loss: 168.62166341328881 best fit 168.62166341328881\n",
      "330 :test prob  0.9994858507752723 loss: 258.04957324512213\n",
      "331 :train prob 0.9997125622444435 loss: 167.21878365692638 best fit 167.21878365692638\n",
      "331 :test prob  0.9995020444516416 loss: 257.0723031023966\n",
      "332 :train prob 0.9997125622444435 loss: 167.2863408387063 best fit 167.2863408387063\n",
      "332 :test prob  0.9995141897089187 loss: 257.4318442422336\n",
      "333 :train prob 0.9997125622444435 loss: 167.16455501498695 best fit 167.16455501498695\n",
      "333 :test prob  0.9995101412898263 loss: 257.8468564178166\n",
      "334 :train prob 0.9997125622444435 loss: 166.57032945217261 best fit 166.57032945217261\n",
      "334 :test prob  0.9995101412898263 loss: 256.94775345666505\n",
      "335 :train prob 0.9997125622444435 loss: 166.36327836111002 best fit 166.36327836111002\n",
      "335 :test prob  0.9995101412898263 loss: 256.48401359185715\n",
      "336 :train prob 0.9997125622444435 loss: 166.35006010751758 best fit 166.35006010751758\n",
      "336 :test prob  0.9995101412898263 loss: 256.2678040637581\n",
      "337 :train prob 0.9997125622444435 loss: 166.20566008026358 best fit 166.20566008026358\n",
      "337 :test prob  0.9995101412898263 loss: 255.9756016359951\n",
      "338 :train prob 0.9997125622444435 loss: 166.12270276951975 best fit 166.12270276951975\n",
      "338 :test prob  0.9995101412898263 loss: 256.2694122368042\n",
      "339 :train prob 0.9997125622444435 loss: 165.89141348216086 best fit 165.89141348216086\n",
      "339 :test prob  0.9995101412898263 loss: 255.55509827889136\n",
      "340 :train prob 0.9997125622444435 loss: 165.22951319806037 best fit 165.22951319806037\n",
      "340 :test prob  0.9995141897089187 loss: 255.44005779695553\n",
      "341 :train prob 0.9997125622444435 loss: 165.0817805977506 best fit 165.0817805977506\n",
      "341 :test prob  0.9995141897089187 loss: 254.90898686234937\n",
      "342 :train prob 0.9997206590826282 loss: 164.79695267690914 best fit 164.79695267690914\n",
      "342 :test prob  0.9995263349661957 loss: 254.89176137127475\n",
      "343 :train prob 0.9997206590826282 loss: 164.35628819654073 best fit 164.35628819654073\n",
      "343 :test prob  0.9995222865471034 loss: 254.20546633487405\n",
      "344 :train prob 0.9997206590826282 loss: 164.2235449357959 best fit 164.2235449357959\n",
      "344 :test prob  0.999530383385288 loss: 254.15706646456698\n",
      "345 :train prob 0.9997206590826282 loss: 163.8952906489908 best fit 163.8952906489908\n",
      "345 :test prob  0.999530383385288 loss: 253.61806855047172\n",
      "346 :train prob 0.9997206590826282 loss: 163.77713420156522 best fit 163.77713420156522\n",
      "346 :test prob  0.999530383385288 loss: 253.79678587660706\n",
      "347 :train prob 0.9997206590826282 loss: 163.64019494562714 best fit 163.64019494562714\n",
      "347 :test prob  0.9995263349661957 loss: 253.32850735058645\n",
      "348 :train prob 0.9997206590826282 loss: 163.36485822358378 best fit 163.36485822358378\n",
      "348 :test prob  0.9995222865471034 loss: 254.03566718467954\n",
      "349 :train prob 0.9997247075017206 loss: 162.08775976228785 best fit 162.08775976228785\n",
      "349 :test prob  0.999518238128011 loss: 252.4583160638416\n",
      "350 :train prob 0.9997206590826282 loss: 161.23233375994562 best fit 161.23233375994562\n",
      "350 :test prob  0.999530383385288 loss: 251.9691797582272\n",
      "351 :train prob 0.9997247075017206 loss: 161.08324662162457 best fit 161.08324662162457\n",
      "351 :test prob  0.9995425286425651 loss: 251.93688319576458\n",
      "352 :train prob 0.9997247075017206 loss: 160.4281368211986 best fit 160.4281368211986\n",
      "352 :test prob  0.9995425286425651 loss: 251.21181226928107\n",
      "353 :train prob 0.9997247075017206 loss: 160.1988687350357 best fit 160.1988687350357\n",
      "353 :test prob  0.9995425286425651 loss: 250.76329506624526\n",
      "354 :train prob 0.9997247075017206 loss: 160.01690677343524 best fit 160.01690677343524\n",
      "354 :test prob  0.9995425286425651 loss: 250.46135382645943\n",
      "355 :train prob 0.9997247075017206 loss: 159.9309254687329 best fit 159.9309254687329\n",
      "355 :test prob  0.9995425286425651 loss: 250.10852652557975\n",
      "356 :train prob 0.9997247075017206 loss: 159.5827266509363 best fit 159.5827266509363\n",
      "356 :test prob  0.9995425286425651 loss: 249.69535419917074\n",
      "357 :train prob 0.9997247075017206 loss: 159.23320243742705 best fit 159.23320243742705\n",
      "357 :test prob  0.9995465770616574 loss: 249.87900177705612\n",
      "358 :train prob 0.9997247075017206 loss: 159.00109373210782 best fit 159.00109373210782\n",
      "358 :test prob  0.9995425286425651 loss: 250.18283948321027\n",
      "359 :train prob 0.9997247075017206 loss: 158.75806000425806 best fit 158.75806000425806\n",
      "359 :test prob  0.9995425286425651 loss: 250.09836407439707\n",
      "360 :train prob 0.9997287559208129 loss: 155.50339157792214 best fit 155.50339157792214\n",
      "360 :test prob  0.9995425286425651 loss: 245.91876572878832\n",
      "361 :train prob 0.9997287559208129 loss: 155.2158040468361 best fit 155.2158040468361\n",
      "361 :test prob  0.9995425286425651 loss: 246.45464163388368\n",
      "362 :train prob 0.9997287559208129 loss: 155.1670634469987 best fit 155.1670634469987\n",
      "362 :test prob  0.9995425286425651 loss: 246.16240279059386\n",
      "363 :train prob 0.9997368527589976 loss: 153.98415924547 best fit 153.98415924547\n",
      "363 :test prob  0.9995546738998421 loss: 244.75725702400143\n",
      "364 :train prob 0.9997368527589976 loss: 154.14164438470297 best fit 154.14164438470297\n",
      "364 :test prob  0.9995506254807498 loss: 245.40932921120304\n",
      "365 :train prob 0.9997368527589976 loss: 153.51654930497392 best fit 153.51654930497392\n",
      "365 :test prob  0.9995546738998421 loss: 244.55418186243287\n",
      "366 :train prob 0.9997328043399053 loss: 153.4858208382851 best fit 153.4858208382851\n",
      "366 :test prob  0.9995546738998421 loss: 244.47727906745976\n",
      "367 :train prob 0.9997328043399053 loss: 153.03278885424837 best fit 153.03278885424837\n",
      "367 :test prob  0.9995546738998421 loss: 243.7862510035674\n",
      "368 :train prob 0.9997328043399053 loss: 152.81281805439426 best fit 152.81281805439426\n",
      "368 :test prob  0.9995546738998421 loss: 243.3805906175479\n",
      "369 :train prob 0.9997328043399053 loss: 152.70962368978533 best fit 152.70962368978533\n",
      "369 :test prob  0.9995546738998421 loss: 243.18874765534142\n",
      "370 :train prob 0.99974090117809 loss: 152.363322190395 best fit 152.363322190395\n",
      "370 :test prob  0.9995546738998421 loss: 243.42622679680676\n",
      "371 :train prob 0.99974090117809 loss: 152.2419232834714 best fit 152.2419232834714\n",
      "371 :test prob  0.9995546738998421 loss: 243.22808393224932\n",
      "372 :train prob 0.99974090117809 loss: 151.26595552460924 best fit 151.26595552460924\n",
      "372 :test prob  0.9995627707380268 loss: 241.3141014303144\n",
      "373 :train prob 0.99974090117809 loss: 151.17287038550867 best fit 151.17287038550867\n",
      "373 :test prob  0.9995627707380268 loss: 241.03934564786348\n",
      "374 :train prob 0.99974090117809 loss: 151.0884449991726 best fit 151.0884449991726\n",
      "374 :test prob  0.9995668191571191 loss: 241.1540041546404\n",
      "375 :train prob 0.9997449495971823 loss: 149.99984280301342 best fit 149.99984280301342\n",
      "375 :test prob  0.9995708675762115 loss: 240.5850442236709\n",
      "376 :train prob 0.99974090117809 loss: 150.04424699777923 best fit 150.04424699777923\n",
      "376 :test prob  0.9995627707380268 loss: 242.55628000699198\n",
      "377 :train prob 0.99974090117809 loss: 149.88673483121894 best fit 149.88673483121894\n",
      "377 :test prob  0.9995627707380268 loss: 242.65096554511115\n",
      "378 :train prob 0.99974090117809 loss: 149.58034962417068 best fit 149.58034962417068\n",
      "378 :test prob  0.9995587223189345 loss: 241.90678142854372\n",
      "379 :train prob 0.9997449495971823 loss: 149.2545172324961 best fit 149.2545172324961\n",
      "379 :test prob  0.9995627707380268 loss: 242.53148171827766\n",
      "380 :train prob 0.9997489980162746 loss: 149.24972777421382 best fit 149.24972777421382\n",
      "380 :test prob  0.9995668191571191 loss: 243.10624744007612\n",
      "381 :train prob 0.9997489980162746 loss: 149.02171322597843 best fit 149.02171322597843\n",
      "381 :test prob  0.9995627707380268 loss: 242.49277024873243\n",
      "382 :train prob 0.9997368527589976 loss: 147.4741653361206 best fit 147.4741653361206\n",
      "382 :test prob  0.9995627707380268 loss: 238.79537988849114\n",
      "383 :train prob 0.99974090117809 loss: 146.5887637989347 best fit 146.5887637989347\n",
      "383 :test prob  0.9995708675762115 loss: 239.4778316744395\n",
      "384 :train prob 0.99974090117809 loss: 145.74494867961894 best fit 145.74494867961894\n",
      "384 :test prob  0.9995708675762115 loss: 238.19380295947113\n",
      "385 :train prob 0.9997449495971823 loss: 145.098098767699 best fit 145.098098767699\n",
      "385 :test prob  0.9995708675762115 loss: 237.50101792626015\n",
      "386 :train prob 0.9997449495971823 loss: 144.78389601135305 best fit 144.78389601135305\n",
      "386 :test prob  0.9995708675762115 loss: 236.89683238990588\n",
      "387 :train prob 0.9997449495971823 loss: 144.21898533098263 best fit 144.21898533098263\n",
      "387 :test prob  0.9995749159953038 loss: 236.886866174849\n",
      "388 :train prob 0.9997449495971823 loss: 143.72090080372476 best fit 143.72090080372476\n",
      "388 :test prob  0.9995749159953038 loss: 235.8885707913012\n",
      "389 :train prob 0.9997449495971823 loss: 143.45233793936782 best fit 143.45233793936782\n",
      "389 :test prob  0.9995749159953038 loss: 235.0063156882361\n",
      "390 :train prob 0.9997570948544593 loss: 138.9781605030221 best fit 138.9781605030221\n",
      "390 :test prob  0.9995830128334885 loss: 230.11160595044348\n",
      "391 :train prob 0.9997570948544593 loss: 138.51386367988766 best fit 138.51386367988766\n",
      "391 :test prob  0.9995870612525809 loss: 229.9036447329002\n",
      "392 :train prob 0.9997570948544593 loss: 138.34389145429964 best fit 138.34389145429964\n",
      "392 :test prob  0.9995870612525809 loss: 229.4119089391761\n",
      "393 :train prob 0.9997570948544593 loss: 138.18624625142158 best fit 138.18624625142158\n",
      "393 :test prob  0.9995870612525809 loss: 229.06842825568546\n",
      "394 :train prob 0.9997611432735517 loss: 137.64495851104803 best fit 137.64495851104803\n",
      "394 :test prob  0.9995870612525809 loss: 229.16141505236422\n",
      "395 :train prob 0.9997611432735517 loss: 135.99178773694442 best fit 135.99178773694442\n",
      "395 :test prob  0.9995830128334885 loss: 227.19439764379834\n",
      "396 :train prob 0.999765191692644 loss: 134.9465260353403 best fit 134.9465260353403\n",
      "396 :test prob  0.9995830128334885 loss: 226.06220947225512\n",
      "397 :train prob 0.9997611432735517 loss: 132.39554070770572 best fit 132.39554070770572\n",
      "397 :test prob  0.9995789644143962 loss: 225.00267083822263\n",
      "398 :train prob 0.999765191692644 loss: 131.78087253353962 best fit 131.78087253353962\n",
      "398 :test prob  0.9995870612525809 loss: 224.66183165551112\n",
      "399 :train prob 0.999765191692644 loss: 131.3276138110488 best fit 131.3276138110488\n",
      "399 :test prob  0.9995911096716732 loss: 223.9429610463392\n",
      "400 :train prob 0.9997692401117364 loss: 130.49770136552482 best fit 130.49770136552482\n",
      "400 :test prob  0.9995870612525809 loss: 224.14953357500508\n",
      "401 :train prob 0.9997773369499211 loss: 129.72482685434107 best fit 129.72482685434107\n",
      "401 :test prob  0.9995951580907656 loss: 224.75760719554327\n",
      "402 :train prob 0.9997773369499211 loss: 129.49348273131437 best fit 129.49348273131437\n",
      "402 :test prob  0.9995911096716732 loss: 224.75195501630444\n",
      "403 :train prob 0.9997773369499211 loss: 129.17839681803378 best fit 129.17839681803378\n",
      "403 :test prob  0.9995911096716732 loss: 225.35741070769689\n",
      "404 :train prob 0.9997773369499211 loss: 129.09495270018252 best fit 129.09495270018252\n",
      "404 :test prob  0.9995870612525809 loss: 225.39231415437547\n",
      "405 :train prob 0.9997732885308287 loss: 128.2735996801844 best fit 128.2735996801844\n",
      "405 :test prob  0.9995870612525809 loss: 225.70552524173317\n",
      "406 :train prob 0.9997773369499211 loss: 128.39490855504482 best fit 128.39490855504482\n",
      "406 :test prob  0.9995870612525809 loss: 226.4263861432365\n",
      "407 :train prob 0.9997773369499211 loss: 128.32019049655946 best fit 128.32019049655946\n",
      "407 :test prob  0.9995870612525809 loss: 226.85427041574445\n",
      "408 :train prob 0.9997732885308287 loss: 127.76017194755886 best fit 127.76017194755886\n",
      "408 :test prob  0.9995870612525809 loss: 225.69973195884913\n",
      "409 :train prob 0.9997732885308287 loss: 127.71446626489985 best fit 127.71446626489985\n",
      "409 :test prob  0.9995911096716732 loss: 225.83616777069938\n",
      "410 :train prob 0.9997773369499211 loss: 127.62206062559012 best fit 127.62206062559012\n",
      "410 :test prob  0.9995830128334885 loss: 225.78238633816272\n",
      "411 :train prob 0.9997773369499211 loss: 127.17758775866609 best fit 127.17758775866609\n",
      "411 :test prob  0.9995830128334885 loss: 224.95214410619872\n",
      "412 :train prob 0.9997773369499211 loss: 127.36130145624293 best fit 127.36130145624293\n",
      "412 :test prob  0.9995830128334885 loss: 225.0287969836637\n",
      "413 :train prob 0.9997773369499211 loss: 126.91748706883972 best fit 126.91748706883972\n",
      "413 :test prob  0.9995830128334885 loss: 224.27733697064596\n",
      "414 :train prob 0.9997773369499211 loss: 126.78619893409925 best fit 126.78619893409925\n",
      "414 :test prob  0.9995911096716732 loss: 223.8099948985909\n",
      "415 :train prob 0.9997773369499211 loss: 126.50558043501864 best fit 126.50558043501864\n",
      "415 :test prob  0.9995870612525809 loss: 222.93002526513177\n",
      "416 :train prob 0.9997773369499211 loss: 123.13478059621448 best fit 123.13478059621448\n",
      "416 :test prob  0.9995708675762115 loss: 222.38173417798524\n",
      "417 :train prob 0.9997773369499211 loss: 123.3725146272349 best fit 123.3725146272349\n",
      "417 :test prob  0.9995789644143962 loss: 222.55815484292074\n",
      "418 :train prob 0.9997773369499211 loss: 123.25489387257453 best fit 123.25489387257453\n",
      "418 :test prob  0.9995789644143962 loss: 221.85261854782286\n",
      "419 :train prob 0.9997773369499211 loss: 123.24336497875909 best fit 123.24336497875909\n",
      "419 :test prob  0.9995789644143962 loss: 221.80509564930915\n",
      "420 :train prob 0.9997732885308287 loss: 122.81175879778876 best fit 122.81175879778876\n",
      "420 :test prob  0.9995627707380268 loss: 221.77306719767992\n",
      "421 :train prob 0.9997773369499211 loss: 121.75720385302961 best fit 121.75720385302961\n",
      "421 :test prob  0.9995708675762115 loss: 220.18312304093894\n",
      "422 :train prob 0.9997773369499211 loss: 121.19172022568425 best fit 121.19172022568425\n",
      "422 :test prob  0.9995708675762115 loss: 219.96518748781384\n",
      "423 :train prob 0.9997773369499211 loss: 121.30698927004751 best fit 121.30698927004751\n",
      "423 :test prob  0.9995708675762115 loss: 220.1205011430434\n",
      "424 :train prob 0.9997773369499211 loss: 120.56278323728955 best fit 120.56278323728955\n",
      "424 :test prob  0.9995708675762115 loss: 219.31638893303142\n",
      "425 :train prob 0.9997773369499211 loss: 120.17409465496205 best fit 120.17409465496205\n",
      "425 :test prob  0.9995708675762115 loss: 218.77350193106705\n",
      "426 :train prob 0.9997773369499211 loss: 120.01429467880901 best fit 120.01429467880901\n",
      "426 :test prob  0.9995749159953038 loss: 218.26361090422944\n",
      "427 :train prob 0.9997773369499211 loss: 119.90257522426859 best fit 119.90257522426859\n",
      "427 :test prob  0.9995749159953038 loss: 218.31485697710445\n",
      "428 :train prob 0.9997773369499211 loss: 119.67161393129754 best fit 119.67161393129754\n",
      "428 :test prob  0.9995708675762115 loss: 218.16035989093115\n",
      "429 :train prob 0.9997813853690134 loss: 118.5167304920893 best fit 118.5167304920893\n",
      "429 :test prob  0.9995870612525809 loss: 216.73563302851383\n",
      "430 :train prob 0.9997813853690134 loss: 118.31545427359497 best fit 118.31545427359497\n",
      "430 :test prob  0.9995870612525809 loss: 216.5267138691054\n",
      "431 :train prob 0.9997813853690134 loss: 118.08528653728632 best fit 118.08528653728632\n",
      "431 :test prob  0.9995870612525809 loss: 216.2179234053187\n",
      "432 :train prob 0.9997813853690134 loss: 117.65455299221941 best fit 117.65455299221941\n",
      "432 :test prob  0.9995870612525809 loss: 216.19650824452663\n",
      "433 :train prob 0.9997813853690134 loss: 117.5912607335026 best fit 117.5912607335026\n",
      "433 :test prob  0.9995830128334885 loss: 216.2801856846401\n",
      "434 :train prob 0.9997854337881057 loss: 116.59641494357021 best fit 116.59641494357021\n",
      "434 :test prob  0.9995870612525809 loss: 214.17096636514364\n",
      "435 :train prob 0.9997894822071981 loss: 116.29295397174134 best fit 116.29295397174134\n",
      "435 :test prob  0.9995830128334885 loss: 213.61889711843372\n",
      "436 :train prob 0.9997854337881057 loss: 115.93736357454361 best fit 115.93736357454361\n",
      "436 :test prob  0.9995870612525809 loss: 213.13197909770375\n",
      "437 :train prob 0.9997854337881057 loss: 115.61924510461337 best fit 115.61924510461337\n",
      "437 :test prob  0.9995789644143962 loss: 213.1772869833573\n",
      "438 :train prob 0.9997854337881057 loss: 115.34777731755007 best fit 115.34777731755007\n",
      "438 :test prob  0.9995870612525809 loss: 212.83376065265247\n",
      "439 :train prob 0.9997854337881057 loss: 115.41426871082915 best fit 115.41426871082915\n",
      "439 :test prob  0.9995830128334885 loss: 213.06052805844232\n",
      "440 :train prob 0.9997854337881057 loss: 115.38699630521812 best fit 115.38699630521812\n",
      "440 :test prob  0.9995789644143962 loss: 213.52782388455202\n",
      "441 :train prob 0.9997854337881057 loss: 115.55322362747903 best fit 115.55322362747903\n",
      "441 :test prob  0.9995830128334885 loss: 213.06051863666872\n",
      "442 :train prob 0.9997854337881057 loss: 114.90844743964195 best fit 114.90844743964195\n",
      "442 :test prob  0.9995911096716732 loss: 212.5812903111067\n",
      "443 :train prob 0.9997813853690134 loss: 114.04770447967337 best fit 114.04770447967337\n",
      "443 :test prob  0.9995830128334885 loss: 211.60747410430946\n",
      "444 :train prob 0.9997813853690134 loss: 113.64940251732101 best fit 113.64940251732101\n",
      "444 :test prob  0.9995830128334885 loss: 211.2276740951012\n",
      "445 :train prob 0.9997854337881057 loss: 112.93986711788824 best fit 112.93986711788824\n",
      "445 :test prob  0.9995911096716732 loss: 211.04321462720043\n",
      "446 :train prob 0.9997854337881057 loss: 112.56983677487807 best fit 112.56983677487807\n",
      "446 :test prob  0.9995911096716732 loss: 210.65057548164535\n",
      "447 :train prob 0.9997935306262904 loss: 112.00233569631075 best fit 112.00233569631075\n",
      "447 :test prob  0.9995951580907656 loss: 211.11226434467542\n",
      "448 :train prob 0.9997935306262904 loss: 111.7215849748661 best fit 111.7215849748661\n",
      "448 :test prob  0.9995951580907656 loss: 210.73540364209126\n",
      "449 :train prob 0.9997935306262904 loss: 111.54300674803437 best fit 111.54300674803437\n",
      "449 :test prob  0.9995911096716732 loss: 210.71433585853148\n",
      "450 :train prob 0.9997935306262904 loss: 110.57789732976035 best fit 110.57789732976035\n",
      "450 :test prob  0.9995951580907656 loss: 209.15399268107979\n",
      "451 :train prob 0.9997935306262904 loss: 110.26919567679425 best fit 110.26919567679425\n",
      "451 :test prob  0.9995951580907656 loss: 208.7331193806147\n",
      "452 :train prob 0.9997935306262904 loss: 110.0504185025311 best fit 110.0504185025311\n",
      "452 :test prob  0.9995992065098579 loss: 208.39017017400244\n",
      "453 :train prob 0.9997935306262904 loss: 110.19085740341802 best fit 110.19085740341802\n",
      "453 :test prob  0.9995992065098579 loss: 208.90149985258535\n",
      "454 :train prob 0.9997894822071981 loss: 108.1772159066811 best fit 108.1772159066811\n",
      "454 :test prob  0.9996032549289502 loss: 207.9524619191638\n",
      "455 :train prob 0.9997854337881057 loss: 108.0603678585506 best fit 108.0603678585506\n",
      "455 :test prob  0.9995911096716732 loss: 208.30914421677755\n",
      "456 :train prob 0.9997854337881057 loss: 107.25510405579706 best fit 107.25510405579706\n",
      "456 :test prob  0.9995992065098579 loss: 207.14436957660783\n",
      "457 :train prob 0.9997854337881057 loss: 106.54626227892025 best fit 106.54626227892025\n",
      "457 :test prob  0.9995911096716732 loss: 206.5652899923161\n",
      "458 :train prob 0.9997854337881057 loss: 105.87884901175786 best fit 105.87884901175786\n",
      "458 :test prob  0.9995911096716732 loss: 205.64929937923802\n",
      "459 :train prob 0.9997854337881057 loss: 105.41618713316676 best fit 105.41618713316676\n",
      "459 :test prob  0.9995951580907656 loss: 204.69918997518693\n",
      "460 :train prob 0.9997854337881057 loss: 105.16201729967412 best fit 105.16201729967412\n",
      "460 :test prob  0.9995951580907656 loss: 204.5364700499254\n",
      "461 :train prob 0.9997854337881057 loss: 104.98991534114236 best fit 104.98991534114236\n",
      "461 :test prob  0.9995992065098579 loss: 204.6778293638634\n",
      "462 :train prob 0.9997854337881057 loss: 104.52806385785013 best fit 104.52806385785013\n",
      "462 :test prob  0.9995992065098579 loss: 204.32368887987212\n",
      "463 :train prob 0.9997854337881057 loss: 104.32949641254359 best fit 104.32949641254359\n",
      "463 :test prob  0.9995951580907656 loss: 204.03791204102652\n",
      "464 :train prob 0.9997854337881057 loss: 104.22539318156171 best fit 104.22539318156171\n",
      "464 :test prob  0.9995951580907656 loss: 204.1175002477126\n",
      "465 :train prob 0.9997854337881057 loss: 104.19128931632719 best fit 104.19128931632719\n",
      "465 :test prob  0.9995951580907656 loss: 203.89939056155725\n",
      "466 :train prob 0.9997854337881057 loss: 103.8490030812659 best fit 103.8490030812659\n",
      "466 :test prob  0.9995951580907656 loss: 203.97294400657333\n",
      "467 :train prob 0.9997854337881057 loss: 103.70299486869071 best fit 103.70299486869071\n",
      "467 :test prob  0.9995951580907656 loss: 203.47259787089143\n",
      "468 :train prob 0.9997854337881057 loss: 103.57613818210224 best fit 103.57613818210224\n",
      "468 :test prob  0.9995992065098579 loss: 203.47709698978778\n",
      "469 :train prob 0.9998016274644751 loss: 101.36276369320848 best fit 101.36276369320848\n",
      "469 :test prob  0.9996032549289502 loss: 202.85863184416485\n",
      "470 :train prob 0.9997975790453828 loss: 101.12198730983106 best fit 101.12198730983106\n",
      "470 :test prob  0.9995992065098579 loss: 202.85202697720118\n",
      "471 :train prob 0.9997975790453828 loss: 100.88230646785061 best fit 100.88230646785061\n",
      "471 :test prob  0.9995992065098579 loss: 202.4158089263692\n",
      "472 :train prob 0.9997975790453828 loss: 100.79306693328446 best fit 100.79306693328446\n",
      "472 :test prob  0.9995992065098579 loss: 202.2079689020692\n",
      "473 :train prob 0.9997975790453828 loss: 100.10319867952079 best fit 100.10319867952079\n",
      "473 :test prob  0.9996032549289502 loss: 201.5647126229183\n",
      "474 :train prob 0.9997975790453828 loss: 99.83295736632996 best fit 99.83295736632996\n",
      "474 :test prob  0.9996032549289502 loss: 200.80147329493838\n",
      "475 :train prob 0.9997975790453828 loss: 99.31409998048396 best fit 99.31409998048396\n",
      "475 :test prob  0.9996032549289502 loss: 200.13400567759456\n",
      "476 :train prob 0.9998137727217522 loss: 98.61850427088795 best fit 98.61850427088795\n",
      "476 :test prob  0.9996073033480426 loss: 200.14449320056\n",
      "477 :train prob 0.9998137727217522 loss: 98.43318476774118 best fit 98.43318476774118\n",
      "477 :test prob  0.9996073033480426 loss: 199.82246761022847\n",
      "478 :train prob 0.9998137727217522 loss: 98.39208657613815 best fit 98.39208657613815\n",
      "478 :test prob  0.9996073033480426 loss: 199.71454859433098\n",
      "479 :train prob 0.9998137727217522 loss: 98.073340121294 best fit 98.073340121294\n",
      "479 :test prob  0.9996073033480426 loss: 199.2480659446043\n",
      "480 :train prob 0.9998137727217522 loss: 97.86407151902287 best fit 97.86407151902287\n",
      "480 :test prob  0.9996113517671349 loss: 198.95654521327816\n",
      "481 :train prob 0.9998016274644751 loss: 97.86954505700247 best fit 97.86954505700247\n",
      "481 :test prob  0.9996113517671349 loss: 199.55867089062622\n",
      "482 :train prob 0.9998056758835675 loss: 97.54868261102632 best fit 97.54868261102632\n",
      "482 :test prob  0.9996073033480426 loss: 199.67312955626895\n",
      "483 :train prob 0.9998016274644751 loss: 97.7165899043284 best fit 97.7165899043284\n",
      "483 :test prob  0.9996073033480426 loss: 199.16206886447574\n",
      "484 :train prob 0.9998016274644751 loss: 97.43761659855333 best fit 97.43761659855333\n",
      "484 :test prob  0.9996073033480426 loss: 198.55667883554574\n",
      "485 :train prob 0.9998016274644751 loss: 97.14055714426276 best fit 97.14055714426276\n",
      "485 :test prob  0.9996073033480426 loss: 198.7618768227913\n",
      "486 :train prob 0.9998016274644751 loss: 97.00915157538279 best fit 97.00915157538279\n",
      "486 :test prob  0.9996073033480426 loss: 198.32150859643096\n",
      "487 :train prob 0.9998016274644751 loss: 96.91501471639242 best fit 96.91501471639242\n",
      "487 :test prob  0.9996032549289502 loss: 198.59489745495577\n",
      "488 :train prob 0.9998016274644751 loss: 97.06635008856257 best fit 97.06635008856257\n",
      "488 :test prob  0.9996032549289502 loss: 199.13174336560766\n",
      "489 :train prob 0.9998016274644751 loss: 96.83901139967004 best fit 96.83901139967004\n",
      "489 :test prob  0.9996032549289502 loss: 198.4030044057259\n",
      "490 :train prob 0.9998016274644751 loss: 96.87889280027919 best fit 96.87889280027919\n",
      "490 :test prob  0.9996032549289502 loss: 198.53830237512966\n",
      "491 :train prob 0.9998016274644751 loss: 96.6043278575333 best fit 96.6043278575333\n",
      "491 :test prob  0.9996032549289502 loss: 198.3114356154656\n",
      "492 :train prob 0.9998056758835675 loss: 96.24117461984574 best fit 96.24117461984574\n",
      "492 :test prob  0.9996032549289502 loss: 198.37834692942116\n",
      "493 :train prob 0.9998056758835675 loss: 95.60894220140932 best fit 95.60894220140932\n",
      "493 :test prob  0.9996032549289502 loss: 196.80742001930042\n",
      "494 :train prob 0.9998097243026598 loss: 95.47462872506497 best fit 95.47462872506497\n",
      "494 :test prob  0.9996032549289502 loss: 197.31135389487577\n",
      "495 :train prob 0.9998097243026598 loss: 95.20534632705298 best fit 95.20534632705298\n",
      "495 :test prob  0.9996073033480426 loss: 196.27003984384524\n",
      "496 :train prob 0.9998097243026598 loss: 94.9676992433598 best fit 94.9676992433598\n",
      "496 :test prob  0.9996073033480426 loss: 195.66091946486793\n",
      "497 :train prob 0.9998097243026598 loss: 94.72956594211149 best fit 94.72956594211149\n",
      "497 :test prob  0.9996073033480426 loss: 195.35079914438558\n",
      "498 :train prob 0.9998097243026598 loss: 94.54816652116081 best fit 94.54816652116081\n",
      "498 :test prob  0.9996073033480426 loss: 195.62830894904704\n",
      "499 :train prob 0.9998097243026598 loss: 94.40421591709949 best fit 94.40421591709949\n",
      "499 :test prob  0.9996073033480426 loss: 194.95263918114048\n",
      "500 :train prob 0.9998097243026598 loss: 94.22153725303532 best fit 94.22153725303532\n",
      "500 :test prob  0.9996073033480426 loss: 195.34088595175444\n",
      "501 :train prob 0.9998097243026598 loss: 93.59460983201797 best fit 93.59460983201797\n",
      "501 :test prob  0.9996073033480426 loss: 194.58839613989062\n",
      "502 :train prob 0.9998097243026598 loss: 93.599350387927 best fit 93.599350387927\n",
      "502 :test prob  0.9996073033480426 loss: 194.96645997247285\n",
      "503 :train prob 0.9998097243026598 loss: 93.51243893867142 best fit 93.51243893867142\n",
      "503 :test prob  0.9996073033480426 loss: 195.26625766332572\n",
      "504 :train prob 0.9998097243026598 loss: 93.08368057896895 best fit 93.08368057896895\n",
      "504 :test prob  0.9996032549289502 loss: 195.59696443549933\n",
      "505 :train prob 0.9998097243026598 loss: 92.87190239072207 best fit 92.87190239072207\n",
      "505 :test prob  0.9996073033480426 loss: 195.35192374774294\n",
      "506 :train prob 0.9998097243026598 loss: 92.72521188797077 best fit 92.72521188797077\n",
      "506 :test prob  0.9996073033480426 loss: 195.1046612875604\n",
      "507 :train prob 0.9998097243026598 loss: 92.607653686332 best fit 92.607653686332\n",
      "507 :test prob  0.9996154001862273 loss: 195.15667481337442\n",
      "508 :train prob 0.9998097243026598 loss: 92.12091773454037 best fit 92.12091773454037\n",
      "508 :test prob  0.9996154001862273 loss: 194.43651661178905\n",
      "509 :train prob 0.9998137727217522 loss: 91.62312447974449 best fit 91.62312447974449\n",
      "509 :test prob  0.9996154001862273 loss: 194.65344150899148\n",
      "510 :train prob 0.9998137727217522 loss: 91.5513352779694 best fit 91.5513352779694\n",
      "510 :test prob  0.9996154001862273 loss: 195.47610328159442\n",
      "511 :train prob 0.9998137727217522 loss: 91.29606761942964 best fit 91.29606761942964\n",
      "511 :test prob  0.9996154001862273 loss: 194.94285750388732\n",
      "512 :train prob 0.9998137727217522 loss: 91.01435081835696 best fit 91.01435081835696\n",
      "512 :test prob  0.9996154001862273 loss: 194.50840727052713\n",
      "513 :train prob 0.9998137727217522 loss: 90.76401976256571 best fit 90.76401976256571\n",
      "513 :test prob  0.9996154001862273 loss: 194.3761470660303\n",
      "514 :train prob 0.9998137727217522 loss: 90.74695069302486 best fit 90.74695069302486\n",
      "514 :test prob  0.9996113517671349 loss: 193.82551101600257\n",
      "515 :train prob 0.9998097243026598 loss: 90.17689187003363 best fit 90.17689187003363\n",
      "515 :test prob  0.9996113517671349 loss: 193.9970083703023\n",
      "516 :train prob 0.9998137727217522 loss: 89.99710991504493 best fit 89.99710991504493\n",
      "516 :test prob  0.9996154001862273 loss: 193.7603741726037\n",
      "517 :train prob 0.9998137727217522 loss: 89.75193536109548 best fit 89.75193536109548\n",
      "517 :test prob  0.9996194486053196 loss: 192.58944853570645\n",
      "518 :train prob 0.9998137727217522 loss: 89.88606654970822 best fit 89.88606654970822\n",
      "518 :test prob  0.9996194486053196 loss: 193.03786731230912\n",
      "519 :train prob 0.9998137727217522 loss: 89.2817087516888 best fit 89.2817087516888\n",
      "519 :test prob  0.9996194486053196 loss: 192.38734246705562\n",
      "520 :train prob 0.9998137727217522 loss: 89.27962366831478 best fit 89.27962366831478\n",
      "520 :test prob  0.9996194486053196 loss: 192.5873354948372\n",
      "521 :train prob 0.9998137727217522 loss: 89.38254690438778 best fit 89.38254690438778\n",
      "521 :test prob  0.9996194486053196 loss: 193.22086303018398\n",
      "522 :train prob 0.9998137727217522 loss: 89.1702051496775 best fit 89.1702051496775\n",
      "522 :test prob  0.9996194486053196 loss: 193.14984185137897\n",
      "523 :train prob 0.9998178211408445 loss: 89.10590132065936 best fit 89.10590132065936\n",
      "523 :test prob  0.9996154001862273 loss: 194.16154108208488\n",
      "524 :train prob 0.9998178211408445 loss: 89.09470328556193 best fit 89.09470328556193\n",
      "524 :test prob  0.9996194486053196 loss: 194.16381286026441\n",
      "525 :train prob 0.9998178211408445 loss: 89.10468012522757 best fit 89.10468012522757\n",
      "525 :test prob  0.9996154001862273 loss: 195.19698102932415\n",
      "526 :train prob 0.9998178211408445 loss: 88.87285614694306 best fit 88.87285614694306\n",
      "526 :test prob  0.9996154001862273 loss: 194.30038419664803\n",
      "527 :train prob 0.9998218695599368 loss: 88.54692898551399 best fit 88.54692898551399\n",
      "527 :test prob  0.9996113517671349 loss: 194.49565355467536\n",
      "528 :train prob 0.9998218695599368 loss: 87.89274788633685 best fit 87.89274788633685\n",
      "528 :test prob  0.9996154001862273 loss: 194.8786745795774\n",
      "529 :train prob 0.9998218695599368 loss: 87.54189399739744 best fit 87.54189399739744\n",
      "529 :test prob  0.9996154001862273 loss: 193.73329042438323\n",
      "530 :train prob 0.9998218695599368 loss: 87.53343275860514 best fit 87.53343275860514\n",
      "530 :test prob  0.9996154001862273 loss: 194.29673661346686\n",
      "531 :train prob 0.9998299663981215 loss: 87.58752228796452 best fit 87.58752228796452\n",
      "531 :test prob  0.9996194486053196 loss: 194.87328531426482\n",
      "532 :train prob 0.9998299663981215 loss: 87.19811501624912 best fit 87.19811501624912\n",
      "532 :test prob  0.9996194486053196 loss: 193.61742548456203\n",
      "533 :train prob 0.9998259179790292 loss: 87.20945372422175 best fit 87.20945372422175\n",
      "533 :test prob  0.9996154001862273 loss: 194.1307902258943\n",
      "534 :train prob 0.9998259179790292 loss: 87.05089887257806 best fit 87.05089887257806\n",
      "534 :test prob  0.9996154001862273 loss: 193.24049205839378\n",
      "535 :train prob 0.9998259179790292 loss: 87.05173793854983 best fit 87.05173793854983\n",
      "535 :test prob  0.9996194486053196 loss: 193.3161740337875\n",
      "536 :train prob 0.9998299663981215 loss: 86.97307519872872 best fit 86.97307519872872\n",
      "536 :test prob  0.9996154001862273 loss: 193.8035590839658\n",
      "537 :train prob 0.9998259179790292 loss: 86.6935402464813 best fit 86.6935402464813\n",
      "537 :test prob  0.9996194486053196 loss: 194.467229885483\n",
      "538 :train prob 0.9998259179790292 loss: 86.7486529127097 best fit 86.7486529127097\n",
      "538 :test prob  0.9996194486053196 loss: 195.19740580011745\n",
      "539 :train prob 0.9998259179790292 loss: 85.89341628399887 best fit 85.89341628399887\n",
      "539 :test prob  0.9996194486053196 loss: 193.77242019799408\n",
      "540 :train prob 0.9998259179790292 loss: 85.76914137667738 best fit 85.76914137667738\n",
      "540 :test prob  0.9996154001862273 loss: 192.8330855722016\n",
      "541 :train prob 0.9998259179790292 loss: 85.69291931611207 best fit 85.69291931611207\n",
      "541 :test prob  0.9996154001862273 loss: 193.45008760856248\n",
      "542 :train prob 0.9998259179790292 loss: 85.58526662322605 best fit 85.58526662322605\n",
      "542 :test prob  0.9996194486053196 loss: 192.59855800105146\n",
      "543 :train prob 0.9998299663981215 loss: 85.06065776962126 best fit 85.06065776962126\n",
      "543 :test prob  0.9996275454435043 loss: 192.77564170121886\n",
      "544 :train prob 0.9998299663981215 loss: 84.9613387287021 best fit 84.9613387287021\n",
      "544 :test prob  0.9996194486053196 loss: 193.00389112318035\n",
      "545 :train prob 0.9998299663981215 loss: 84.56211761476253 best fit 84.56211761476253\n",
      "545 :test prob  0.9996194486053196 loss: 193.6708559710211\n",
      "546 :train prob 0.9998299663981215 loss: 84.68751720995353 best fit 84.68751720995353\n",
      "546 :test prob  0.9996194486053196 loss: 194.00735154923757\n",
      "547 :train prob 0.9998299663981215 loss: 84.60831275521112 best fit 84.60831275521112\n",
      "547 :test prob  0.9996154001862273 loss: 193.586857012221\n",
      "548 :train prob 0.9998340148172139 loss: 84.1807968956217 best fit 84.1807968956217\n",
      "548 :test prob  0.9996154001862273 loss: 192.34390072425177\n",
      "549 :train prob 0.9998340148172139 loss: 84.00991128927618 best fit 84.00991128927618\n",
      "549 :test prob  0.9996154001862273 loss: 192.51708890174206\n",
      "550 :train prob 0.9998299663981215 loss: 84.06596789330598 best fit 84.06596789330598\n",
      "550 :test prob  0.9996154001862273 loss: 192.3770872232962\n",
      "551 :train prob 0.9998299663981215 loss: 83.86633335560737 best fit 83.86633335560737\n",
      "551 :test prob  0.9996154001862273 loss: 192.83433767022964\n",
      "552 :train prob 0.9998299663981215 loss: 83.89280456076328 best fit 83.89280456076328\n",
      "552 :test prob  0.9996154001862273 loss: 193.27335412701598\n",
      "553 :train prob 0.9998299663981215 loss: 83.7918854085866 best fit 83.7918854085866\n",
      "553 :test prob  0.9996194486053196 loss: 192.49111374252797\n",
      "554 :train prob 0.9998299663981215 loss: 83.68711517158621 best fit 83.68711517158621\n",
      "554 :test prob  0.9996113517671349 loss: 192.79925261151095\n",
      "555 :train prob 0.9998299663981215 loss: 83.6365169866786 best fit 83.6365169866786\n",
      "555 :test prob  0.9996154001862273 loss: 192.2434919302413\n",
      "556 :train prob 0.9998299663981215 loss: 83.22030049812189 best fit 83.22030049812189\n",
      "556 :test prob  0.9996154001862273 loss: 190.95442654181338\n",
      "557 :train prob 0.9998299663981215 loss: 82.85011745966985 best fit 82.85011745966985\n",
      "557 :test prob  0.9996154001862273 loss: 190.17388732291724\n",
      "558 :train prob 0.9998380632363062 loss: 81.14398021110908 best fit 81.14398021110908\n",
      "558 :test prob  0.9996194486053196 loss: 190.03774361580128\n",
      "559 :train prob 0.9998380632363062 loss: 80.70333615421528 best fit 80.70333615421528\n",
      "559 :test prob  0.9996194486053196 loss: 188.88209686914644\n",
      "560 :train prob 0.9998380632363062 loss: 80.7543606782464 best fit 80.7543606782464\n",
      "560 :test prob  0.9996194486053196 loss: 189.10129005230178\n",
      "561 :train prob 0.9998380632363062 loss: 80.87615543236217 best fit 80.87615543236217\n",
      "561 :test prob  0.9996154001862273 loss: 189.6778315532873\n",
      "562 :train prob 0.9998380632363062 loss: 80.97709957824117 best fit 80.97709957824117\n",
      "562 :test prob  0.9996194486053196 loss: 189.60461470369057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m     27\u001b[0m     eg \u001b[39m=\u001b[39m update_engine(X_train,y_train,y_train_one_hot,log_odds,p,residual,learning_rate,max_depth,eg)\n\u001b[1;32m---> 28\u001b[0m     node,grads \u001b[39m=\u001b[39m train_engine(eg)\n\u001b[0;32m     29\u001b[0m     band\u001b[39m.\u001b[39mappend(grads)\n\u001b[0;32m     31\u001b[0m     test_val \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36mtrain_engine\u001b[1;34m(eg)\u001b[0m\n\u001b[0;32m      5\u001b[0m learning_rate \u001b[39m=\u001b[39m eg\u001b[39m.\u001b[39mlearning_rate\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m         eg\u001b[39m.\u001b[39;49mevolve(\u001b[39m10\u001b[39;49m,\u001b[39m10\u001b[39;49m,\u001b[39m1\u001b[39;49m,[\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m],\u001b[39m0\u001b[39;49m)    \n\u001b[0;32m     10\u001b[0m node \u001b[39m=\u001b[39m eg\u001b[39m.\u001b[39mbest[\u001b[39m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m val \u001b[39m=\u001b[39m eg\u001b[39m.\u001b[39mvals[node\u001b[39m.\u001b[39mindex]\n",
      "Cell \u001b[1;32mIn[4], line 89\u001b[0m, in \u001b[0;36mEngine.evolve\u001b[1;34m(self, total_size, batch_size, elite_size, beta, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m vals \u001b[39m=\u001b[39m [funcs[i]([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvals[s\u001b[39m.\u001b[39mindex] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sons[i]]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size)]\n\u001b[0;32m     88\u001b[0m vals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(vals)\n\u001b[1;32m---> 89\u001b[0m fitness,clfs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_fitness(vals,beta)\n\u001b[0;32m     91\u001b[0m elites\u001b[39m.\u001b[39mextend([(funcs[i],sons[i],vals[i],clfs[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size)])\n\u001b[0;32m     92\u001b[0m elites_fitness\u001b[39m.\u001b[39mextend(fitness)\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mEngine.calculate_fitness\u001b[1;34m(self, vals, beta)\u001b[0m\n\u001b[0;32m     57\u001b[0m clfs \u001b[39m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m vals:\n\u001b[1;32m---> 59\u001b[0m     loss,clf,clf_param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_decision(np\u001b[39m.\u001b[39;49marray([val])\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m     61\u001b[0m     fitness\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     62\u001b[0m     clfs\u001b[39m.\u001b[39mappend((clf,clf_param))\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m, in \u001b[0;36mEngine.calculate_decision\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m     48\u001b[0m     grads[index\u001b[39m==\u001b[39mi] \u001b[39m=\u001b[39m grad_bin[i]\n\u001b[0;32m     49\u001b[0m log_odds,p \u001b[39m=\u001b[39m update_log_p(grads,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_odds,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate)\n\u001b[1;32m---> 51\u001b[0m loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39;49mpower(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mY_one_hot \u001b[39m-\u001b[39;49m p,\u001b[39m2\u001b[39;49m))\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m loss,clf,(bins,grad_bin)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stack = []\n",
    "learning_rate = 0.5\n",
    "max_depth = 1\n",
    "\n",
    "train_acc = []\n",
    "train_grads_acc = []\n",
    "test_acc = []\n",
    "test_grads_acc = []\n",
    "\n",
    "y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "\n",
    "init_log_odds,init_p = initial(y_train_one_hot)\n",
    "log_odds,p = initial_first_bin(init_log_odds,init_p,X_train)\n",
    "residual = y_train_one_hot - p\n",
    "\n",
    "test_log_odds,test_p = initial_first_bin(init_log_odds,init_p,X_test)\n",
    "test_residual = y_test_one_hot - test_p\n",
    "\n",
    "\n",
    "for _ in range(150000):\n",
    "    band = []\n",
    "    test_band = []\n",
    "\n",
    "    if _ % 40 == 0:\n",
    "        eg = None\n",
    "    for g in range(1):\n",
    "        eg = update_engine(X_train,y_train,y_train_one_hot,log_odds,p,residual,learning_rate,max_depth,eg)\n",
    "        node,grads = train_engine(eg)\n",
    "        band.append(grads)\n",
    "\n",
    "        test_val = node.predict(X_test)\n",
    "        test_grads = gradient(node,test_val)\n",
    "        test_band.append(test_grads)\n",
    "        \n",
    "    stack.append(node)\n",
    "    grads = np.array(band)\n",
    "    test_grads = np.array(test_band)\n",
    "\n",
    "    grads = np.average(grads,axis=0)\n",
    "    test_grads = np.average(test_grads,axis=0)\n",
    "    # train_grads_acc.append(np.sum(abs(grads))/grads.shape[0])\n",
    "    # test_grads_acc.append(np.sum(abs(test_grads))/test_grads.shape[0])\n",
    "\n",
    "    # print(\"train grads\",train_grads_acc[-1])\n",
    "    # print(\"test  grads\",test_grads_acc[-1])\n",
    "\n",
    "    mask = np.ones(p.shape[0])\n",
    "    log_odds,p = update_log_p(grads,log_odds,p,mask,learning_rate)\n",
    "    residual = y_train_one_hot - p\n",
    "    train_loss = np.sum(np.power(residual,2))\n",
    "\n",
    "    train_acc.append(accuracy_score(y_train,p.argmax(axis=1)))\n",
    "    print(_,\":train prob\",train_acc[-1],\"loss:\",train_loss,\"best fit\",eg.best[0])\n",
    "\n",
    "    mask = np.ones(test_p.shape[0])\n",
    "    test_log_odds,test_p = update_log_p(test_grads,test_log_odds,test_p,mask,learning_rate)\n",
    "    test_residual = y_test_one_hot - test_p\n",
    "    test_loss = np.sum(np.power(test_residual,2))\n",
    "\n",
    "    test_acc.append(accuracy_score(y_test,test_p.argmax(axis=1)))\n",
    "    print(_,\":test prob \",test_acc[-1],\"loss:\",test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5e69f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhouz\\AppData\\Local\\Temp\\ipykernel_21236\\1286640705.py:1: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  np.sum(n.numNode for n in stack),np.average([n.depth for n in stack])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(454377, 3.104479067137258)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(n.numNode for n in stack),np.average([n.depth for n in stack]) # 15000 generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b41d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.857     0.812     0.834    190787\n",
      "           1      0.845     0.900     0.872    254806\n",
      "           2      0.844     0.882     0.863     32169\n",
      "           3      0.848     0.704     0.769      2457\n",
      "           4      0.868     0.490     0.626      8568\n",
      "           5      0.784     0.680     0.728     15655\n",
      "           6      0.912     0.829     0.869     18469\n",
      "\n",
      "    accuracy                          0.850    522911\n",
      "   macro avg      0.851     0.757     0.794    522911\n",
      "weighted avg      0.850     0.850     0.848    522911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,test_p.argmax(axis=1),digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f187638b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x288d2e4c210>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "# plt.plot(train_grads_acc)\n",
    "plt.plot(test_acc)\n",
    "# plt.plot(test_grads_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in range(10000):\n",
    "    if g % 100 == 0:\n",
    "        eg = update_engine(X_train,y_train,log_odds,p,residual,learning_rate,max_depth)\n",
    "    else:\n",
    "        eg = update_engine(X_train,y_train,log_odds,p,residual,learning_rate,max_depth,eg)\n",
    "\n",
    "    start = time()\n",
    "    node,val,log_odds,p,residual = train_engine(eg)\n",
    "    training_time = time() - start\n",
    "    \n",
    "    residual = y_train_one_hot - p\n",
    "    loss = np.sum(np.power(residual,2))\n",
    "\n",
    "    if not np.isnan(loss):\n",
    "        stack.append(node)\n",
    "\n",
    "        if g % 1000 == 0:\n",
    "            #residual plot\n",
    "            plt.hist(np.max(p,axis=1),bins=100)\n",
    "            plt.show()\n",
    "            \n",
    "        # train_acc.append(loss)\n",
    "        # test_acc.append(test_loss)\n",
    "        \n",
    "        pred = np.argmax(p,axis=1)\n",
    "        train_acc.append(accuracy_score(y_train,pred))\n",
    "        # pred = p[:,1]\n",
    "        # train_acc.append(roc_auc_score(y_train,pred))\n",
    "\n",
    "\n",
    "        # test data prediction\n",
    "        test_log_odds,test_p = predict_single_node(node,X_test,test_log_odds,test_p,learning_rate)\n",
    "        test_residual = y_test_one_hot - test_p\n",
    "        test_loss = np.sum(np.power(test_residual,2))\n",
    "\n",
    "        \n",
    "        # test_pred = predict(X_test,init_log_odds,init_p,learning_rate,stack)\n",
    "        test_pred = np.argmax(test_p,axis=1)\n",
    "        test_acc.append(accuracy_score(y_test,test_pred))\n",
    "        # test_pred = test_p[:,1]\n",
    "        # test_acc.append(roc_auc_score(y_test,test_pred))\n",
    "    \n",
    "        # print(\"Generation:\",g,loss,train_acc[-1],str(round(training_time,3))+'s')\n",
    "        print(\"Generation:\",g,loss,test_loss,train_acc[-1],test_acc[-1],str(round(training_time,3))+'s')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecdd30c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_engine(eg):\n",
    "    log_odds = eg.log_odds\n",
    "    p = eg.p\n",
    "    residual = eg.residual\n",
    "    learning_rate = eg.learning_rate\n",
    "\n",
    "    #masking\n",
    "    mask = np.sum(np.power(p,2),axis=1) < 1\n",
    "#     mask = np.array(residual).T[0] > 0.1\n",
    "                \n",
    "    for j in range(1):\n",
    "            eg.evolve(10,10,1,[0,0,0],0)\n",
    "    \n",
    "    node = eg.best[1]\n",
    "    val = eg.vals[node.index]\n",
    "    grads = gradient(node,val)\n",
    "    log_odds,p = update_log_p(grads,log_odds,p,mask,learning_rate)\n",
    "\n",
    "\n",
    "    # masked_grads = np.zeros(log_odds.shape)\n",
    "    # masked_grads[mask] = grads_slim\n",
    "    # log_odds,p = update_log_p(masked_grads,log_odds,p,mask,learning_rate)\n",
    "\n",
    "    return node,val,log_odds,p,residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b04aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_engine(X,y,log_odds,p,residual,learning_rate,max_depth,prev_eg=None):\n",
    "    # eg = Engine(Functions.simple_opset,\n",
    "    #             X[mask],\n",
    "    #             y[mask],\n",
    "    #             residual[mask],\n",
    "    #             log_odds[mask],\n",
    "    #             p[mask],\n",
    "    #             learning_rate,max_depth)\n",
    "    \n",
    "    if prev_eg == None:\n",
    "        eg = Engine(Functions.simple_opset,\n",
    "            X,\n",
    "            y,\n",
    "            residual,\n",
    "            log_odds,\n",
    "            p,\n",
    "            learning_rate,max_depth)\n",
    "    else:\n",
    "        eg = prev_eg\n",
    "        \n",
    "        eg.residual = residual\n",
    "        eg.log_odds = log_odds\n",
    "        eg.p = p\n",
    "        eg.learning_rate = learning_rate\n",
    "        eg.max_depth = max_depth\n",
    "\n",
    "        eg.best = (np.inf,None)\n",
    "\n",
    "    return eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c23e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 0 36083.087064121704 325460.0719413065 0.488287637045834 0.48752273331408214 0.156s\n",
      "Generation: 1 35909.951264044146 323888.95539718494 0.488287637045834 0.48752273331408214 0.182s\n",
      "Generation: 2 35393.8574411639 319176.1181803428 0.488287637045834 0.48752273331408214 0.178s\n",
      "Generation: 3 35286.487478461706 318211.6277188745 0.488287637045834 0.48752273331408214 0.173s\n",
      "Generation: 4 35166.51394260169 317112.68886903964 0.488287637045834 0.48752273331408214 0.205s\n",
      "Generation: 5 33426.07053740031 301509.50925283064 0.488287637045834 0.48752273331408214 0.204s\n",
      "Generation: 6 32062.998499757225 289277.2423806249 0.503003390647321 0.5022652038300973 0.2s\n",
      "Generation: 7 31961.657857050042 288349.8567434523 0.5218843049172992 0.5211842933118638 0.167s\n",
      "Generation: 8 31833.342925373036 287186.14736026106 0.5261699454398375 0.5254240205312185 0.169s\n",
      "Generation: 9 31751.757682327046 286447.2613881189 0.5480112218378341 0.5472785999912031 0.187s\n",
      "Generation: 10 31684.514001405052 285835.39129360585 0.5717285416774238 0.5703838702953276 0.194s\n",
      "Generation: 11 31593.429745797956 285001.9487587918 0.5771501351095506 0.5761401079724848 0.219s\n",
      "Generation: 12 31443.144246164735 283615.4504745437 0.5858418960086745 0.5854916037337138 0.153s\n",
      "Generation: 13 31394.959716836565 283180.7009304407 0.5889571608061823 0.5887292483806996 0.16s\n",
      "Generation: 14 31331.90772985053 282607.0926501484 0.5980792068983322 0.5975146822308194 0.275s\n",
      "Generation: 15 30344.144056754856 273746.2640261975 0.6472005645341733 0.6462992746375579 0.196s\n",
      "Generation: 16 30262.833262013504 272996.88633272867 0.6473210443882205 0.6463681200051251 0.208s\n",
      "Generation: 17 29482.874973773218 266010.35866618075 0.6587494191149894 0.6572896726211535 0.214s\n",
      "Generation: 18 29447.665477186365 265693.46080823755 0.658112597029311 0.6563334869604961 0.192s\n",
      "Generation: 19 29411.81378539634 265369.7976502611 0.6587838419304315 0.6567905437062903 0.213s\n",
      "Generation: 20 29320.174382796056 264556.82922476355 0.6585945164455 0.6568001055628969 0.152s\n",
      "Generation: 21 28704.850560753464 259063.916037275 0.6661159016195934 0.6634876680735345 0.279s\n",
      "Generation: 22 28210.756997605888 254644.94472821234 0.6690590523398908 0.667337271543341 0.291s\n",
      "Generation: 23 28196.322052607917 254503.72098085287 0.6682845389924442 0.6656486476666201 0.222s\n",
      "Generation: 24 28004.724694147742 252814.18285432865 0.6689213610781226 0.6665684982721725 0.24s\n",
      "Generation: 25 27620.719102059073 249379.91098564607 0.671847300390699 0.6698960243712602 0.249s\n",
      "Generation: 26 27320.397455248763 246700.20089407905 0.6726734479613088 0.6706628852711073 0.269s\n",
      "Generation: 27 27073.409824260223 244505.87398116692 0.6736028639782448 0.6710625708772621 0.216s\n",
      "Generation: 28 27061.421687966154 244396.89305435415 0.6725013338840984 0.6699878181946832 0.226s\n",
      "Generation: 29 26850.94814675729 242517.93438808405 0.6734307499010344 0.6709535657119472 0.278s\n",
      "Generation: 30 26679.996721596362 241002.1427373338 0.675547753050722 0.673060998908036 0.204s\n",
      "Generation: 31 26545.61244953983 239814.7757761261 0.6759091926128638 0.6735142309111876 0.211s\n",
      "Generation: 32 26486.20261374858 239294.5479003212 0.6760813066900742 0.6738794938335587 0.246s\n",
      "Generation: 33 26402.052880440908 238558.5204603852 0.67783687027762 0.6754610249162859 0.216s\n",
      "Generation: 34 26216.86564474804 236905.49889534366 0.6785253265864615 0.67620876210292 0.35s\n",
      "Generation: 35 26136.261547030634 236197.14625566412 0.6794031083802344 0.6769641487748393 0.375s\n",
      "Generation: 36 26072.490501825458 235633.86126570206 0.6803497358048914 0.6778629632958573 0.294s\n",
      "Generation: 37 26012.017338541777 235099.05192382506 0.6807628095901964 0.6786107004824913 0.312s\n",
      "Generation: 38 25968.564208039017 234721.71608008211 0.6813824202681538 0.6792054479634202 0.309s\n",
      "Generation: 39 25933.570163177013 234418.1178194543 0.6808832894442437 0.679186324250207 0.358s\n",
      "Generation: 40 25902.37486803803 234140.01465883123 0.6810209807060119 0.6793335768419483 0.326s\n",
      "Generation: 41 25875.008060237662 233904.8277016703 0.6809693464828488 0.6794062469521582 0.376s\n",
      "Generation: 42 25798.080941597418 233220.10611161147 0.6811586719677802 0.6797772469884933 0.282s\n",
      "Generation: 43 25777.13021150266 233038.35852515884 0.6811930947832223 0.679805932558313 0.307s\n",
      "Generation: 44 25754.223740464477 232844.19545384712 0.6807628095901964 0.6795707108857912 0.37s\n",
      "Generation: 45 25743.358988616008 232753.32397738882 0.6807972324056385 0.6794770046910469 0.238s\n",
      "Generation: 46 25734.64705795745 232681.17832709057 0.6807800209979175 0.6795649737718273 0.344s\n",
      "Generation: 47 25713.202194351743 232475.10009032246 0.6824667389545791 0.6806990099653669 0.322s\n",
      "Generation: 48 25692.913726782328 232292.39034937168 0.6822257792464845 0.6804676130354879 0.315s\n",
      "Generation: 49 25647.095541128754 231885.28309952695 0.6820880879847163 0.6802782882746777 0.319s\n",
      "Generation: 50 25642.20003377647 231844.600614157 0.6824495275468581 0.6804790872634158 0.302s\n",
      "Generation: 51 25605.43777068642 231518.7082001539 0.6820020309461111 0.6799876078338378 0.353s\n",
      "Generation: 52 25602.43043585096 231495.56586156075 0.6819159739075059 0.6799264119515558 0.357s\n",
      "Generation: 53 25600.11524093994 231477.82816033647 0.6818643396843428 0.6799780459772313 0.352s\n",
      "Generation: 54 25598.262629566514 231463.99182824892 0.6816922256071324 0.6799550975213755 0.344s\n",
      "Generation: 55 25533.6355625333 230899.87103609383 0.6820536651692742 0.6808596491563574 0.348s\n",
      "Generation: 56 25532.23037186623 230889.12367049712 0.6820364537615532 0.6807544687336851 0.335s\n",
      "Generation: 57 25531.170072147863 230880.83336598641 0.6820364537615532 0.6807410821344358 0.365s\n",
      "Generation: 58 25530.23989088479 230874.5915459677 0.6821397222078793 0.680863473899 0.367s\n",
      "Generation: 59 25508.87524179721 230701.037448779 0.6823290476928108 0.6810222007186691 0.237s\n",
      "Generation: 60 25484.739087343678 230474.28014551708 0.6826044302163474 0.6810948708288791 0.405s\n",
      "Generation: 61 25471.41226201662 230364.21034635403 0.6822429906542056 0.6809131955533542 0.322s\n",
      "Generation: 62 25462.826159251454 230295.5813228118 0.6822429906542056 0.6809323192665674 0.442s\n",
      "Generation: 63 25455.827060941232 230240.19704277097 0.6823806819159739 0.681100607942843 0.38s\n",
      "Generation: 64 25413.07405267454 229847.82954963853 0.6830863496325364 0.6815787007731717 0.385s\n",
      "Generation: 65 25400.794119153088 229751.32431108874 0.682914235555326 0.6813759894131124 0.317s\n",
      "Generation: 66 25395.05964533636 229705.38190312884 0.6826560644395105 0.6811904893949449 0.312s\n",
      "Generation: 67 25390.555153064077 229670.44312260288 0.6825183731777422 0.6811101697994496 0.302s\n",
      "Generation: 68 25385.7244725804 229632.5521049109 0.682415104731416 0.6810202883473478 0.39s\n",
      "Generation: 69 25382.37030874535 229607.1054568509 0.6826560644395105 0.6811445924832333 0.375s\n",
      "Generation: 70 25364.033453488224 229444.1334379576 0.682914235555326 0.6813473038432927 0.339s\n",
      "Generation: 71 25363.251535285322 229437.86995018873 0.6828109671089998 0.6813606904425419 0.407s\n",
      "Generation: 72 25340.439041880356 229247.75028432053 0.6828281785167208 0.6813052316742237 0.329s\n",
      "Generation: 73 25312.436098119688 228988.28167001012 0.6828626013321629 0.6813530409572566 0.364s\n",
      "Generation: 74 25289.88558088749 228810.94948386296 0.6828281785167208 0.6814964688063552 0.35s\n",
      "Generation: 75 25286.143674929328 228781.54363116747 0.6829314469630471 0.6815614894312799 0.383s\n",
      "Generation: 76 25262.688478685548 228563.37167288852 0.6830519268170944 0.6815041182916405 0.306s\n",
      "Generation: 77 25254.35381537723 228493.52093427815 0.6829486583707681 0.6813645151851845 0.419s\n",
      "Generation: 78 25251.672604355073 228473.20265445497 0.6830002925939312 0.6814180615821813 0.329s\n",
      "Generation: 79 25231.49601045464 228284.59155321072 0.6830175040016523 0.6812593347625122 0.362s\n",
      "Generation: 80 25225.505011137975 228238.79711418028 0.6829486583707681 0.6812478605345843 0.435s\n",
      "Generation: 81 25205.138369066357 228057.0829455713 0.682897024147605 0.6812899327036532 0.384s\n",
      "Generation: 82 25188.900131231832 227905.57422599022 0.6832068294865837 0.68141614921086 0.447s\n",
      "Generation: 83 25153.71645922957 227561.43888531573 0.6845665306965457 0.6827911441908853 0.521s\n",
      "Generation: 84 25141.101397235718 227443.83849609343 0.6849107588509664 0.6828064431614558 0.353s\n",
      "Generation: 85 25138.684440188263 227426.51188403644 0.6851000843358979 0.6830148916354791 0.392s\n",
      "Generation: 86 25034.130207621914 226496.70047416145 0.6863220942840915 0.6843325154758649 0.436s\n",
      "Generation: 87 25021.870392500838 226381.1888241319 0.6867523794771174 0.684424309299288 0.407s\n",
      "Generation: 88 25013.691312196086 226318.70830785006 0.6865458425844649 0.6847149897401279 0.398s\n",
      "Generation: 89 25001.96890118621 226206.32548968255 0.6866663224385122 0.6846901289129508 0.407s\n",
      "Generation: 90 24996.81239711501 226168.55837312594 0.686976127777491 0.6847761856224099 0.438s\n",
      "Generation: 91 24992.69417324067 226139.26520899177 0.6869417049620489 0.6846614433431311 0.427s\n",
      "Generation: 92 24982.15055046439 226037.72826154745 0.6870793962238172 0.6847016031408787 0.482s\n",
      "Generation: 93 24978.609205078745 226013.6436541351 0.6868900707388857 0.6847838351076951 0.386s\n",
      "Generation: 94 24976.674918799483 225999.71710759168 0.6869589163697699 0.6847169021114492 0.36s\n",
      "Generation: 95 24946.5281516691 225709.31230730258 0.6875957384554483 0.6852581031953813 0.496s\n",
      "Generation: 96 24933.704975730114 225592.1555986148 0.6880260236484742 0.6855487836362211 0.42s\n",
      "Generation: 97 24929.09477882788 225558.98044143032 0.6878366981635428 0.6855564331215064 0.422s\n",
      "Generation: 98 24895.671394147972 225251.83361512597 0.688404674618337 0.6861282321465795 0.402s\n",
      "Generation: 99 24893.059650472558 225234.10602512222 0.6884907316569422 0.6860555620363695 0.438s\n",
      "Generation: 100 24858.09897084253 224929.58686462982 0.6888865940345261 0.6865183558961276 0.248s\n",
      "Generation: 101 24838.223871874172 224759.40623104753 0.6890242852962943 0.6866273610614426 0.221s\n",
      "Generation: 102 24767.91721729644 224157.13271133875 0.6904872549525826 0.6879277735599366 0.217s\n",
      "Generation: 103 24727.005363265176 223798.95670863768 0.6909347515533295 0.6884364643314063 0.298s\n",
      "Generation: 104 24660.28292176754 223203.41768777042 0.6917092649007762 0.6891937633746469 0.292s\n",
      "Generation: 105 24600.99157146637 222661.5430370359 0.6920879158706391 0.6898669180797498 0.222s\n",
      "Generation: 106 24574.692226184976 222447.82557692713 0.6930861775184592 0.6905802325826002 0.242s\n",
      "Generation: 107 24572.023471966295 222429.07111013247 0.6932066573725065 0.690585969696564 0.305s\n",
      "Generation: 108 24515.266494207674 221947.19900720145 0.6939467479045112 0.6913528305964113 0.248s\n",
      "Generation: 109 24413.535889884144 221063.01961473384 0.6949105867368892 0.6923224028563179 0.298s\n",
      "Generation: 110 24411.212545676954 221047.20760139165 0.6947212612519578 0.6922363461468587 0.226s\n",
      "Generation: 111 24328.6128742631 220330.90320935595 0.6955646202302886 0.6928138822858957 0.293s\n",
      "Generation: 112 24261.79570433405 219753.18481509737 0.6964768248395036 0.6936591504099168 0.254s\n",
      "Generation: 113 24259.56416930483 219738.08948170606 0.6964768248395036 0.6935903050423494 0.25s\n",
      "Generation: 114 24218.811396324007 219374.3420985703 0.6972341267792292 0.6944355731663706 0.223s\n",
      "Generation: 115 24207.6810757246 219269.11756420555 0.6975955663413711 0.6948926299121648 0.232s\n",
      "Generation: 116 24138.60146303012 218644.51094472423 0.6996265124524534 0.697233372409454 0.294s\n",
      "Generation: 117 24093.430004305326 218265.8800453774 0.7001084318686426 0.6979313879417338 0.294s\n",
      "Generation: 118 24091.28008153986 218250.9086102712 0.7001256432763636 0.6979600735115535 0.33s\n",
      "Generation: 119 24070.07724654202 218068.9744251416 0.7007968881774841 0.6982564910663573 0.284s\n",
      "Generation: 120 24037.852792263984 217782.28537501508 0.7014509216708834 0.6990979344477358 0.248s\n",
      "Generation: 121 24018.463931898852 217612.71490944858 0.7017951498253042 0.6995148313957824 0.262s\n",
      "Generation: 122 23998.71555816983 217420.22245221073 0.7021393779797249 0.6998131613219075 0.246s\n",
      "Generation: 123 23973.620997183636 217205.57328562732 0.7027245658422402 0.700314202608092 0.276s\n",
      "Generation: 124 23931.0031234583 216842.513595905 0.7034990791896869 0.7008362799788109 0.248s\n",
      "Generation: 125 23928.589788937083 216825.20255934462 0.7036023476360131 0.7006966768723549 0.324s\n",
      "Generation: 126 23913.511683393455 216698.07961144304 0.7035851362282921 0.7011097490777589 0.256s\n",
      "Generation: 127 23911.62005843894 216685.38478842378 0.7034818677819659 0.7009548470007324 0.315s\n",
      "Generation: 128 23910.106783071034 216675.75422160391 0.7036367704514552 0.7008898263758078 0.293s\n",
      "Generation: 129 23889.066376998137 216480.20818449007 0.7043080153525757 0.701438676945025 0.241s\n",
      "Generation: 130 23871.53348689879 216321.88818407577 0.7044801294297861 0.7018708728636421 0.243s\n",
      "Generation: 131 23870.35712568334 216314.87607328873 0.7045833978761122 0.7017714295549338 0.366s\n",
      "Generation: 132 23869.43933259614 216309.96501597043 0.7045145522452282 0.7016643367609402 0.296s\n",
      "Generation: 133 23868.789645661305 216307.09500055943 0.7044973408375071 0.701578280051481 0.301s\n",
      "Generation: 134 23868.38024438634 216305.83607289023 0.7044629180220651 0.7014405893163463 0.388s\n",
      "Generation: 135 23851.92783946375 216190.0881204776 0.7045833978761122 0.7016146151065861 0.362s\n",
      "Generation: 136 23851.819369207406 216191.57505357283 0.7046522435069964 0.7015668058235531 0.327s\n",
      "Generation: 137 23839.879573956136 216104.5382283033 0.7047038777301595 0.7016031408786582 0.359s\n",
      "Generation: 138 23839.784671452755 216106.12673162928 0.7046006092838333 0.701541944996376 0.301s\n",
      "Generation: 139 23839.760761755268 216108.02636962553 0.7045489750606702 0.7015362078824121 0.335s\n",
      "Generation: 140 23826.755458271393 216019.1320949582 0.7046006092838333 0.701589754279409 0.382s\n",
      "Generation: 141 23814.55233554866 215916.90334612847 0.7047383005456016 0.7018861718342128 0.257s\n",
      "Generation: 142 23804.011109311792 215847.27330005972 0.704428495206623 0.7016050532499795 0.266s\n",
      "Generation: 143 23804.157215889714 215850.4774227631 0.7043768609834599 0.7015687181948744 0.188s\n",
      "Generation: 144 23804.30724583511 215853.56491783785 0.7043596495757388 0.701511347055235 0.275s\n",
      "Generation: 145 23804.45321266789 215856.49184905604 0.7043080153525757 0.701480749114094 0.302s\n",
      "Generation: 146 23804.571727499155 215859.2010688322 0.7042563811294126 0.7014616254008809 0.327s\n",
      "Generation: 147 23804.771655404504 215862.22402864954 0.7042391697216915 0.7014329398310611 0.279s\n",
      "Generation: 148 23796.474076084094 215808.24482898673 0.7041703240908074 0.7012895119819625 0.405s\n",
      "Generation: 149 23789.41439181113 215765.11502802963 0.7043768609834599 0.7012111047577886 0.299s\n",
      "Generation: 150 23789.4774663907 215766.99861710254 0.7043940723911809 0.7012340532136444 0.273s\n",
      "Generation: 151 23789.544049665175 215768.8925518073 0.7043768609834599 0.7012455274415723 0.363s\n",
      "Generation: 152 23789.677280007327 215771.13941251297 0.7043596495757388 0.7012053676438247 0.235s\n",
      "Generation: 153 23764.59022931186 215543.1370621891 0.7048415689919278 0.7015209089118416 0.416s\n",
      "Generation: 154 23724.368561623985 215177.998302175 0.7052374313695117 0.7018402749225011 0.346s\n",
      "Generation: 155 23724.331228591924 215178.50036231393 0.7052202199617906 0.7018613110070356 0.393s\n",
      "Generation: 156 23724.285202676117 215179.13606465844 0.7052202199617906 0.7018766099776061 0.337s\n",
      "Generation: 157 23724.25070760705 215179.7722261699 0.7052374313695117 0.70188234709157 0.303s\n",
      "Generation: 158 23724.267027911585 215180.50730009074 0.7052202199617906 0.7018804347202487 0.276s\n",
      "Generation: 159 23724.254378164413 215181.0853140902 0.7052202199617906 0.7018804347202487 0.326s\n",
      "Generation: 160 23724.274982203307 215181.74977918778 0.7052202199617906 0.7018670481209995 0.266s\n",
      "Generation: 161 23724.28063977302 215182.2426250927 0.7052374313695117 0.7018842594628913 0.435s\n",
      "Generation: 162 23724.336240630906 215183.05483308982 0.7052374313695117 0.7018766099776061 0.37s\n",
      "Generation: 163 23691.78691870317 214887.46191023683 0.7052374313695117 0.7021003574222 0.388s\n",
      "Generation: 164 23691.56829850883 214885.84223459868 0.7052374313695117 0.7021290429920197 0.358s\n",
      "Generation: 165 23691.399749201602 214884.68880281743 0.7052374313695117 0.7021462543339115 0.329s\n",
      "Generation: 166 23665.155656691342 214645.1497103484 0.7052030085540696 0.7021768522750526 0.431s\n",
      "Generation: 167 23664.767715844835 214642.1604950997 0.7052374313695117 0.7022284863007281 0.323s\n",
      "Generation: 168 23664.454200772 214639.4264211504 0.7052546427772327 0.7022877698116888 0.353s\n",
      "Generation: 169 23664.183244022093 214637.11665301488 0.7052546427772327 0.7023164553815086 0.369s\n",
      "Generation: 170 23642.091900403222 214452.13859004917 0.70537512263128 0.7022399605286559 0.473s\n",
      "Generation: 171 23641.644216927547 214448.46947700475 0.70537512263128 0.7023164553815086 0.33s\n",
      "Generation: 172 23641.29016279571 214445.536277476 0.70537512263128 0.7023547028079348 0.376s\n",
      "Generation: 173 23600.72233026812 214089.04413489043 0.705857042047469 0.7028251461529782 0.345s\n",
      "Generation: 174 23600.502341796182 214087.47832218438 0.705857042047469 0.7028767801786537 0.36s\n",
      "Generation: 175 23600.325808918715 214086.61551509722 0.7058226192320269 0.7028538317227979 0.262s\n",
      "Generation: 176 23599.817958353316 214082.7678963355 0.7058226192320269 0.7028825172926176 0.435s\n",
      "Generation: 177 23599.68880522848 214081.79792810083 0.705839830639748 0.7029016410058309 0.347s\n",
      "Generation: 178 23599.594448727952 214081.22262204904 0.705839830639748 0.7028978162631881 0.268s\n",
      "Generation: 179 23599.52627721102 214080.912675038 0.705857042047469 0.7028939915205455 0.379s\n",
      "Generation: 180 23580.85596691009 213925.43149569485 0.7060807903478425 0.7031961461893133 0.338s\n",
      "Generation: 181 23580.80116506163 213925.07732823672 0.7060635789401215 0.7031770224761001 0.325s\n",
      "Generation: 182 23580.749973302394 213924.56212612404 0.7060980017555636 0.7031904090753494 0.39s\n",
      "Generation: 183 23549.79326515455 213649.26013711907 0.7059947333092373 0.7031100894798541 0.317s\n",
      "Generation: 184 23524.85225762523 213427.73523909564 0.7059603104937953 0.7030450688549295 0.37s\n",
      "Generation: 185 23524.319631710998 213423.3936990352 0.7059603104937953 0.7030374193696441 0.471s\n",
      "Generation: 186 23521.13175939158 213398.21159793454 0.7060463675324005 0.7031120018511754 0.478s\n",
      "Generation: 187 23506.755079553353 213271.92715942365 0.7059086762706321 0.7029723987447195 0.33s\n",
      "Generation: 188 23505.948618299437 213264.82104886815 0.7059775219015163 0.7030068214285031 0.315s\n",
      "Generation: 189 23485.193080373712 213076.28619652888 0.7066659782103578 0.703626429736609 0.4s\n",
      "Generation: 190 23467.52866619818 212919.13792321357 0.706287327240495 0.7036053936520746 0.367s\n",
      "Generation: 191 23456.63161004055 212820.11540901248 0.7068208808798472 0.7039457957472687 0.367s\n",
      "Generation: 192 23455.96291697682 212815.16235962493 0.7068725151030103 0.7039725689457671 0.339s\n",
      "Generation: 193 23444.009847085847 212719.84750610316 0.7068380922875682 0.7039610947178392 0.454s\n",
      "Generation: 194 23442.662297857907 212711.6079306889 0.7068553036952893 0.7039878679163376 0.496s\n",
      "Generation: 195 23436.826860402685 212659.545458448 0.7066487668026368 0.7039209349200916 0.358s\n",
      "Generation: 196 23435.601655094084 212649.2280528928 0.7066659782103578 0.7039400586333047 0.427s\n",
      "Generation: 197 23434.566072404647 212640.48574773766 0.706734823841242 0.703978306059731 0.361s\n",
      "Generation: 198 23423.15984185064 212545.64152681542 0.7066487668026368 0.703813842126098 0.358s\n",
      "Generation: 199 23420.37388523825 212522.9198095757 0.7065282869485895 0.703808105012134 0.439s\n",
      "Generation: 200 23391.04619065112 212254.14249394104 0.7073028002960362 0.7046074762244435 0.205s\n",
      "Generation: 201 23373.183397425048 212112.3590998513 0.707681451265899 0.7045768782833025 0.243s\n",
      "Generation: 202 23358.192728818707 211997.79774697352 0.707629817042736 0.7045443679708402 0.261s\n",
      "Generation: 203 23334.21140125367 211785.40613778893 0.7081805820898092 0.7050454092570246 0.218s\n",
      "Generation: 204 23314.203364708857 211587.84370794272 0.7083354847592984 0.7055292392013173 0.244s\n",
      "Generation: 205 23296.224849774575 211412.59813373984 0.7084043303901826 0.7058084454142292 0.271s\n",
      "Generation: 206 23284.17913782378 211322.14963361408 0.7083010619438563 0.7057816722157307 0.212s\n",
      "Generation: 207 23264.1523542595 211132.80116584108 0.7088346155832086 0.706166058851315 0.161s\n",
      "Generation: 208 23229.703757544765 210838.66268358374 0.7095574947074921 0.7068143527292408 0.185s\n",
      "Generation: 209 23205.22264852724 210637.6608295235 0.7100566255314023 0.7070782599715821 0.184s\n",
      "Generation: 210 23176.970027626612 210399.0618589042 0.7103147966472177 0.7078374713861441 0.176s\n",
      "Generation: 211 23147.2866855535 210125.75027484106 0.710899984509733 0.7082907033892957 0.271s\n",
      "Generation: 212 23129.041294410967 209956.6003813415 0.7110204643637803 0.7086712652822373 0.291s\n",
      "Generation: 213 23118.49081746889 209878.0484720066 0.7112097898487117 0.7088127807600146 0.267s\n",
      "Generation: 214 23105.313861795046 209762.0129434704 0.7116572864494587 0.7091359715133168 0.228s\n",
      "Generation: 215 23087.803870042535 209610.68288681548 0.7120875716424846 0.709495497321724 0.255s\n",
      "Generation: 216 23066.507831135154 209430.456898649 0.7128620849899313 0.7100806829460463 0.271s\n",
      "Generation: 217 23046.678974499824 209245.79027921855 0.7133956386292835 0.7103981365853845 0.297s\n",
      "Generation: 218 23033.060989301688 209112.0613504243 0.7137054439682622 0.7107308891952933 0.22s\n",
      "Generation: 219 23025.766722298737 209041.63583993915 0.7139119808609146 0.7108704923017493 0.228s\n",
      "Generation: 220 23018.65981866799 208973.51828000462 0.7140668835304039 0.7111458737720185 0.274s\n",
      "Generation: 221 23004.3724348225 208837.33344213868 0.7146348599851982 0.7112912139924384 0.256s\n",
      "Generation: 222 22988.57015831212 208718.37907491144 0.7148930311010138 0.7116698635140588 0.196s\n",
      "Generation: 223 22961.391157751706 208479.67161848166 0.7151684136245503 0.712029389322466 0.274s\n",
      "Generation: 224 22947.76223213156 208361.589515307 0.7155642760021342 0.7121766419142072 0.33s\n",
      "Generation: 225 22936.61036453051 208255.6661752656 0.7156847558561815 0.7124558481271192 0.357s\n",
      "Generation: 226 22917.372709017447 208080.26330752837 0.7158912927488339 0.7126700337151064 0.246s\n",
      "Generation: 227 22885.54328871557 207833.4846215478 0.7162183094955337 0.7130180852955856 0.34s\n",
      "Generation: 228 22882.14847721841 207801.07855482644 0.716338789349581 0.7130658945786186 0.262s\n",
      "Generation: 229 22877.97942066987 207761.44329642074 0.716407634980465 0.7130773688065464 0.297s\n",
      "Generation: 230 22852.404363121874 207566.23791174233 0.7169928228429804 0.7134254203870257 0.308s\n",
      "Generation: 231 22849.37817169267 207541.034283944 0.7169239772120962 0.7134751420413799 0.245s\n",
      "Generation: 232 22830.077058827723 207372.39078379647 0.7173370509974011 0.7137811214527903 0.254s\n",
      "Generation: 233 22820.676362335707 207296.76543773164 0.7174575308514484 0.7139226369305676 0.253s\n",
      "Generation: 234 22819.500030381383 207284.49835718537 0.7173370509974011 0.7139379359011381 0.234s\n",
      "Generation: 235 22806.76710850641 207174.96925103466 0.7176468563363797 0.7141540338604466 0.268s\n",
      "Generation: 236 22803.62472425814 207146.1704805 0.717767336190427 0.7142133173714074 0.285s\n",
      "Generation: 237 22782.91629758199 206971.82046479185 0.7179566616753584 0.7145001730696046 0.323s\n",
      "Generation: 238 22775.018108348555 206905.4160673673 0.7180771415294057 0.7146283019481326 0.271s\n",
      "Generation: 239 22768.62202010935 206856.14005646066 0.718197621383453 0.7147067091723066 0.272s\n",
      "Generation: 240 22750.117158278194 206693.1553173391 0.7182320441988951 0.7150165133263595 0.224s\n",
      "Generation: 241 22739.369499969165 206589.27367428574 0.7186451179841999 0.7151809772599926 0.218s\n",
      "Generation: 242 22729.31698438997 206508.79741590735 0.7186967522073631 0.7153645649068389 0.225s\n",
      "Generation: 243 22726.959133232398 206492.51730740228 0.7186967522073631 0.7154391473883701 0.227s\n",
      "Generation: 244 22725.119286893125 206480.38415697723 0.7186967522073631 0.7153856009913733 0.359s\n",
      "Generation: 245 22706.610112622715 206330.89137093388 0.7189377119154575 0.7155959618367179 0.302s\n",
      "Generation: 246 22698.824052206364 206267.34534093976 0.7189549233231786 0.7156342092631442 0.268s\n",
      "Generation: 247 22697.28422129898 206257.29527654557 0.7189377119154575 0.7155940494653966 0.338s\n",
      "Generation: 248 22680.068813099973 206112.27927333023 0.7189893461386206 0.7158005855680986 0.318s\n",
      "Generation: 249 22678.861805579298 206105.25647852823 0.7191786716235521 0.7157680752556362 0.307s\n",
      "Generation: 250 22660.60096483522 205955.0122559685 0.7195745340011359 0.7161467247772566 0.339s\n",
      "Generation: 251 22659.65929421256 205949.97128833397 0.7195228997779728 0.7161256886927221 0.315s\n",
      "Generation: 252 22624.36541834433 205652.56137029405 0.7203662587563037 0.7166974877177952 0.343s\n",
      "Generation: 253 22623.569689770968 205648.54102695375 0.7202974131254195 0.7166783640045821 0.304s\n",
      "Generation: 254 22623.03687145798 205646.4756525558 0.7203318359408616 0.7166592402913688 0.333s\n",
      "Generation: 255 22622.661405605777 205645.55583050687 0.7202974131254195 0.7166668897766542 0.278s\n",
      "Generation: 256 22607.93217453208 205539.2665588043 0.7205727956489562 0.7167892815412183 0.338s\n",
      "Generation: 257 22607.701936783316 205539.38402561742 0.7206244298721193 0.7167778073132904 0.309s\n",
      "Generation: 258 22607.49453650932 205539.7680900599 0.7206588526875614 0.7167892815412183 0.306s\n",
      "Generation: 259 22601.112434200255 205483.69733391254 0.7206932755030034 0.7169040238204972 0.242s\n",
      "Generation: 260 22576.28420649549 205258.69303586925 0.7207965439493296 0.7172080908605862 0.243s\n",
      "Generation: 261 22568.622295459805 205187.80065532483 0.720985869434261 0.7174241888198948 0.305s\n",
      "Generation: 262 22563.50360553619 205160.0547600875 0.7210202922497031 0.7173782919081833 0.301s\n",
      "Generation: 263 22556.774627946594 205101.68201681267 0.7210375036574241 0.7173629929376127 0.22s\n",
      "Generation: 264 22539.238131158676 204954.12143193235 0.7215366344813342 0.7177244311173412 0.21s\n",
      "Generation: 265 22533.28726238258 204904.77735753916 0.7216743257431025 0.7178755084517251 0.264s\n",
      "Generation: 266 22521.60420863699 204821.85546432392 0.7219669196743602 0.7179634775325056 0.283s\n",
      "Generation: 267 22521.376055280438 204821.82922263906 0.721880862635755 0.7179443538192923 0.307s\n",
      "Generation: 268 22511.6615095401 204745.8153894672 0.7220013424898022 0.7181451528080305 0.32s\n",
      "Generation: 269 22511.44552266199 204745.98917214925 0.7221046109361284 0.7181164672382107 0.365s\n",
      "Generation: 270 22511.291561491886 204746.4593633016 0.7220701881206864 0.7180973435249975 0.322s\n",
      "Generation: 271 22474.983462269647 204420.8876303742 0.7230168155453435 0.7189120137078776 0.278s\n",
      "Generation: 272 22470.82100389079 204384.66220621165 0.7231028725839487 0.7188890652520219 0.37s\n",
      "Generation: 273 22458.237970108246 204281.6966656069 0.7229823927299014 0.7190554415569762 0.335s\n",
      "Generation: 274 22423.88678830543 203968.68951903615 0.723722483261906 0.719260065288357 0.372s\n",
      "Generation: 275 22390.616199039014 203706.44852173253 0.7239462315622794 0.7198509880266432 0.394s\n",
      "Generation: 276 22379.880110902628 203630.53234382684 0.7240667114163267 0.7199140962802465 0.329s\n",
      "Generation: 277 22379.43424288837 203628.5878676901 0.7241011342317688 0.7198796735964629 0.293s\n",
      "Generation: 278 22365.644223263578 203512.73244959788 0.7244453623861896 0.7203941014818965 0.297s\n",
      "Generation: 279 22335.467607919993 203249.77443135844 0.7246174764633999 0.7205949004706346 0.23s\n",
      "Generation: 280 22318.874312614524 203117.9241407926 0.7244109395707475 0.7207019932646282 0.341s\n",
      "Generation: 281 22311.855706841758 203058.83870561092 0.7247551677251682 0.7207172922351988 0.319s\n",
      "Generation: 282 22305.29465959796 203005.17577009837 0.7245658422402368 0.7208320345144776 0.238s\n",
      "Generation: 283 22269.53460326689 202687.47340709026 0.725082184471868 0.7212336324919537 0.401s\n",
      "Generation: 284 22268.94137260741 202684.2035832631 0.7250477616564259 0.7212623180617734 0.429s\n",
      "Generation: 285 22262.220481235992 202625.43988727 0.7248928589869366 0.72119920980817 0.398s\n",
      "Generation: 286 22261.632896235333 202621.69260419995 0.7250305502487049 0.7212336324919537 0.393s\n",
      "Generation: 287 22250.11359143546 202521.2835205474 0.7254952582571729 0.7214019211682294 0.419s\n",
      "Generation: 288 22249.74793802961 202519.7467729022 0.725564103888057 0.7214325191093705 0.326s\n",
      "Generation: 289 22241.62808944829 202452.04455699888 0.7256157381112202 0.7215529985026132 0.336s\n",
      "Generation: 290 22222.19880934494 202288.5962529623 0.7258739092270357 0.72175188512003 0.437s\n",
      "Generation: 291 22209.251750835436 202179.30290823002 0.7262181373814565 0.7220234418476567 0.362s\n",
      "Generation: 292 22204.420427131332 202135.7145058057 0.7262697716046196 0.7221171480424011 0.369s\n",
      "Generation: 293 22189.99027206047 202020.6741645086 0.726493519904993 0.722396354255313 0.367s\n",
      "Generation: 294 22173.0983742597 201863.39412826535 0.7266828453899244 0.7225206583911985 0.346s\n",
      "Generation: 295 22159.565331997157 201736.88357806057 0.7270270735443451 0.7226239264425495 0.364s\n",
      "Generation: 296 22159.25791855641 201735.51263571437 0.726975439321182 0.7226315759278348 0.361s\n",
      "Generation: 297 22159.034646768607 201734.82080312778 0.7270098621366241 0.7226124522146216 0.388s\n",
      "Generation: 298 22148.657460026174 201648.30344506196 0.7270270735443451 0.7228075140893957 0.395s\n",
      "Generation: 299 22130.784648682427 201517.6019290454 0.7274573587373712 0.7231938130963013 0.384s\n",
      "Generation: 300 22121.29849757075 201438.5994992355 0.7274745701450922 0.7234137357982525 0.165s\n",
      "Generation: 301 22115.358722863286 201402.12673995862 0.7275262043682553 0.7234519832246787 0.251s\n",
      "Generation: 302 22110.437137010114 201374.64854013917 0.7275950499991394 0.7234940553937477 0.263s\n",
      "Generation: 303 22107.658266286307 201351.53185345416 0.7276466842223025 0.7235284780775314 0.218s\n",
      "Generation: 304 22107.248303360117 201348.38957041164 0.7276294728145815 0.7235514265333871 0.218s\n",
      "Generation: 305 22106.901173544946 201345.7320758569 0.7275950499991394 0.7235571636473511 0.236s\n",
      "Generation: 306 22093.849746360862 201225.13057838046 0.7277155298531867 0.7238516688308335 0.248s\n",
      "Generation: 307 22067.374710828404 201020.3740039486 0.7281113922307706 0.7242437049517031 0.204s\n",
      "Generation: 308 22066.561109987982 201011.40077306324 0.7282146606770967 0.724255179179631 0.16s\n",
      "Generation: 309 22060.200395524687 200961.30841749985 0.7282318720848178 0.7242991637200212 0.224s\n",
      "Generation: 310 22055.671968344643 200926.3028682378 0.7282490834925388 0.7242934266060572 0.195s\n",
      "Generation: 311 22047.99255190357 200853.86863804745 0.728317929123423 0.7243412358890902 0.213s\n",
      "Generation: 312 22041.219097021338 200789.61944270128 0.7284211975697492 0.7243603596023033 0.264s\n",
      "Generation: 313 22035.71028858137 200737.26806481375 0.7284039861620282 0.724383308058159 0.24s\n",
      "Generation: 314 22030.45000579958 200704.95415366904 0.7283867747543071 0.7244100812566574 0.233s\n",
      "Generation: 315 22026.493691026004 200678.86849163557 0.7283867747543071 0.7244636276536542 0.21s\n",
      "Generation: 316 22020.59142319963 200641.93183606304 0.7285933116469596 0.7245401225065068 0.241s\n",
      "Generation: 317 22016.400688028552 200603.58805886985 0.7285416774237965 0.7245343853925429 0.283s\n",
      "Generation: 318 22012.787115154202 200576.48625374076 0.7285588888315175 0.7245190864219724 0.242s\n",
      "Generation: 319 22006.054501293267 200518.05459363584 0.7289375398013803 0.7247581328371367 0.26s\n",
      "Generation: 320 22002.530491228605 200484.83494054095 0.7291440766940328 0.7248843493443435 0.167s\n",
      "Generation: 321 21999.24254074429 200451.40177879718 0.729212922324917 0.7249283338847338 0.233s\n",
      "Generation: 322 21995.074434602237 200420.1015978106 0.729212922324917 0.7249761431677666 0.255s\n",
      "Generation: 323 21994.472809273164 200416.39320647472 0.7291612881017538 0.724941720483983 0.351s\n",
      "Generation: 324 21990.4436305556 200377.049766929 0.729247345140359 0.724972318425124 0.247s\n",
      "Generation: 325 21977.062694886932 200255.790537243 0.7294883048484535 0.7253184576342819 0.238s\n",
      "Generation: 326 21973.091994087743 200222.0466333894 0.7296087847025008 0.7253987772297772 0.259s\n",
      "Generation: 327 21969.413848224827 200190.86043546803 0.7296259961102218 0.7254657102260231 0.287s\n",
      "Generation: 328 21962.129684834432 200129.95821262564 0.7295915732947797 0.7255249937369839 0.217s\n",
      "Generation: 329 21953.496272733613 200055.07451299363 0.7297292645565481 0.7255307308509479 0.298s\n",
      "Generation: 330 21953.050533022022 200052.55168364226 0.7296604189256639 0.7254905710532003 0.213s\n",
      "Generation: 331 21952.74547610692 200051.1478440315 0.7296604189256639 0.7254504112554526 0.193s\n",
      "Generation: 332 21952.54889711848 200050.58776737965 0.7297636873719902 0.7254370246562034 0.203s\n",
      "Generation: 333 21952.426449568513 200050.55932065292 0.729712053148827 0.7254370246562034 0.286s\n",
      "Generation: 334 21941.36337998326 199964.62645238248 0.729712053148827 0.7256091380751217 0.218s\n",
      "Generation: 335 21933.24177768106 199895.85327786332 0.7298669558183164 0.7258749576887845 0.311s\n",
      "Generation: 336 21932.784137327006 199893.06046480432 0.7297636873719902 0.7258539216042501 0.329s\n",
      "Generation: 337 21924.466299497555 199829.9275945609 0.7301423383418529 0.726024122651847 0.307s\n",
      "Generation: 338 21913.47170760648 199739.1883304834 0.7300907041186898 0.7260910556480931 0.236s\n",
      "Generation: 339 21913.06568485927 199736.77987504 0.7300907041186898 0.7260375092510962 0.275s\n",
      "Generation: 340 21901.133213603065 199637.26387324344 0.7301423383418529 0.7261350401884833 0.239s\n",
      "Generation: 341 21887.741429107387 199514.75418150076 0.7304693550885527 0.7262803804089032 0.338s\n",
      "Generation: 342 21881.312142513674 199459.0394753305 0.7306242577580421 0.7262861175228672 0.272s\n",
      "Generation: 343 21876.118760609887 199414.26859312816 0.7307791604275313 0.7263740866036477 0.317s\n",
      "Generation: 344 21875.600089743213 199411.0245782735 0.7306931033889262 0.7262956793794737 0.295s\n",
      "Generation: 345 21871.20052024183 199373.00342970886 0.7307275262043682 0.7263492257764705 0.315s\n",
      "Generation: 346 21857.571762519936 199266.56106823773 0.7309856973201838 0.7264295453719658 0.246s\n",
      "Generation: 347 21854.02102640615 199235.5068186285 0.7309684859124628 0.7264773546549986 0.244s\n",
      "Generation: 348 21827.360886055238 198988.51074757855 0.7310029087279049 0.726850267062655 0.28s\n",
      "Generation: 349 21817.343677959474 198900.33786679548 0.7314159825132097 0.727068277393285 0.306s\n",
      "Generation: 350 21805.950928876886 198810.43167692496 0.7314504053286518 0.7271160866763178 0.265s\n",
      "Generation: 351 21796.700820710572 198718.09539425757 0.7320355931911671 0.7274010300031937 0.269s\n",
      "Generation: 352 21789.139930958965 198642.13820452997 0.7321904958606564 0.7276917104440335 0.264s\n",
      "Generation: 353 21770.292634449415 198503.89409385112 0.73241424416103 0.7278064527233123 0.21s\n",
      "Generation: 354 21759.30289080938 198417.9442398134 0.73241424416103 0.7279231073739125 0.436s\n",
      "Generation: 355 21757.207463574603 198400.75210940096 0.7322765528992616 0.7279575300576963 0.319s\n",
      "Generation: 356 21747.79201781008 198330.5286811799 0.7321732844529354 0.7280110764546931 0.307s\n",
      "Generation: 357 21746.069062232735 198316.44644623357 0.7322937643069827 0.7280569733664046 0.244s\n",
      "Generation: 358 21742.139284605913 198278.72124530055 0.7324830897919141 0.7281315558479359 0.245s\n",
      "Generation: 359 21735.009973990786 198216.6998057216 0.7325863582382403 0.7283438290646018 0.28s\n",
      "Generation: 360 21729.15685389852 198157.04447360762 0.7328101065386138 0.7286134734209072 0.349s\n",
      "Generation: 361 21723.77981272038 198110.69028513963 0.732896163577219 0.728703354873009 0.365s\n",
      "Generation: 362 21717.877991957626 198059.40520566158 0.7328273179463348 0.7289156280896749 0.373s\n",
      "Generation: 363 21713.951692956125 198030.88836406005 0.7327412609077296 0.7289749116006357 0.337s\n",
      "Generation: 364 21707.84808756038 197978.13876988375 0.7329822206158242 0.729110689964449 0.325s\n",
      "Generation: 365 21706.80473295872 197970.3354205256 0.7328273179463348 0.7290494940821669 0.325s\n",
      "Generation: 366 21705.16766085186 197956.69491502087 0.7329650092081031 0.7290647930527374 0.351s\n",
      "Generation: 367 21689.444045772823 197829.78488849592 0.7331715461007556 0.7292139580158 0.267s\n",
      "Generation: 368 21672.48348933916 197687.23841986372 0.7333264487702449 0.7294185817471807 0.257s\n",
      "Generation: 369 21671.22691663417 197677.92842537974 0.7334125058088501 0.7293860714347183 0.302s\n",
      "Generation: 370 21666.563030816316 197641.33037893462 0.7332403917316397 0.7294472673170004 0.298s\n",
      "Generation: 371 21665.380998149536 197632.5613071751 0.7330166434312663 0.7294434425743578 0.32s\n",
      "Generation: 372 21651.654966539983 197517.32840306577 0.7333264487702449 0.7295868704234564 0.299s\n",
      "Generation: 373 21634.81772857415 197367.64640733367 0.7335674084783395 0.729936834375257 0.445s\n",
      "Generation: 374 21633.561942815442 197357.88857938926 0.7336018312937815 0.7299502209745061 0.3s\n",
      "Generation: 375 21632.554315245554 197350.1911103708 0.7336018312937815 0.7299559580884701 0.277s\n",
      "Generation: 376 21617.07722365859 197226.84217741707 0.7338944252250391 0.7302141282168476 0.351s\n",
      "Generation: 377 21615.899510904525 197217.6603606589 0.733808368186434 0.7302485509006312 0.365s\n",
      "Generation: 378 21608.535891813168 197155.14984065233 0.7340837507099706 0.7304321385474775 0.281s\n",
      "Generation: 379 21607.793782898574 197149.84150991202 0.7340321164868074 0.7304244890621923 0.29s\n",
      "Generation: 380 21607.228077786895 197146.07266942845 0.7339976936713654 0.7304283138048349 0.402s\n",
      "Generation: 381 21602.52365293437 197106.47080179094 0.7342214419717389 0.7304608241172972 0.351s\n",
      "Generation: 382 21597.40685861455 197063.7715509811 0.7343247104180651 0.7306252880509303 0.393s\n",
      "Generation: 383 21592.238504144116 197015.6354815799 0.7346861499802069 0.7306979581611402 0.377s\n",
      "Generation: 384 21591.5512507091 197011.17380881682 0.7344451902721123 0.7306692725913205 0.31s\n",
      "Generation: 385 21586.55157902495 196977.07200589337 0.7345312473107175 0.7307075200177469 0.239s\n",
      "Generation: 386 21586.084313155887 196973.92110534766 0.7344451902721123 0.7306673602199992 0.386s\n",
      "Generation: 387 21579.1638943415 196919.37468196577 0.7345312473107175 0.7306750097052844 0.329s\n",
      "Generation: 388 21578.717053560435 196916.35282076505 0.7345656701261596 0.7306597107347139 0.501s\n",
      "Generation: 389 21574.39717256829 196878.85492373197 0.7345312473107175 0.7306883963045336 0.344s\n",
      "Generation: 390 21573.946465746892 196874.8470965845 0.7345828815338806 0.7307189942456747 0.375s\n",
      "Generation: 391 21570.461281567903 196844.85030648354 0.7344624016798333 0.7307323808449239 0.323s\n",
      "Generation: 392 21568.407997282637 196826.23965693612 0.7345656701261596 0.7307591540434223 0.301s\n",
      "Generation: 393 21568.095190778775 196824.06567427685 0.7345312473107175 0.7307170818743534 0.332s\n",
      "Generation: 394 21567.493863034906 196819.7276290722 0.7345828815338806 0.7307706282713502 0.298s\n",
      "Generation: 395 21554.982717796713 196704.35504694167 0.7346173043493227 0.7308585973521307 0.328s\n",
      "Generation: 396 21552.131360331157 196680.35938641426 0.7345484587184385 0.7308681592087373 0.543s\n",
      "Generation: 397 21542.524279158737 196605.58947013106 0.7345656701261596 0.7310536592269048 0.456s\n",
      "Generation: 398 21537.834889120455 196566.98483016228 0.7346861499802069 0.7311033808812589 0.32s\n",
      "Generation: 399 21537.557340991516 196565.3740470995 0.7347033613879279 0.7311129427378655 0.398s\n",
      "Generation: 400 21529.072100430043 196500.18610182143 0.7347722070188121 0.7311645767635411 0.239s\n",
      "Generation: 401 21523.974586564087 196471.48397879288 0.7345140359029965 0.7312391592450723 0.198s\n",
      "Generation: 402 21522.091831444603 196456.12203820047 0.7344279788643914 0.7311722262488263 0.285s\n",
      "Generation: 403 21518.010603551622 196435.0128413948 0.7345140359029965 0.7312965303847118 0.226s\n",
      "Generation: 404 21488.198359600603 196191.293125683 0.7353746062890484 0.7320194067441687 0.307s\n",
      "Generation: 405 21464.95999557286 196006.14453325668 0.7363212337137054 0.7328034789859077 0.244s\n",
      "Generation: 406 21461.87164632588 195979.24859800376 0.7362179652673793 0.7328130408425143 0.235s\n",
      "Generation: 407 21459.690164992073 195960.3132725631 0.7362695994905424 0.7327862676440159 0.232s\n",
      "Generation: 408 21449.00282045766 195882.08062672307 0.7366138276449631 0.7329966284893605 0.264s\n",
      "Generation: 409 21442.329160131074 195827.8029315356 0.736596616237242 0.7330291388018229 0.244s\n",
      "Generation: 410 21440.969962376123 195816.58784406964 0.7364761363831948 0.7329583810629342 0.299s\n",
      "Generation: 411 21434.839509792295 195763.12240064895 0.7364245021600316 0.7331496181950657 0.234s\n",
      "Generation: 412 21433.023115382817 195747.96435646163 0.7364245021600316 0.7331094583973181 0.298s\n",
      "Generation: 413 21416.565979771716 195616.5619826015 0.7365621934218 0.7332643604743446 0.248s\n",
      "Generation: 414 21393.96612774419 195425.76114839615 0.7374399752155729 0.7338361594994177 0.359s\n",
      "Generation: 415 21393.13633884707 195417.62023074014 0.7373194953615256 0.7338285100141324 0.244s\n",
      "Generation: 416 21374.48332587389 195260.12277046524 0.7380251630780882 0.7342740925319987 0.272s\n",
      "Generation: 417 21359.99652716355 195139.1894219788 0.7384554482711141 0.7347426235057208 0.245s\n",
      "Generation: 418 21351.782086182964 195075.86648709135 0.7384726596788351 0.7349089998106753 0.272s\n",
      "Generation: 419 21351.183733735 195070.3790501439 0.7384726596788351 0.734931948266531 0.244s\n",
      "Generation: 420 21350.45873153007 195064.65741462525 0.7383693912325089 0.7348841389834981 0.324s\n",
      "Generation: 421 21346.03212830299 195025.51538600092 0.7381972771552985 0.7348975255827473 0.262s\n",
      "Generation: 422 21343.805323246328 195008.3084362147 0.7381284315244144 0.7349109121819966 0.282s\n",
      "Generation: 423 21328.098883739203 194881.91871823632 0.7382316999707406 0.7350333039465607 0.259s\n",
      "Generation: 424 21324.20489076931 194849.42154933742 0.7381972771552985 0.7350486029171313 0.24s\n",
      "Generation: 425 21320.909892295444 194820.5295283458 0.7382316999707406 0.7350811132295936 0.201s\n",
      "Generation: 426 21312.271650820643 194737.94443020306 0.738403814047951 0.7351537833398035 0.267s\n",
      "Generation: 427 21304.876108395896 194678.13841272422 0.7382833341939037 0.7352379276779414 0.277s\n",
      "Generation: 428 21303.745350319514 194669.14410293277 0.7381456429321355 0.7351977678801938 0.199s\n",
      "Generation: 429 21301.35007388972 194647.77994211577 0.7381972771552985 0.7352360153066201 0.206s\n",
      "Generation: 430 21299.128041201893 194629.85543775326 0.7382833341939037 0.735272350361725 0.235s\n",
      "Generation: 431 21298.3237072929 194623.5241290576 0.7383693912325089 0.7352092421081217 0.211s\n",
      "Generation: 432 21296.94895364163 194611.2796444047 0.7382833341939037 0.7351977678801938 0.261s\n",
      "Generation: 433 21295.589470282262 194598.86143542622 0.7383177570093458 0.7351155359133772 0.277s\n",
      "Generation: 434 21280.23761432834 194471.17819801028 0.7388513106486979 0.7357294071075192 0.33s\n",
      "Generation: 435 21276.14637496264 194447.05332015458 0.7389717905027452 0.7358097267030145 0.377s\n",
      "Generation: 436 21275.863229617033 194443.461960645 0.7389373676873031 0.7357925153611227 0.204s\n",
      "Generation: 437 21275.14734401899 194436.90418741974 0.7389373676873031 0.7358097267030145 0.327s\n",
      "Generation: 438 21272.970890064153 194422.66889583852 0.73888573346414 0.7358059019603719 0.364s\n",
      "Generation: 439 21272.329787192622 194418.19815583457 0.7388168878332558 0.7358059019603719 0.37s\n",
      "Generation: 440 21270.719632422744 194408.42779907252 0.7388340992409769 0.7357867782471587 0.36s\n",
      "Generation: 441 21263.792630035336 194354.35164083206 0.7390578475413504 0.7360525978608214 0.34s\n",
      "Generation: 442 21260.493686815895 194322.42569111273 0.7387996764255348 0.7360640720887494 0.286s\n",
      "Generation: 443 21257.556941448962 194303.11562331105 0.7387480422023718 0.736134829827638 0.351s\n",
      "Generation: 444 21247.451395421933 194220.16888456093 0.7391266931722346 0.7364293350111204 0.398s\n",
      "Generation: 445 21246.408811290028 194214.19160267917 0.7392127502108398 0.7363853504707302 0.293s\n",
      "Generation: 446 21246.10815365211 194211.57899498922 0.7392299616185608 0.7363451906729825 0.321s\n",
      "Generation: 447 21245.88923467238 194209.69988657444 0.7391955388031187 0.7363337164450547 0.319s\n",
      "Generation: 448 21239.23435629534 194156.88642606756 0.739298807249445 0.7364790566654746 0.303s\n",
      "Generation: 449 21238.422554188488 194151.6245635032 0.7392299616185608 0.736475231922832 0.337s\n",
      "Generation: 450 21238.29041954418 194150.1445957223 0.7392127502108398 0.7364503710956549 0.328s\n",
      "Generation: 451 21234.335881549014 194117.38012219802 0.7396774582193077 0.7367334020512095 0.316s\n",
      "Generation: 452 21234.280532462133 194116.6074114651 0.7396774582193077 0.7366836803968553 0.344s\n",
      "Generation: 453 21233.51120564683 194112.5866489334 0.7397118810347498 0.736681768025534 0.311s\n",
      "Generation: 454 21233.58932825667 194113.30628077404 0.7396774582193077 0.7366951546247832 0.299s\n",
      "Generation: 455 21227.26684674363 194061.90465628472 0.7397463038501919 0.7368232835033113 0.274s\n",
      "Generation: 456 21215.148600336775 193959.03099916433 0.7398667837042392 0.7369418505252328 0.256s\n",
      "Generation: 457 21209.960346842076 193917.23586699157 0.7399528407428444 0.7370240824920493 0.339s\n",
      "Generation: 458 21209.41161523678 193914.67157263157 0.7399700521505654 0.7369992216648722 0.344s\n",
      "Generation: 459 21205.668615673978 193884.35392976855 0.7399872635582865 0.7369762732090165 0.365s\n",
      "Generation: 460 21205.487152968293 193883.23820886225 0.7400388977814495 0.7369762732090165 0.315s\n",
      "Generation: 461 21205.365843116317 193882.6631752176 0.7400561091891705 0.7369590618671246 0.439s\n",
      "Generation: 462 21200.919964640172 193847.08061525854 0.7402110118586599 0.7370699794037608 0.412s\n",
      "Generation: 463 21200.49462198375 193844.84176037982 0.7402282232663809 0.7370814536316888 0.301s\n",
      "Generation: 464 21200.404744896463 193845.29303452713 0.740245434674102 0.73708336600301 0.37s\n",
      "Generation: 465 21193.631070490705 193786.80080012424 0.7404175487513124 0.7370891031169741 0.4s\n",
      "Generation: 466 21191.818678577845 193776.17027000667 0.7403142803049861 0.7371101392015085 0.358s\n",
      "Generation: 467 21183.95684047583 193708.6696078996 0.7405208171976386 0.737234443337394 0.388s\n",
      "Generation: 468 21180.16546850117 193677.96091326975 0.7406412970516859 0.7373071134476039 0.294s\n",
      "Generation: 469 21172.595024283066 193615.51530555796 0.7403487031204282 0.7374199433555615 0.39s\n",
      "Generation: 470 21172.3896999506 193613.34130985884 0.7403659145281493 0.7374314175834894 0.448s\n",
      "Generation: 471 21171.407966244908 193610.00401117152 0.740262646081823 0.7374237680982041 0.311s\n",
      "Generation: 472 21171.564952103254 193611.09615101304 0.7401765890432178 0.7374122938702762 0.229s\n",
      "Generation: 473 21171.723806357026 193612.2595280778 0.7402110118586599 0.7374008196423483 0.373s\n",
      "Generation: 474 21170.229583768352 193599.92033817153 0.7401938004509389 0.7374218557268828 0.321s\n",
      "Generation: 475 21168.818130971456 193595.2371230849 0.7403314917127072 0.7373759588151713 0.383s\n",
      "Generation: 476 21168.510168813227 193592.54735081823 0.7403487031204282 0.737404644384991 0.286s\n",
      "Generation: 477 21163.1187063554 193546.07496960807 0.7403142803049861 0.7374390670687746 0.323s\n",
      "Generation: 478 21158.86520141262 193509.72842632924 0.7402970688972651 0.7374314175834894 0.363s\n",
      "Generation: 479 21157.703518343413 193506.1679608031 0.7403142803049861 0.7373836083004565 0.475s\n",
      "Generation: 480 21151.841017833045 193458.87489534472 0.7404347601590334 0.7374122938702762 0.353s\n",
      "Generation: 481 21145.962566435475 193407.66570309573 0.7403831259358703 0.7374199433555615 0.366s\n",
      "Generation: 482 21145.540794332577 193403.87278114288 0.7404175487513124 0.7374314175834894 0.389s\n",
      "Generation: 483 21136.323492182935 193319.3201686766 0.740658508459407 0.73770871142508 0.37s\n",
      "Generation: 484 21136.200827463887 193318.68707076303 0.7405724514208017 0.7377201856530079 0.354s\n",
      "Generation: 485 21136.013822773682 193317.0245676095 0.7405552400130807 0.73770871142508 0.306s\n",
      "Generation: 486 21132.84441496322 193290.23078066506 0.7405380286053597 0.7377125361677226 0.337s\n",
      "Generation: 487 21122.886756685442 193204.4961796176 0.7405896628285228 0.7377507835941489 0.364s\n",
      "Generation: 488 21108.571474581393 193092.3216410592 0.7405724514208017 0.7379190722704246 0.378s\n",
      "Generation: 489 21108.264717043163 193089.87454409857 0.7406412970516859 0.737909510413818 0.317s\n",
      "Generation: 490 21107.230073213923 193080.485050257 0.7406929312748489 0.7379477578402444 0.286s\n",
      "Generation: 491 21101.117735552634 193030.182093259 0.7406240856439649 0.7379343712409951 0.484s\n",
      "Generation: 492 21097.84302605615 193002.6105118321 0.740727354090291 0.7379707062961001 0.354s\n",
      "Generation: 493 21094.540465334267 192976.9952666014 0.740727354090291 0.7380357269210248 0.356s\n",
      "Generation: 494 21094.081714707692 192973.74328794712 0.7406929312748489 0.7380146908364903 0.366s\n",
      "Generation: 495 21088.842317751354 192932.14065954858 0.740744565498012 0.7380663248621658 0.348s\n",
      "Generation: 496 21084.964890673447 192899.63116554136 0.740727354090291 0.7380777990900937 0.303s\n",
      "Generation: 497 21078.979724355093 192849.58561356686 0.7408306225366172 0.7381064846599135 0.471s\n",
      "Generation: 498 21078.59984129234 192846.97996826627 0.7408134111288962 0.7381351702297332 0.391s\n",
      "Generation: 499 21078.282221935242 192844.50933141232 0.7407961997211752 0.7381638557995529 0.319s\n",
      "Generation: 500 21069.423811909932 192764.91518874574 0.7409166795752224 0.7381963661120152 0.205s\n",
      "Generation: 501 21068.365408238893 192759.38779833497 0.7409166795752224 0.7382059279686218 0.183s\n",
      "Generation: 502 21065.75693059766 192740.87776395722 0.7409338909829435 0.7382422630237268 0.209s\n",
      "Generation: 503 21055.418299617013 192651.14326539103 0.7411748506910381 0.7383627424169696 0.217s\n",
      "Generation: 504 21052.441530535245 192636.6585903316 0.7412953305450852 0.7384679228396419 0.217s\n",
      "Generation: 505 21050.164168313884 192626.55501585672 0.7413985989914115 0.7384736599536059 0.228s\n",
      "Generation: 506 21048.379745896877 192619.90520401875 0.7413985989914115 0.7385042578947469 0.24s\n",
      "Generation: 507 21046.93846508837 192616.3397470485 0.7413469647682484 0.7384966084094616 0.263s\n",
      "Generation: 508 21042.37080842052 192590.84206810544 0.7414846560300167 0.7385711908909929 0.216s\n",
      "Generation: 509 21026.532469026304 192442.59843871175 0.7416911929226692 0.7388924692729738 0.278s\n",
      "Generation: 510 21022.314752708557 192413.34855738078 0.7417256157381112 0.7390225105228232 0.314s\n",
      "Generation: 511 21009.38605020433 192292.8841326893 0.7418633069998796 0.7393055414783778 0.236s\n",
      "Generation: 512 21008.43451746067 192291.90178811908 0.7419493640384847 0.7392998043644138 0.263s\n",
      "Generation: 513 21007.683367577414 192287.14410148075 0.7419149412230426 0.7393265775629122 0.29s\n",
      "Generation: 514 21005.290297782503 192275.28680049034 0.7419321526307636 0.7393170157063057 0.268s\n",
      "Generation: 515 20998.71179271814 192224.2091038086 0.7421559009311371 0.7393896858165157 0.263s\n",
      "Generation: 516 20984.203428743054 192090.0339058773 0.7421903237465792 0.7396402064596078 0.261s\n",
      "Generation: 517 20975.563441305676 192007.19195033715 0.7423108036006265 0.739726263169067 0.218s\n",
      "Generation: 518 20968.03818237958 191952.11735272975 0.742568974716442 0.739762598224172 0.317s\n",
      "Generation: 519 20967.279638537744 191947.783330357 0.7426033975318841 0.7397472992536015 0.214s\n",
      "Generation: 520 20966.635810033727 191944.15265815463 0.742568974716442 0.7397243507977457 0.243s\n",
      "Generation: 521 20965.207280639417 191934.68266637457 0.7425345519009999 0.7397243507977457 0.272s\n",
      "Generation: 522 20956.759792343506 191853.2231541358 0.7427583002013735 0.7397587734815294 0.238s\n",
      "Generation: 523 20948.161870639025 191774.14678722367 0.7427755116090945 0.7399079384445919 0.27s\n",
      "Generation: 524 20940.646348036746 191726.20226796748 0.7429648370940259 0.7399289745291264 0.306s\n",
      "Generation: 525 20935.93628961892 191684.56723071076 0.742982048501747 0.7400647528929397 0.316s\n",
      "Generation: 526 20928.056981559148 191612.91271750815 0.7431713739866783 0.7402540776537498 0.254s\n",
      "Generation: 527 20922.58788632524 191559.30013738293 0.7431713739866783 0.7403382219918877 0.296s\n",
      "Generation: 528 20916.05758731015 191499.64570527 0.7432574310252835 0.7403611704477435 0.267s\n",
      "Generation: 529 20913.791308529675 191477.32904537977 0.7432918538407256 0.7404338405579535 0.245s\n",
      "Generation: 530 20913.330935726637 191475.2198258459 0.7432230082098414 0.7404453147858814 0.216s\n",
      "Generation: 531 20912.958343494363 191473.69372803048 0.7432230082098414 0.7404261910726682 0.251s\n",
      "Generation: 532 20912.546502579993 191471.80790970658 0.7431369511712363 0.7404414900432387 0.291s\n",
      "Generation: 533 20912.213003780216 191470.23664582882 0.7431197397635153 0.7404261910726682 0.367s\n",
      "Generation: 534 20905.009016048 191427.24016164325 0.7431713739866783 0.7404338405579535 0.306s\n",
      "Generation: 535 20902.242171310914 191407.9840439925 0.7430164713171891 0.7403477838484943 0.321s\n",
      "Generation: 536 20898.81620358524 191379.88062356767 0.7431369511712363 0.7404166292160617 0.228s\n",
      "Generation: 537 20898.613964593067 191379.45677166566 0.7431025283557943 0.7404242787013469 0.239s\n",
      "Generation: 538 20892.907997917955 191341.26328841946 0.7432574310252835 0.7404892993262716 0.257s\n",
      "Generation: 539 20890.638586734807 191321.06685867286 0.7432746424330046 0.7405619694364816 0.319s\n",
      "Generation: 540 20888.232369526122 191301.9053731424 0.7431197397635153 0.740613603462157 0.313s\n",
      "Generation: 541 20888.00724505119 191301.2314564636 0.7431369511712363 0.7405734436644095 0.263s\n",
      "Generation: 542 20886.62531745856 191290.24943208022 0.7432057968021204 0.7406078663481931 0.299s\n",
      "Generation: 543 20879.156175395263 191234.20262960793 0.7432574310252835 0.7407704179105048 0.31s\n",
      "Generation: 544 20874.131693193078 191208.41325088884 0.7433434880638887 0.7408010158516459 0.266s\n",
      "Generation: 545 20869.93530590614 191185.28166292227 0.7433606994716098 0.740856474619964 0.268s\n",
      "Generation: 546 20867.593339640647 191164.89354855186 0.7433606994716098 0.7408947220463903 0.325s\n",
      "Generation: 547 20867.373764532458 191164.35110671006 0.7433606994716098 0.7408947220463903 0.303s\n",
      "Generation: 548 20865.91525875561 191151.7136346415 0.743481179325657 0.7408373509067508 0.305s\n",
      "Generation: 549 20864.227528336152 191137.21640236647 0.743481179325657 0.7408889849324264 0.275s\n",
      "Generation: 550 20860.95420773457 191109.65673772714 0.7435156021410991 0.7408985467890329 0.33s\n",
      "Generation: 551 20860.53128399626 191108.3418627187 0.7434295451024939 0.7409310571014953 0.273s\n",
      "Generation: 552 20860.205229422816 191107.65565739517 0.743446756510215 0.7409195828735674 0.282s\n",
      "Generation: 553 20860.097930477423 191107.9753152864 0.7434295451024939 0.7408947220463903 0.25s\n",
      "Generation: 554 20857.490459649915 191081.9772173558 0.743446756510215 0.7407665931678622 0.271s\n",
      "Generation: 555 20853.477725857254 191053.43948277875 0.743481179325657 0.7408775107044985 0.307s\n",
      "Generation: 556 20853.29832534545 191052.93849165872 0.743463967917936 0.7408526498773214 0.263s\n",
      "Generation: 557 20843.307145818537 190961.5383308309 0.7435844477719833 0.7409387065867805 0.252s\n",
      "Generation: 558 20842.798157946643 190954.32249996037 0.7436188705874254 0.7409769540132068 0.351s\n",
      "Generation: 559 20837.391869430056 190922.30825057862 0.7438081960723567 0.741026675667561 0.375s\n",
      "Generation: 560 20834.175254346388 190900.1658352083 0.743928675926404 0.7411127323770201 0.283s\n",
      "Generation: 561 20833.799628909448 190894.05426108738 0.743911464518683 0.7410974334064496 0.247s\n",
      "Generation: 562 20833.43613147449 190891.81912840423 0.7439803101495671 0.7410878715498431 0.227s\n",
      "Generation: 563 20830.25084937493 190867.01000833814 0.7441524242267775 0.7410955210351283 0.34s\n",
      "Generation: 564 20827.647536772536 190849.30560500076 0.7440491557804513 0.7411395055755186 0.346s\n",
      "Generation: 565 20825.085267459057 190829.71264309197 0.7441524242267775 0.7412121756857285 0.348s\n",
      "Generation: 566 20825.032768753143 190830.1223844762 0.7442040584499406 0.7412140880570499 0.317s\n",
      "Generation: 567 20818.17986941959 190782.5735705667 0.7442556926731038 0.7412848457959385 0.302s\n",
      "Generation: 568 20815.92701468989 190759.81890798433 0.7440147329650092 0.7411471550608039 0.239s\n",
      "Generation: 569 20811.95979872372 190734.43861075092 0.7441180014113354 0.7412198251710138 0.38s\n",
      "Generation: 570 20810.241797603623 190721.52522481838 0.7440491557804513 0.741227474656299 0.396s\n",
      "Generation: 571 20808.5489800726 190703.95189777788 0.7438598302955198 0.7411643664026957 0.332s\n",
      "Generation: 572 20800.43697665199 190650.42686930107 0.7440835785958934 0.7411184694909841 0.305s\n",
      "Generation: 573 20800.063422614825 190647.58905909368 0.7442556926731038 0.7411165571196628 0.214s\n",
      "Generation: 574 20796.748834861915 190623.43918217212 0.7444622295657561 0.7411911396011941 0.407s\n",
      "Generation: 575 20794.597143446885 190606.8411335052 0.7444278067503141 0.7412236499136564 0.4s\n",
      "Generation: 576 20794.224411101688 190605.35037473097 0.7444278067503141 0.7412255622849777 0.276s\n",
      "Generation: 577 20792.772969160767 190594.76609829342 0.7443245383039879 0.7412848457959385 0.311s\n",
      "Generation: 578 20792.45159016201 190591.954642322 0.744341749711709 0.7412676344540466 0.328s\n",
      "Generation: 579 20789.645351636493 190572.26805207357 0.7444450181580351 0.7413575159061485 0.314s\n",
      "Generation: 580 20787.223513380668 190556.23567494686 0.744410595342593 0.7414359231303224 0.225s\n",
      "Generation: 581 20786.969160643686 190550.92786354036 0.7443933839348721 0.7414531344722142 0.22s\n",
      "Generation: 582 20786.603612755818 190549.67651975263 0.744410595342593 0.7414282736450372 0.325s\n",
      "Generation: 583 20785.327667088804 190542.63889047908 0.7446171322352455 0.7414799076707126 0.311s\n",
      "Generation: 584 20774.73418849846 190442.71146931069 0.744806457720177 0.7415602272662078 0.345s\n",
      "Generation: 585 20772.486380327206 190425.72418099185 0.7446515550506876 0.7416730571741654 0.331s\n",
      "Generation: 586 20771.356918062058 190417.94645638095 0.7446859778661297 0.7417361654277688 0.385s\n",
      "Generation: 587 20771.109492934334 190417.17432619026 0.7446687664584086 0.7417476396556967 0.46s\n",
      "Generation: 588 20768.291041865625 190395.815684467 0.7447204006815717 0.7416730571741654 0.361s\n",
      "Generation: 589 20766.69509240412 190381.3054606286 0.7447204006815717 0.7416424592330244 0.367s\n",
      "Generation: 590 20762.860095267486 190349.3918666474 0.7448925147587822 0.741707479857949 0.317s\n",
      "Generation: 591 20761.202652006818 190336.05892438535 0.7447376120892928 0.7416902685160572 0.317s\n",
      "Generation: 592 20758.79205155336 190318.02816204765 0.7448580919433401 0.7417266035711622 0.396s\n",
      "Generation: 593 20756.641040499493 190302.04477458555 0.744806457720177 0.7418356087364771 0.377s\n",
      "Generation: 594 20755.767041036386 190297.3162451238 0.744806457720177 0.741841345850441 0.409s\n",
      "Generation: 595 20746.785327411657 190230.4506125281 0.7447548234970138 0.7418623819349756 0.451s\n",
      "Generation: 596 20746.663945015713 190226.3306948027 0.7448925147587822 0.7418700314202608 0.297s\n",
      "Generation: 597 20745.707776090847 190220.81215618766 0.7448753033510611 0.7418872427621527 0.411s\n",
      "Generation: 598 20745.053125166825 190215.9429365094 0.7449269375742242 0.7418776809055461 0.466s\n",
      "Generation: 599 20744.733152285124 190214.50229691973 0.7449097261665032 0.74188341801951 0.288s\n",
      "Generation: 600 20740.645706980413 190192.2829025394 0.744823669127898 0.7418222221372279 0.23s\n",
      "Generation: 601 20727.542857503347 190096.918163697 0.7449957832051084 0.7421396757765661 0.251s\n",
      "Generation: 602 20723.86900678373 190067.7694364739 0.7451334744668766 0.7422945778535927 0.245s\n",
      "Generation: 603 20717.911828779892 190021.83343049564 0.7451678972823187 0.742223820114704 0.207s\n",
      "Generation: 604 20707.582393969486 189948.50029591526 0.7453400113595291 0.7424743407577963 0.276s\n",
      "Generation: 605 20702.253908471146 189908.9334961858 0.7454777026212974 0.742564222209898 0.292s\n",
      "Generation: 606 20694.06781901091 189852.36174024607 0.7458219307757181 0.7427210366582458 0.274s\n",
      "Generation: 607 20689.961866172474 189823.96940640442 0.7460456790760917 0.7428147428529903 0.264s\n",
      "Generation: 608 20688.336095249055 189804.98413642638 0.7460801018915337 0.7428510779080952 0.198s\n",
      "Generation: 609 20685.63028023344 189783.76928698827 0.7461145247069758 0.7429084490477347 0.253s\n",
      "Generation: 610 20677.326376623834 189724.23509663175 0.7463038501919071 0.743080562466653 0.255s\n",
      "Generation: 611 20675.81833414229 189711.08724063312 0.7463726958227913 0.7431245470070432 0.218s\n",
      "Generation: 612 20674.660878878 189702.0593611508 0.7464415414536755 0.7431570573195057 0.231s\n",
      "Generation: 613 20648.74625127853 189493.18182251614 0.7465964441231648 0.7433884542493847 0.289s\n",
      "Generation: 614 20625.903004935677 189305.326170415 0.7468029810158173 0.7436198511792638 0.313s\n",
      "Generation: 615 20621.433015147635 189270.1494992846 0.7469750950930276 0.7436389748924769 0.242s\n",
      "Generation: 616 20599.62674499135 189091.131175514 0.7472332662088432 0.7438512481091428 0.31s\n",
      "Generation: 617 20580.204997466088 188931.80048212444 0.7477496084404743 0.7441782636050877 0.325s\n",
      "Generation: 618 20573.834061069127 188888.35949816645 0.7478012426636375 0.7442738821711534 0.31s\n",
      "Generation: 619 20568.724274554694 188854.73293082573 0.7479045111099637 0.7444134852776094 0.268s\n",
      "Generation: 620 20567.642641840517 188846.98922039295 0.7479733567408479 0.7444746811598915 0.289s\n",
      "Generation: 621 20545.9439648546 188680.5308422691 0.7482487392643844 0.7447443255161968 0.222s\n",
      "Generation: 622 20530.918691182367 188559.98395793847 0.7485413331956421 0.7449795471887185 0.323s\n",
      "Generation: 623 20526.39081340769 188560.68031947693 0.7485757560110842 0.7449240884204004 0.3s\n",
      "Generation: 624 20513.88488018934 188461.92794598927 0.7486618130496894 0.7450770781261056 0.307s\n",
      "Generation: 625 20513.406185114454 188459.59431716613 0.7487134472728525 0.7450981142106401 0.312s\n",
      "Generation: 626 20501.832163105333 188371.18544343757 0.7487822929037365 0.7452032946333124 0.345s\n",
      "Generation: 627 20489.427918254634 188280.77755974518 0.7492642123199257 0.7454021812507291 0.256s\n",
      "Generation: 628 20488.214312775635 188276.48687126255 0.7493158465430888 0.7454289544492275 0.264s\n",
      "Generation: 629 20481.98269587357 188216.7920183278 0.7493330579508098 0.7454958874454735 0.265s\n",
      "Generation: 630 20475.398966582892 188162.4681510403 0.7493158465430888 0.7455398719858638 0.323s\n",
      "Generation: 631 20460.853473510426 188044.46590124286 0.7495740176589043 0.7458095163421691 0.326s\n",
      "Generation: 632 20451.30639166818 187973.99893340148 0.7499698800364882 0.745899397794271 0.309s\n",
      "Generation: 633 20444.48867830626 187924.10721199724 0.7499182458133251 0.7459529441912677 0.382s\n",
      "Generation: 634 20429.059525252902 187810.69977887653 0.7501247827059775 0.7460600369852614 0.399s\n",
      "Generation: 635 20420.608649242433 187751.66315811028 0.7504173766372352 0.7462455370034289 0.327s\n",
      "Generation: 636 20408.056383133047 187649.7322233549 0.7506239135298876 0.7464444236208456 0.411s\n",
      "Generation: 637 20404.165181945857 187619.88888740452 0.7505722793067245 0.7463889648525275 0.31s\n",
      "Generation: 638 20398.565859484006 187574.52148275354 0.7506411249376086 0.7463966143378128 0.326s\n",
      "Generation: 639 20393.089393009355 187529.55081902785 0.7507099705684928 0.7463889648525275 0.29s\n",
      "Generation: 640 20392.625071570215 187526.83094795796 0.750796027607098 0.746410000937062 0.357s\n",
      "Generation: 641 20387.09195452453 187490.39941457167 0.7509337188688663 0.7464922329038784 0.319s\n",
      "Generation: 642 20386.034543176513 187486.77193768072 0.7508820846457032 0.7464788463046292 0.499s\n",
      "Generation: 643 20375.23780299539 187408.69734034056 0.7510714101306346 0.7466758205507247 0.354s\n",
      "Generation: 644 20369.423588983507 187368.56912581326 0.7510025644997504 0.7468039494292528 0.346s\n",
      "Generation: 645 20364.94418655038 187334.29792473075 0.7511574671692398 0.7469033927379611 0.311s\n",
      "Generation: 646 20363.13824680936 187321.1665909129 0.7511574671692398 0.7469186917085316 0.329s\n",
      "Generation: 647 20361.63923678017 187310.56916720734 0.7511402557615188 0.746926341193817 0.346s\n",
      "Generation: 648 20360.137695443715 187301.48701094437 0.7512091013924029 0.7469626762489219 0.443s\n",
      "Generation: 649 20358.730174006087 187291.35417183684 0.7512607356155659 0.7469970989327056 0.399s\n",
      "Generation: 650 20357.840905355883 187285.76518592684 0.751243524207845 0.7469684133628859 0.277s\n",
      "Generation: 651 20353.081939473755 187246.52859073196 0.7513812154696132 0.7470831556421648 0.372s\n",
      "Generation: 652 20338.732800967726 187113.23490515965 0.7516049637699868 0.7474044340241456 0.404s\n",
      "Generation: 653 20330.61278302849 187045.9716176601 0.7518459234780813 0.7475325629026737 0.312s\n",
      "Generation: 654 20329.210669419364 187035.63549626063 0.7518287120703603 0.7475363876453163 0.355s\n",
      "Generation: 655 20324.520156225226 186996.37759631473 0.7520868831861758 0.7476281814687394 0.406s\n",
      "Generation: 656 20323.89486604886 186992.71092291435 0.7520868831861758 0.7476281814687394 0.317s\n",
      "Generation: 657 20322.80624717375 186984.9802104358 0.7520180375552916 0.7476511299245951 0.402s\n",
      "Generation: 658 20317.638061448863 186942.7829382128 0.7520696717784547 0.7476587794098805 0.348s\n",
      "Generation: 659 20303.781098648928 186844.5634745318 0.7520180375552916 0.747947547479399 0.413s\n",
      "Generation: 660 20302.84443824413 186838.11894089502 0.7520180375552916 0.7479819701631827 0.513s\n",
      "Generation: 661 20282.92792615492 186685.15648370262 0.7524827455637597 0.7479934443911105 0.421s\n",
      "Generation: 662 20282.333368538755 186681.96470038287 0.7525515911946438 0.7479972691337532 0.351s\n",
      "Generation: 663 20274.578948847375 186619.7776099034 0.7526892824564121 0.747983882534504 0.428s\n",
      "Generation: 664 20268.37722697408 186570.76575536613 0.7528441851259015 0.7479647588212908 0.478s\n",
      "Generation: 665 20263.441927606596 186532.14193679878 0.7529302421645067 0.7479551969646843 0.462s\n",
      "Generation: 666 20258.446994581747 186490.97088182377 0.7529130307567856 0.7481712949239928 0.445s\n",
      "Generation: 667 20254.978429225506 186463.3554156956 0.7529990877953908 0.7482210165783469 0.434s\n",
      "Generation: 668 20250.295345323946 186426.68508036347 0.7528786079413435 0.7483816557693375 0.47s\n",
      "Generation: 669 20247.46206680999 186404.21750857995 0.7529646649799487 0.7484810990780458 0.389s\n",
      "Generation: 670 20243.356642397037 186372.75850475978 0.753085144833996 0.7484313774236916 0.399s\n",
      "Generation: 671 20242.55794848394 186367.69436672155 0.7531023562417171 0.7484447640229408 0.515s\n",
      "Generation: 672 20240.205088435774 186349.3373391758 0.7531712018726011 0.7485442073316492 0.337s\n",
      "Generation: 673 20230.225142757055 186253.68235950766 0.753085144833996 0.7487660424049217 0.525s\n",
      "Generation: 674 20228.393671730002 186239.23966251747 0.7528097623104594 0.7488195888019185 0.459s\n",
      "Generation: 675 20220.349904086655 186161.70286103018 0.7536014870656271 0.7488750475702366 0.469s\n",
      "Generation: 676 20219.466270968198 186157.40901589865 0.7535154300270219 0.7488540114857022 0.394s\n",
      "Generation: 677 20216.003488178652 186135.67735698924 0.753532641434743 0.7488597485996661 0.305s\n",
      "Generation: 678 20215.334089873988 186131.8973033856 0.753532641434743 0.7488654857136301 0.412s\n",
      "Generation: 679 20212.013955570637 186106.6199043514 0.7533777387652536 0.7488654857136301 0.462s\n",
      "Generation: 680 20210.460845592414 186094.85508376596 0.7533605273575326 0.7489343310811973 0.434s\n",
      "Generation: 681 20207.76546976249 186074.77329762504 0.7534121615806957 0.7489171197393055 0.46s\n",
      "Generation: 682 20207.169552585394 186071.44080908882 0.7533777387652536 0.7489266815959121 0.44s\n",
      "Generation: 683 20206.845206058475 186070.5649346052 0.7533777387652536 0.7489190321106268 0.364s\n",
      "Generation: 684 20195.66367062024 185984.62260969612 0.7534465843961378 0.7489706661363024 0.344s\n",
      "Generation: 685 20184.49902282106 185901.4083570342 0.7535154300270219 0.7491045321287944 0.363s\n",
      "Generation: 686 20170.726546890888 185804.0974752512 0.7537736011428374 0.7494640579372015 0.512s\n",
      "Generation: 687 20162.063759162826 185726.35044595593 0.754031772258653 0.7496227847568707 0.471s\n",
      "Generation: 688 20159.74531488437 185708.98036534886 0.754014560850932 0.7496113105289428 0.43s\n",
      "Generation: 689 20156.975299731093 185685.92239796944 0.7538080239582795 0.7496074857863002 0.422s\n",
      "Generation: 690 20146.70200648241 185606.51279159507 0.7541006178895372 0.749655295069333 0.466s\n",
      "Generation: 691 20146.21702317048 185604.36606089672 0.7540834064818162 0.7496533826980117 0.386s\n",
      "Generation: 692 20144.395747923907 185591.72282466636 0.7540661950740951 0.7496476455840477 0.412s\n",
      "Generation: 693 20142.670594838848 185578.76494794243 0.7540661950740951 0.749655295069333 0.384s\n",
      "Generation: 694 20141.03153865142 185567.13463262093 0.754014560850932 0.7496572074406543 0.348s\n",
      "Generation: 695 20139.754654622877 185558.4613551079 0.754014560850932 0.7495807125878018 0.424s\n",
      "Generation: 696 20138.730955463794 185551.81199151624 0.7540661950740951 0.7495768878451591 0.496s\n",
      "Generation: 697 20137.496204452636 185542.41548639262 0.7541006178895372 0.7495998363010149 0.404s\n",
      "Generation: 698 20136.69704681583 185537.44818363246 0.7541350407049793 0.7495749754738378 0.371s\n",
      "Generation: 699 20136.07994435115 185533.91350537268 0.7541350407049793 0.7495443775326968 0.565s\n",
      "Generation: 700 20133.026100660565 185506.21509245026 0.7543587890053527 0.7495501146466607 0.229s\n",
      "Generation: 701 20132.94848131636 185505.32639809887 0.7543071547821897 0.7495520270179821 0.214s\n",
      "Generation: 702 20129.696388861375 185477.09782894317 0.7544104232285158 0.7495462899040181 0.261s\n",
      "Generation: 703 20128.06684699106 185465.81426725374 0.7543415775976318 0.7495348156760901 0.268s\n",
      "Generation: 704 20125.034931976516 185447.75985891986 0.7542555205590266 0.7495482022753394 0.252s\n",
      "Generation: 705 20122.928378150504 185430.89927043967 0.7542899433744686 0.7496074857863002 0.221s\n",
      "Generation: 706 20121.16110840647 185424.0655918148 0.7543243661899107 0.749649557955369 0.271s\n",
      "Generation: 707 20119.71567548601 185414.2714203846 0.7542899433744686 0.749655295069333 0.22s\n",
      "Generation: 708 20116.2211113428 185390.12947837255 0.7543071547821897 0.7497260528082217 0.243s\n",
      "Generation: 709 20108.678497838842 185326.8729654108 0.7544620574516789 0.7497470888927561 0.284s\n",
      "Generation: 710 20107.443406774008 185322.79222555988 0.7543932118207948 0.7497337022935069 0.307s\n",
      "Generation: 711 20106.77476987048 185316.92799673625 0.7543587890053527 0.7497451765214348 0.243s\n",
      "Generation: 712 20104.22053911934 185304.4616849158 0.7542727319667476 0.7496897177531167 0.198s\n",
      "Generation: 713 20101.307435660587 185285.6096538001 0.7543415775976318 0.7496629445546182 0.271s\n",
      "Generation: 714 20089.943257571944 185184.5567550771 0.7546169601211683 0.7498427074588219 0.208s\n",
      "Generation: 715 20083.80235459106 185124.01668593127 0.7547202285674945 0.7499364136535663 0.288s\n",
      "Generation: 716 20083.11974795371 185121.18835739282 0.7546513829366104 0.7499402383962089 0.245s\n",
      "Generation: 717 20082.57674749577 185119.25993915234 0.7547374399752156 0.7499765734513139 0.318s\n",
      "Generation: 718 20082.08208132671 185115.84039106296 0.7545481144902841 0.7499440631388515 0.291s\n",
      "Generation: 719 20077.055379397974 185085.56483964325 0.7546169601211683 0.7499956971645271 0.279s\n",
      "Generation: 720 20076.502230840597 185083.58971534815 0.7546513829366104 0.7499956971645271 0.27s\n",
      "Generation: 721 20073.244153312276 185060.86908081983 0.7546685943443314 0.750062630160773 0.218s\n",
      "Generation: 722 20070.162256928696 185038.29632375718 0.7546685943443314 0.7500779291313435 0.309s\n",
      "Generation: 723 20063.955092959026 184986.90558775348 0.7547030171597735 0.7501027899585206 0.247s\n",
      "Generation: 724 20060.71155887789 184951.93677500112 0.7547718627906577 0.7501888466679798 0.189s\n",
      "Generation: 725 20059.572733573707 184946.01387950394 0.7548062856060997 0.7501945837819438 0.315s\n",
      "Generation: 726 20058.5823203507 184938.35660622586 0.7548234970138208 0.750280640491403 0.246s\n",
      "Generation: 727 20057.51528651835 184929.12614153963 0.7548407084215418 0.7502902023480095 0.242s\n",
      "Generation: 728 20052.34719825143 184884.8467729187 0.7550300339064732 0.7504068569986098 0.247s\n",
      "Generation: 729 20049.537447883966 184861.34626394504 0.7549783996833102 0.7504604033956065 0.296s\n",
      "Generation: 730 20047.898235450182 184845.5475444995 0.7551505137605204 0.7504986508220328 0.287s\n",
      "Generation: 731 20046.73315027259 184833.20979958045 0.7551505137605204 0.7504737899948557 0.241s\n",
      "Generation: 732 20045.72703165783 184827.13765398486 0.7550816681296363 0.7504871765941049 0.332s\n",
      "Generation: 733 20044.466895356527 184819.03709621125 0.7550644567219152 0.7504986508220328 0.305s\n",
      "Generation: 734 20038.55003507541 184779.84922948104 0.7552882050222888 0.7506114807299904 0.29s\n",
      "Generation: 735 20034.574162965928 184749.43300455326 0.7553054164300098 0.7505636714469575 0.283s\n",
      "Generation: 736 20033.151287928737 184736.771750203 0.7553570506531729 0.7505885322741346 0.256s\n",
      "Generation: 737 20031.880090957697 184726.14526028984 0.7552365707991257 0.750542635362423 0.244s\n",
      "Generation: 738 20027.54954358243 184696.05957174464 0.7552365707991257 0.7505713209322428 0.304s\n",
      "Generation: 739 20021.40160715852 184641.60022877995 0.7554258962840571 0.7506497281564167 0.353s\n",
      "Generation: 740 20021.01693020645 184640.9531203806 0.7554603190994992 0.7506650271269872 0.326s\n",
      "Generation: 741 20016.628055190733 184601.5968491227 0.7554431076917781 0.7506612023843445 0.31s\n",
      "Generation: 742 20012.72724574574 184566.44262919342 0.7555635875458254 0.7507797694062661 0.252s\n",
      "Generation: 743 20010.80557496936 184552.66117034733 0.7555807989535465 0.7508103673474071 0.291s\n",
      "Generation: 744 20009.884255842015 184545.03276096837 0.7555463761381044 0.7507816817775874 0.302s\n",
      "Generation: 745 20005.81152792133 184514.656731677 0.7555635875458254 0.7507682951783382 0.352s\n",
      "Generation: 746 20005.419632274694 184514.07853939617 0.7555980103612675 0.7507778570349447 0.296s\n",
      "Generation: 747 19999.94781184934 184472.95887153034 0.7555980103612675 0.75078550652023 0.295s\n",
      "Generation: 748 19996.559211844244 184442.0942037414 0.7557357016230357 0.7507778570349447 0.297s\n",
      "Generation: 749 19992.990032169953 184418.6299413188 0.7556840673998726 0.7508256663179776 0.265s\n",
      "Generation: 750 19992.780746891236 184417.70250557602 0.7557012788075936 0.7508141920900497 0.293s\n",
      "Generation: 751 19989.78452881947 184396.47297704828 0.7557701244384778 0.7507950683768366 0.304s\n",
      "Generation: 752 19986.153728291825 184367.32195060715 0.7557873358461988 0.7508964240568663 0.224s\n",
      "Generation: 753 19983.390336265795 184342.0773324178 0.755873392884804 0.7509365838546139 0.281s\n",
      "Generation: 754 19982.088012841232 184334.86247527922 0.7559078157002461 0.7509308467406499 0.265s\n",
      "Generation: 755 19980.804125690294 184323.86181293795 0.7557873358461988 0.7509480580825417 0.346s\n",
      "Generation: 756 19979.520512468433 184309.88144462585 0.755856181477083 0.7509805683950042 0.31s\n",
      "Generation: 757 19978.805685443633 184304.13733691516 0.7558045472539199 0.7509518828251844 0.318s\n",
      "Generation: 758 19968.328586322434 184238.25746465544 0.755838970069362 0.7508715632296892 0.309s\n",
      "Generation: 759 19966.871402068253 184232.03044462684 0.7558045472539199 0.7508792127149745 0.29s\n",
      "Generation: 760 19966.675573248915 184232.6967393949 0.7558217586616409 0.7508562642591187 0.353s\n",
      "Generation: 761 19966.44264384156 184232.97136359458 0.7558045472539199 0.7508620013730826 0.356s\n",
      "Generation: 762 19963.65188979203 184212.21953227528 0.7558045472539199 0.7507300477519119 0.288s\n",
      "Generation: 763 19961.862756186318 184200.16296011768 0.7556840673998726 0.750676501354915 0.291s\n",
      "Generation: 764 19953.95276713606 184152.95451229726 0.7557529130307568 0.7506439910424527 0.33s\n",
      "Generation: 765 19945.54115261389 184080.91150849243 0.7560110841465724 0.750821841575335 0.375s\n",
      "Generation: 766 19944.569893992637 184071.79204340675 0.7560971411851776 0.7508696508583679 0.39s\n",
      "Generation: 767 19944.100362362275 184067.5646212908 0.7560455069620144 0.7508524395164761 0.281s\n",
      "Generation: 768 19943.762041879796 184064.59696459348 0.7560110841465724 0.7508409652885482 0.321s\n",
      "Generation: 769 19941.910489603084 184047.91158603344 0.7561487754083407 0.7508543518877974 0.275s\n",
      "Generation: 770 19941.68712534888 184046.21372180202 0.7561143525928986 0.7508505271451548 0.315s\n",
      "Generation: 771 19941.454947087914 184043.86361402395 0.7561143525928986 0.7508314034319415 0.32s\n",
      "Generation: 772 19936.046510612734 184008.47409518313 0.7559766613311303 0.7509442333398991 0.41s\n",
      "Generation: 773 19930.78841373362 183971.06805446532 0.7560282955542934 0.7510532385052141 0.379s\n",
      "Generation: 774 19928.192187310877 183944.77409266043 0.7559938727388513 0.7510608879904993 0.318s\n",
      "Generation: 775 19927.308629397936 183936.77253804388 0.7560282955542934 0.7511412075859946 0.391s\n",
      "Generation: 776 19925.12050170301 183913.43680189503 0.7561487754083407 0.7511737178984569 0.288s\n",
      "Generation: 777 19922.297496921474 183892.58341818865 0.7561831982237828 0.7511890168690274 0.338s\n",
      "Generation: 778 19921.581011498227 183886.19515344896 0.7560627183697355 0.7512272642954537 0.4s\n",
      "Generation: 779 19913.31099141294 183819.13577806368 0.7562004096315038 0.7512808106924506 0.282s\n",
      "Generation: 780 19912.379174296686 183808.969589933 0.7562864666701089 0.7513152333762342 0.397s\n",
      "Generation: 781 19909.178674968884 183774.22920743714 0.756320889485551 0.7513114086335916 0.422s\n",
      "Generation: 782 19908.693404521946 183770.07901580635 0.7563897351164351 0.751307583890949 0.364s\n",
      "Generation: 783 19907.773664234166 183761.84950130628 0.756320889485551 0.7513152333762342 0.363s\n",
      "Generation: 784 19902.63850541587 183723.56560080082 0.7563725237087141 0.7513286199754834 0.374s\n",
      "Generation: 785 19901.89680383206 183716.91667431325 0.7562520438546668 0.7513190581188768 0.443s\n",
      "Generation: 786 19896.039822689734 183682.02166778382 0.7563725237087141 0.7513879034864441 0.333s\n",
      "Generation: 787 19890.78868589765 183630.2536206189 0.7564413693395983 0.7514338003981557 0.44s\n",
      "Generation: 788 19890.40205835057 183628.92163187073 0.7563725237087141 0.7514242385415492 0.394s\n",
      "Generation: 789 19886.43077693268 183592.59664546952 0.7564930035627614 0.7515255942215788 0.435s\n",
      "Generation: 790 19880.5292383104 183557.5161633423 0.7566306948245297 0.7514299756555131 0.327s\n",
      "Generation: 791 19879.93499013058 183552.30480538777 0.7565790606013666 0.7514701354532607 0.374s\n",
      "Generation: 792 19875.222499553667 183509.4916374303 0.756733963270856 0.7516537231001069 0.332s\n",
      "Generation: 793 19874.623105648465 183505.9954893388 0.7567855974940191 0.7516824086699266 0.23s\n",
      "Generation: 794 19873.211931053975 183502.7545389911 0.756733963270856 0.7516269499016085 0.392s\n",
      "Generation: 795 19873.0569467671 183501.54430734337 0.7566479062322508 0.7516326870155724 0.338s\n",
      "Generation: 796 19869.4836440783 183476.5629160093 0.7566995404554139 0.7517474292948513 0.378s\n",
      "Generation: 797 19866.163875400536 183445.16841958705 0.756768386086298 0.7517378674382448 0.329s\n",
      "Generation: 798 19865.674574919227 183440.6895514141 0.756733963270856 0.7517665530080645 0.462s\n",
      "Generation: 799 19861.825105473807 183414.10119308688 0.7568372317171822 0.7518220117763826 0.355s\n",
      "Generation: 800 19861.378515952554 183408.75802742975 0.7569577115712294 0.7518353983756318 0.269s\n",
      "Generation: 801 19857.52954646803 183387.96834545746 0.7570609800175556 0.7517742024933497 0.24s\n",
      "Generation: 802 19856.87947702348 183381.14377584946 0.7570437686098346 0.7517837643499563 0.206s\n",
      "Generation: 803 19855.317388584488 183368.54030151656 0.7570265572021135 0.7517569911514579 0.221s\n",
      "Generation: 804 19853.22303400474 183358.16882875893 0.7569060773480663 0.7518048004344907 0.257s\n",
      "Generation: 805 19851.453863231778 183350.4828140758 0.7570781914252767 0.7518315736329891 0.252s\n",
      "Generation: 806 19845.236659431805 183305.25874021815 0.7571814598716029 0.751915717971127 0.284s\n",
      "Generation: 807 19844.11163286272 183297.79452148982 0.7571470370561608 0.7519558777688746 0.264s\n",
      "Generation: 808 19842.909813264974 183287.26419226543 0.7571814598716029 0.751946315912268 0.264s\n",
      "Generation: 809 19841.590530440004 183276.29577631073 0.757215882687045 0.7519291045703762 0.258s\n",
      "Generation: 810 19834.20723761469 183223.84280169112 0.7576633792877919 0.751988388081337 0.222s\n",
      "Generation: 811 19828.893696304036 183184.8506443958 0.7576117450646288 0.7521605015002553 0.255s\n",
      "Generation: 812 19815.744270103394 183074.55746332955 0.757697802103234 0.7523268778052097 0.209s\n",
      "Generation: 813 19815.430530367914 183070.82258374547 0.757715013510955 0.7523230530625671 0.219s\n",
      "Generation: 814 19809.190227408286 183019.57009109785 0.7578010705495603 0.7524416200844886 0.191s\n",
      "Generation: 815 19809.135589762685 183019.2524933689 0.7578010705495603 0.7524416200844886 0.275s\n",
      "Generation: 816 19807.916857826072 183009.43712239526 0.7578871275881655 0.752468393282987 0.217s\n",
      "Generation: 817 19807.260721244707 183003.98890217018 0.7578699161804444 0.7524913417388428 0.264s\n",
      "Generation: 818 19806.840160853044 183004.512699252 0.7578871275881655 0.7524798675109149 0.238s\n",
      "Generation: 819 19806.46771305075 183002.66498370902 0.7579387618113286 0.7524550066837378 0.156s\n",
      "Generation: 820 19797.40477544682 182929.83072395713 0.758179721519423 0.7526041716468003 0.248s\n",
      "Generation: 821 19795.51228091866 182918.8140657161 0.7582657785580282 0.7526060840181217 0.311s\n",
      "Generation: 822 19784.592791946634 182827.66045120973 0.7583518355966334 0.7529235376574599 0.232s\n",
      "Generation: 823 19784.52628972644 182826.6193657405 0.7583346241889124 0.7529350118853878 0.232s\n",
      "Generation: 824 19779.373752060706 182787.76633408386 0.7584551040429597 0.7531300737601618 0.245s\n",
      "Generation: 825 19769.22258515063 182691.97799697096 0.7585411610815649 0.7533385222341852 0.37s\n",
      "Generation: 826 19767.03022907004 182673.7000061772 0.7585583724892859 0.7533633830613623 0.264s\n",
      "Generation: 827 19763.105485391046 182640.79005554304 0.7586272181201701 0.7534207542010017 0.278s\n",
      "Generation: 828 19762.489521890286 182636.2540954618 0.7586960637510543 0.7534035428591098 0.206s\n",
      "Generation: 829 19754.10079228127 182555.23797102147 0.7589714462745908 0.7536100789618119 0.346s\n",
      "Generation: 830 19753.269963075913 182548.83451522805 0.7589370234591487 0.7535813933919921 0.207s\n",
      "Generation: 831 19752.47169861442 182543.81989663164 0.7589542348668697 0.7535794810206708 0.191s\n",
      "Generation: 832 19745.520977148863 182476.19294128602 0.7591091375363591 0.753726733612412 0.302s\n",
      "Generation: 833 19745.071232401762 182475.38156495942 0.7591435603518012 0.7537802800094089 0.281s\n",
      "Generation: 834 19740.28103593289 182438.43001063066 0.7591951945749643 0.7538548624909401 0.224s\n",
      "Generation: 835 19740.017244007755 182437.92831920827 0.7591607717595222 0.753872073832832 0.233s\n",
      "Generation: 836 19738.08219769222 182424.53968576508 0.7593328858367326 0.7539676923988977 0.304s\n",
      "Generation: 837 19730.206449853238 182360.97250515549 0.7594533656907798 0.7542163006706686 0.288s\n",
      "Generation: 838 19725.057438546242 182316.34012078668 0.759539422729385 0.7542889707808785 0.25s\n",
      "Generation: 839 19709.058672067342 182191.73004849907 0.7598320166606427 0.7544343110012985 0.297s\n",
      "Generation: 840 19708.99249329405 182190.74410487065 0.7597459596220375 0.754455347085833 0.357s\n",
      "Generation: 841 19698.521045552894 182099.54824850243 0.7598664394760848 0.75462554813343 0.3s\n",
      "Generation: 842 19685.882877425047 182003.06560001182 0.7600729763687372 0.7546178986481447 0.202s\n",
      "Generation: 843 19677.751365959644 181931.9383166901 0.760417204523158 0.7547536770119581 0.255s\n",
      "Generation: 844 19671.566342353348 181882.65221959568 0.7605376843772053 0.7548492955780238 0.377s\n",
      "Generation: 845 19661.89335422814 181810.78167299688 0.7607958554930208 0.7548703316625582 0.385s\n",
      "Generation: 846 19656.29428981583 181773.11978024803 0.7607097984544156 0.7549410894014469 0.282s\n",
      "Generation: 847 19652.896252807728 181752.1720095783 0.7607958554930208 0.7550118471403355 0.275s\n",
      "Generation: 848 19647.450163322923 181724.98646887348 0.760916335347068 0.7551285017909357 0.245s\n",
      "Generation: 849 19639.596780455482 181668.17651089208 0.7610540266088364 0.7551514502467915 0.261s\n",
      "Generation: 850 19633.43334561908 181643.77907638153 0.7610196037933943 0.7552298574709654 0.281s\n",
      "Generation: 851 19624.605193404663 181577.67303231213 0.7610196037933943 0.755482290485379 0.321s\n",
      "Generation: 852 19610.371255629052 181465.30723582578 0.7616047916559096 0.7559297853745666 0.331s\n",
      "Generation: 853 19608.172221615885 181452.72509647952 0.7616736372867937 0.7559661204296716 0.282s\n",
      "Generation: 854 19602.484504906282 181413.3335000699 0.7616392144713516 0.7559833317715634 0.432s\n",
      "Generation: 855 19601.985678746863 181409.35069374915 0.7617080601022358 0.7559967183708126 0.299s\n",
      "Generation: 856 19597.28739243277 181372.3220335464 0.7618801741794461 0.7561420585912325 0.322s\n",
      "Generation: 857 19594.609578244144 181355.9473501984 0.7617080601022358 0.756199429730872 0.303s\n",
      "Generation: 858 19594.526226375812 181355.38346270472 0.7617080601022358 0.756205166844836 0.31s\n",
      "Generation: 859 19584.946116076702 181291.59653853724 0.7620350768489355 0.756339032837328 0.356s\n",
      "Generation: 860 19580.3488354015 181261.11943630953 0.7623104593724721 0.7564614246018921 0.292s\n",
      "Generation: 861 19576.6375863454 181237.5520365437 0.7623448821879142 0.7565015843996397 0.348s\n",
      "Generation: 862 19573.628891761626 181219.38161486416 0.7623793050033563 0.7564174400615019 0.361s\n",
      "Generation: 863 19564.680226006316 181146.6976170919 0.7626202647114507 0.7565226204841742 0.264s\n",
      "Generation: 864 19563.77269313507 181136.8547646232 0.7625858418960086 0.7565168833702103 0.26s\n",
      "Generation: 865 19559.228289379033 181119.6245707165 0.7625686304882876 0.7566105895649546 0.374s\n",
      "Generation: 866 19547.283005526613 181014.5967295162 0.763188241166245 0.7568324246382272 0.375s\n",
      "Generation: 867 19545.232128304913 180995.90672796173 0.7631710297585239 0.7568878834065453 0.251s\n",
      "Generation: 868 19536.459049856734 180929.36033743265 0.763205452573966 0.7570791205386768 0.436s\n",
      "Generation: 869 19531.41986545492 180899.1280813209 0.7632915096125712 0.7571078061084965 0.343s\n",
      "Generation: 870 19529.389295921555 180882.5175961893 0.7632570867971291 0.7571250174503883 0.354s\n",
      "Generation: 871 19519.457134547003 180805.72729535386 0.7639799659214127 0.757536177284471 0.429s\n",
      "Generation: 872 19519.07805293929 180804.36865488222 0.7639971773291337 0.7575495638837202 0.288s\n",
      "Generation: 873 19510.94739998521 180743.48976241396 0.7642553484449494 0.7578039092694551 0.243s\n",
      "Generation: 874 19505.411293747864 180701.41112641487 0.7645307309684859 0.7580984144529375 0.328s\n",
      "Generation: 875 19503.59776947391 180692.8948640944 0.7644963081530438 0.7581366618793638 0.334s\n",
      "Generation: 876 19503.484391437174 180694.29617447226 0.7645135195607649 0.7581251876514359 0.418s\n",
      "Generation: 877 19503.23535047042 180692.77019322524 0.764582365191649 0.7581596103352196 0.327s\n",
      "Generation: 878 19496.36141882825 180648.82031381462 0.7647544792688594 0.7583126000409247 0.375s\n",
      "Generation: 879 19492.76698236457 180621.50886713355 0.7647889020843015 0.7583087752982821 0.349s\n",
      "Generation: 880 19485.719847514112 180581.48182670493 0.7647716906765805 0.7583565845813149 0.346s\n",
      "Generation: 881 19481.669334901264 180559.1123451443 0.7649093819383488 0.7583393732394231 0.292s\n",
      "Generation: 882 19481.075549577414 180555.59722950784 0.7649438047537909 0.7583546722099936 0.271s\n",
      "Generation: 883 19478.67500169485 180541.48766114484 0.7649610161615118 0.7583565845813149 0.344s\n",
      "Generation: 884 19478.054603376626 180540.22087938493 0.7648921705306277 0.7583431979820657 0.245s\n",
      "Generation: 885 19477.839293810077 180542.86184676443 0.7649265933460698 0.7583087752982821 0.311s\n",
      "Generation: 886 19472.122411655157 180512.3354919659 0.7649093819383488 0.7583164247835674 0.263s\n",
      "Generation: 887 19468.15634349688 180483.34876301506 0.7649093819383488 0.7583718835518855 0.323s\n",
      "Generation: 888 19465.226749644524 180467.47783098757 0.7649093819383488 0.7583967443790626 0.329s\n",
      "Generation: 889 19456.759199229262 180392.7882176915 0.7651331302387222 0.7585879815111941 0.387s\n",
      "Generation: 890 19456.557354139524 180391.8540371876 0.7651503416464432 0.758630053680263 0.377s\n",
      "Generation: 891 19451.74304491691 180361.7536781013 0.7651503416464432 0.75865491450744 0.382s\n",
      "Generation: 892 19449.653627362513 180345.87061029318 0.7651847644618853 0.7587122856470795 0.405s\n",
      "Generation: 893 19445.128763650715 180323.92529280018 0.7652708215004905 0.7587543578161484 0.335s\n",
      "Generation: 894 19441.40326289004 180300.62735750634 0.7652708215004905 0.7587658320440763 0.376s\n",
      "Generation: 895 19434.98604300855 180252.40052314833 0.7655978382471902 0.7589551568048865 0.297s\n",
      "Generation: 896 19423.972698870253 180181.4986568641 0.7657871637321216 0.7592439248744051 0.419s\n",
      "Generation: 897 19422.888558230406 180170.730061589 0.7657699523244006 0.7592324506464772 0.307s\n",
      "Generation: 898 19416.276251030256 180114.87958707195 0.7658560093630058 0.7592649609589395 0.303s\n",
      "Generation: 899 19410.289740452652 180063.98505023416 0.7661141804788214 0.7593605795250052 0.234s\n",
      "Generation: 900 19407.910559895805 180050.988137461 0.7661313918865424 0.759341455811792 0.176s\n",
      "Generation: 901 19406.12348622793 180042.71424743976 0.7661486032942635 0.7593338063265068 0.166s\n",
      "Generation: 902 19404.095914440983 180027.48648474182 0.7660625462556583 0.7594122135506807 0.287s\n",
      "Generation: 903 19399.231055789867 180002.30182262516 0.765959277809332 0.7594103011793594 0.261s\n",
      "Generation: 904 19399.097155558306 180000.53475737182 0.765959277809332 0.7594160382933234 0.203s\n",
      "Generation: 905 19394.574295339713 179966.92633203667 0.7662690831483107 0.759517393973353 0.224s\n",
      "Generation: 906 19390.65911530578 179948.68781693064 0.7662862945560317 0.7594581104623923 0.252s\n",
      "Generation: 907 19383.392700621705 179889.89251083942 0.7663207173714738 0.7596359609952745 0.25s\n",
      "Generation: 908 19381.738835934248 179880.08089113949 0.7663723515946369 0.7596436104805598 0.226s\n",
      "Generation: 909 19378.130964460513 179853.29364391314 0.7665960998950104 0.7596856826496288 0.213s\n",
      "Generation: 910 19376.626618860355 179846.76540381133 0.7666133113027315 0.7597143682194485 0.241s\n",
      "Generation: 911 19373.358068523434 179826.5390547809 0.7668370596031049 0.7597124558481271 0.311s\n",
      "Generation: 912 19362.76196618669 179735.67075109913 0.7670263850880363 0.7600241723735014 0.267s\n",
      "Generation: 913 19355.41604816468 179681.77949212145 0.7672845562038519 0.7601044919689967 0.3s\n",
      "Generation: 914 19355.270989160188 179681.28059079096 0.7672673447961309 0.7601083167116393 0.321s\n",
      "Generation: 915 19349.32830780613 179638.63797371587 0.7671640763498047 0.7600605074286064 0.281s\n",
      "Generation: 916 19340.367596519085 179567.72612056675 0.7674910930965043 0.7602727806452724 0.289s\n",
      "Generation: 917 19338.03530289286 179553.1242135375 0.7673878246501782 0.7603091157003773 0.288s\n",
      "Generation: 918 19328.056267432712 179468.95948714393 0.7675599387273885 0.7605577239721483 0.316s\n",
      "Generation: 919 19319.995723486132 179401.2487882439 0.7677492642123199 0.7607585229608863 0.389s\n",
      "Generation: 920 19316.614798965737 179386.3785062593 0.7679041668818093 0.7607393992476731 0.234s\n",
      "Generation: 921 19307.393348485108 179314.62775086807 0.7679558011049724 0.7608981260673422 0.319s\n",
      "Generation: 922 19300.1001962928 179257.68330718385 0.7679385896972514 0.7610052188613359 0.274s\n",
      "Generation: 923 19299.993792975332 179256.9227674526 0.7678869554740883 0.7610052188613359 0.241s\n",
      "Generation: 924 19292.80851948887 179197.99945426924 0.7680590695512985 0.7611964559934673 0.394s\n",
      "Generation: 925 19287.87489970585 179163.1816010473 0.768265606443951 0.7611735075376116 0.285s\n",
      "Generation: 926 19281.466274025366 179109.3829980878 0.7682828178516721 0.7612653013610347 0.288s\n",
      "Generation: 927 19272.84819888628 179052.18692281193 0.7684205091134404 0.7614087292101333 0.326s\n",
      "Generation: 928 19271.48205367176 179043.13006447622 0.7684377205211614 0.761437414779953 0.348s\n",
      "Generation: 929 19269.71388072974 179034.68802336862 0.768231183628509 0.7613781312689922 0.244s\n",
      "Generation: 930 19268.983784367032 179031.21953690588 0.76824839503623 0.7613800436403135 0.265s\n",
      "Generation: 931 19268.26032788761 179025.8246986283 0.7683172406671142 0.7613915178682414 0.336s\n",
      "Generation: 932 19267.798179705245 179024.34365144235 0.7683172406671142 0.7613915178682414 0.283s\n",
      "Generation: 933 19266.406764530675 179016.29321712532 0.7683688748902773 0.761401079724848 0.293s\n",
      "Generation: 934 19266.106188943442 179010.21111813674 0.7684205091134404 0.7614221158093825 0.248s\n",
      "Generation: 935 19258.30898273963 178947.3405986222 0.7685582003752087 0.7615119972614842 0.385s\n",
      "Generation: 936 19236.189923148428 178762.28080484705 0.7684549319288825 0.7617070591362584 0.34s\n",
      "Generation: 937 19225.236604527872 178665.15970167692 0.7687819486755821 0.7619690540072784 0.347s\n",
      "Generation: 938 19220.36664332518 178633.3537630492 0.7688852171219084 0.7620111261763474 0.425s\n",
      "Generation: 939 19219.138547072904 178625.55759340315 0.7689712741605136 0.762020688032954 0.358s\n",
      "Generation: 940 19217.87959535747 178619.95538721452 0.7690401197913977 0.7620283375182393 0.211s\n",
      "Generation: 941 19217.61537314363 178619.11816534036 0.7689712741605136 0.7620417241174885 0.353s\n",
      "Generation: 942 19216.76908860647 178615.9770690144 0.7689368513450715 0.762057023088059 0.312s\n",
      "Generation: 943 19210.408446601912 178565.57592587124 0.7690401197913977 0.7621679406246952 0.318s\n",
      "Generation: 944 19206.52717422529 178543.820788869 0.769143388237724 0.7622329612496199 0.332s\n",
      "Generation: 945 19202.172925756535 178509.43994165096 0.769160599645445 0.7623285798156857 0.263s\n",
      "Generation: 946 19197.53041468125 178485.83153547783 0.7692122338686082 0.7623018066171873 0.325s\n",
      "Generation: 947 19193.91564900418 178460.2690597901 0.7693671365380974 0.7623534406428627 0.233s\n",
      "Generation: 948 19193.67786108734 178460.57880984995 0.7693155023149343 0.7623343169296496 0.243s\n",
      "Generation: 949 19189.34367377548 178440.48110976553 0.7695736734307499 0.7623630024994693 0.382s\n",
      "Generation: 950 19181.79013132287 178370.45596718363 0.7696425190616341 0.7624720076647843 0.308s\n",
      "Generation: 951 19177.837122024444 178339.20913484046 0.7697802103234024 0.7625121674625319 0.309s\n",
      "Generation: 952 19160.061145933076 178172.19668529258 0.7696769418770761 0.7626881056240928 0.351s\n",
      "Generation: 953 19158.24428138509 178155.82059549185 0.7698146331388445 0.762737827278447 0.29s\n",
      "Generation: 954 19153.063414667966 178113.89880453242 0.7699179015851707 0.7627760747048733 0.406s\n",
      "Generation: 955 19146.745141948006 178071.36167504906 0.7701244384778231 0.7628028479033717 0.348s\n",
      "Generation: 956 19146.49507008375 178070.61138653848 0.7701760727009862 0.7627971107894077 0.343s\n",
      "Generation: 957 19141.9371825425 178040.71814306258 0.7701244384778231 0.7628238839879061 0.344s\n",
      "Generation: 958 19141.832663571644 178039.08056302258 0.7701416498855441 0.7628200592452635 0.293s\n",
      "Generation: 959 19137.04609359452 178003.71342461248 0.7703998210013597 0.7629252396679358 0.239s\n",
      "Generation: 960 19135.76346047698 177996.16026933957 0.7703826095936387 0.7629290644105785 0.364s\n",
      "Generation: 961 19135.507312093756 177994.9555129062 0.7704686666322439 0.7628965540981161 0.38s\n",
      "Generation: 962 19131.455400658433 177966.85984243653 0.7705891464862911 0.7629539252377555 0.365s\n",
      "Generation: 963 19124.99415597732 177910.46713575767 0.7708128947866646 0.7630992654581755 0.334s\n",
      "Generation: 964 19122.927408035477 177896.3728799043 0.770967797456154 0.7631967963955625 0.367s\n",
      "Generation: 965 19116.19120890175 177842.95356302542 0.771002220271596 0.7632140077374544 0.371s\n",
      "Generation: 966 19114.397418750854 177833.8433032258 0.7710710659024802 0.7632426933072741 0.346s\n",
      "Generation: 967 19109.59936276684 177791.98237169246 0.7710710659024802 0.7633134510461628 0.383s\n",
      "Generation: 968 19106.253381313356 177761.13839563171 0.7713636598337378 0.7634014201269432 0.316s\n",
      "Generation: 969 19104.80680598233 177752.78333337896 0.7713980826491799 0.7634281933254417 0.385s\n",
      "Generation: 970 19101.669518669212 177733.96378029327 0.7716046195418323 0.7634779149797958 0.371s\n",
      "Generation: 971 19097.973629236574 177718.83625276043 0.7715874081341113 0.7634855644650811 0.361s\n",
      "Generation: 972 19097.796864764776 177717.94532304627 0.7716390423572744 0.763491301579045 0.355s\n",
      "Generation: 973 19096.823353014835 177715.04386201224 0.7717078879881586 0.7634511417812974 0.315s\n",
      "Generation: 974 19096.705880969017 177713.64655498153 0.7717423108036007 0.7634683531231892 0.288s\n",
      "Generation: 975 19096.226655205846 177712.6856143359 0.7717767336190428 0.7634702654945106 0.301s\n",
      "Generation: 976 19094.796319085894 177702.8560588255 0.7717939450267637 0.7635046881782942 0.338s\n",
      "Generation: 977 19089.045705262153 177651.258644619 0.7717595222113217 0.7636863634538191 0.292s\n",
      "Generation: 978 19086.256030197357 177629.4626239018 0.771931636288532 0.7636710644832486 0.286s\n",
      "Generation: 979 19085.662578016825 177626.42181972368 0.7719660591039741 0.7636691521119273 0.246s\n",
      "Generation: 980 19083.037599878604 177612.20637791464 0.7719832705116951 0.7637054871670322 0.428s\n",
      "Generation: 981 19079.40443536322 177584.20691239787 0.7720521161425793 0.7637322603655307 0.38s\n",
      "Generation: 982 19074.72508079838 177542.43791981274 0.7719832705116951 0.7638489150161308 0.318s\n",
      "Generation: 983 19070.642962586815 177505.52763962548 0.7721553845889055 0.7639560078101245 0.294s\n",
      "Generation: 984 19068.037411143574 177482.13761245774 0.7722070188120687 0.7639598325527671 0.282s\n",
      "Generation: 985 19057.30278539837 177388.17431526294 0.772344710073837 0.7640611882327968 0.313s\n",
      "Generation: 986 19054.28071496346 177365.86723907423 0.7723274986661159 0.7640898738026165 0.243s\n",
      "Generation: 987 19036.419522166172 177218.01678487935 0.7725340355587683 0.7643193583611743 0.241s\n",
      "Generation: 988 19033.676182226918 177200.59113256028 0.7725684583742104 0.7642868480487119 0.272s\n",
      "Generation: 989 19029.03537444428 177171.41853894774 0.7726889382282577 0.7643652552728858 0.389s\n",
      "Generation: 990 19028.304190115996 177164.31298897884 0.7726717268205366 0.7643939408427055 0.408s\n",
      "Generation: 991 19026.283378160817 177154.18907080338 0.7726717268205366 0.7643862913574203 0.446s\n",
      "Generation: 992 19024.631267861252 177146.46033289764 0.7727233610436998 0.764378641872135 0.381s\n",
      "Generation: 993 19023.439282158215 177142.8496970669 0.7727922066745839 0.7643748171294924 0.402s\n",
      "Generation: 994 19022.094626205915 177137.10476328808 0.772826629490026 0.7644054150706334 0.363s\n",
      "Generation: 995 19020.754270959467 177128.97220210696 0.7728782637131891 0.7644532243536662 0.374s\n",
      "Generation: 996 19017.65090097721 177110.41529052562 0.7728782637131891 0.7644532243536662 0.302s\n",
      "Generation: 997 19015.662903839904 177094.0981345323 0.7729126865286312 0.7644627862102729 0.343s\n",
      "Generation: 998 19010.765355949912 177054.4009707437 0.7729471093440733 0.764518244978591 0.324s\n",
      "Generation: 999 18997.426417982024 176930.49855099624 0.7729298979363523 0.7646291625152273 0.375s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1000 18994.057838307777 176906.66305303483 0.772826629490026 0.7646501985997617 0.246s\n",
      "Generation: 1001 18993.775583595234 176903.37841499952 0.7728610523054681 0.7646138635446568 0.147s\n",
      "Generation: 1002 18988.003521528186 176867.00702389993 0.7730503777903994 0.7647152192246864 0.237s\n",
      "Generation: 1003 18985.32575092688 176848.5972632607 0.7727922066745839 0.7646731470556175 0.21s\n",
      "Generation: 1004 18981.099635421608 176818.4211901225 0.7729987435672364 0.7647821522209325 0.221s\n",
      "Generation: 1005 18978.763183280233 176810.73941314616 0.7730503777903994 0.7647630285077193 0.235s\n",
      "Generation: 1006 18975.586986019713 176793.01088506667 0.7730331663826785 0.7647572913937554 0.213s\n",
      "Generation: 1007 18974.07542357179 176789.23432917998 0.7730503777903994 0.7647324305665782 0.247s\n",
      "Generation: 1008 18972.47458270195 176782.23077081857 0.7731880690521677 0.7647266934526143 0.243s\n",
      "Generation: 1009 18968.368193855793 176762.32176713255 0.7733429717216571 0.7647859769635751 0.245s\n",
      "Generation: 1010 18964.510123634962 176744.5934363462 0.7731364348290046 0.7647955388201816 0.241s\n",
      "Generation: 1011 18956.32252317489 176674.77329274177 0.7732569146830519 0.7649886883236344 0.255s\n",
      "Generation: 1012 18950.34841590222 176648.09303870564 0.7733257603139361 0.7650058996655262 0.259s\n",
      "Generation: 1013 18947.872007208953 176635.54112616114 0.7730331663826785 0.7650078120368475 0.305s\n",
      "Generation: 1014 18943.193376878502 176607.1784300026 0.7733257603139361 0.7651053429742346 0.296s\n",
      "Generation: 1015 18941.399044336067 176595.40783030412 0.7732224918676098 0.7650747450330936 0.314s\n",
      "Generation: 1016 18937.745735472447 176572.41547683347 0.7734290287602623 0.765065183176487 0.215s\n",
      "Generation: 1017 18936.217140047072 176561.74928346806 0.7733773945370992 0.7650728326617723 0.277s\n",
      "Generation: 1018 18928.807147289026 176514.86607376114 0.7737044112837989 0.765095781117628 0.25s\n",
      "Generation: 1019 18927.63916223105 176506.85806612376 0.7735667200220306 0.7651837501984086 0.335s\n",
      "Generation: 1020 18925.491738738994 176495.1145942856 0.7734290287602623 0.7651569769999101 0.366s\n",
      "Generation: 1021 18923.66132900539 176486.963320485 0.7735495086143096 0.7651129924595199 0.448s\n",
      "Generation: 1022 18922.080113271175 176481.07832500304 0.7734118173525413 0.7650843068897002 0.383s\n",
      "Generation: 1023 18917.837526473588 176446.9735555542 0.7736699884683568 0.7651416780293396 0.311s\n",
      "Generation: 1024 18914.897817917823 176429.61373034745 0.7736183542451938 0.7651359409153756 0.319s\n",
      "Generation: 1025 18911.917841399398 176409.84145036797 0.7736871998760778 0.7651665388565166 0.374s\n",
      "Generation: 1026 18909.513159531045 176394.32723847707 0.7735667200220306 0.7651761007131233 0.267s\n",
      "Generation: 1027 18908.667865827127 176388.6355757533 0.7736011428374727 0.7652009615403004 0.32s\n",
      "Generation: 1028 18907.829129784855 176387.13903391827 0.7735150857988675 0.7652086110255856 0.267s\n",
      "Generation: 1029 18905.70402129135 176375.0679050256 0.7735322972065886 0.7652334718527627 0.317s\n",
      "Generation: 1030 18898.975797169336 176317.03377340524 0.7735667200220306 0.7654132347569663 0.341s\n",
      "Generation: 1031 18897.077365907793 176305.53858239113 0.7734978743911465 0.7654438326981073 0.343s\n",
      "Generation: 1032 18896.08300337397 176297.97856244806 0.7734806629834254 0.7654457450694286 0.422s\n",
      "Generation: 1033 18894.561509195795 176289.2595617179 0.7736011428374727 0.7654151471282876 0.471s\n",
      "Generation: 1034 18893.363215364687 176282.92112694954 0.7734462401679834 0.7653921986724318 0.489s\n",
      "Generation: 1035 18891.05887591745 176270.46424786485 0.7735667200220306 0.765344389389399 0.511s\n",
      "Generation: 1036 18888.096381643627 176250.78703653117 0.7736355656529148 0.7653118790769367 0.329s\n",
      "Generation: 1037 18885.39617025777 176237.0948204544 0.7734978743911465 0.7653424770180777 0.448s\n",
      "Generation: 1038 18878.587838929485 176182.64905214278 0.7735322972065886 0.765441920326786 0.393s\n",
      "Generation: 1039 18877.713684729053 176176.26920391736 0.7734462401679834 0.7654610440399992 0.501s\n",
      "Generation: 1040 18875.79903484034 176165.63383071157 0.7734462401679834 0.7654992914664255 0.427s\n",
      "Generation: 1041 18874.603697560295 176156.61848591903 0.7734290287602623 0.7654859048671763 0.439s\n",
      "Generation: 1042 18872.406301456904 176146.18250073254 0.7733946059448202 0.7654572192973566 0.399s\n",
      "Generation: 1043 18871.589324632336 176142.57598685691 0.7733429717216571 0.7654820801245337 0.474s\n",
      "Generation: 1044 18868.655342302314 176125.01006343166 0.7732052804598888 0.765411322385645 0.52s\n",
      "Generation: 1045 18866.91906718649 176117.48884679846 0.7730503777903994 0.7653577759886482 0.423s\n",
      "Generation: 1046 18864.49293721096 176102.0851436905 0.7730675891981205 0.7654074976430023 0.475s\n",
      "Generation: 1047 18861.470300206634 176076.4258125233 0.7732224918676098 0.7655050285803894 0.41s\n",
      "Generation: 1048 18858.44401548768 176051.8304893722 0.7734978743911465 0.7655184151796386 0.355s\n",
      "Generation: 1049 18858.013996281312 176050.11826215204 0.7734978743911465 0.7655107656943534 0.474s\n",
      "Generation: 1050 18856.10262540111 176040.92913129396 0.7735150857988675 0.7654859048671763 0.396s\n",
      "Generation: 1051 18855.616637434818 176039.24244247202 0.7734290287602623 0.7654476574407499 0.475s\n",
      "Generation: 1052 18854.6244219144 176035.00337446277 0.7734462401679834 0.7654208842422515 0.475s\n",
      "Generation: 1053 18853.51885089643 176031.85456729695 0.7735150857988675 0.7654457450694286 0.42s\n",
      "Generation: 1054 18852.62538671492 176029.90572757347 0.7737904683224041 0.7654859048671763 0.439s\n",
      "Generation: 1055 18850.18302152607 176022.92902802987 0.773773256914683 0.7654151471282876 0.252s\n",
      "Generation: 1056 18848.453660372543 176012.56330351523 0.7739797938073355 0.7654094100143236 0.271s\n",
      "Generation: 1057 18847.251206512436 176009.52431637232 0.7739109481764513 0.7654266213562155 0.451s\n",
      "Generation: 1058 18845.413987821477 175999.6836041866 0.7738765253610093 0.7654476574407499 0.471s\n",
      "Generation: 1059 18843.791289214907 175990.49982723745 0.7738421025455672 0.765466781153963 0.464s\n",
      "Generation: 1060 18843.24905427146 175990.41264746955 0.7738248911378461 0.7654686935252845 0.467s\n",
      "Generation: 1061 18841.141450761188 175972.83589266505 0.7738937367687303 0.7654859048671763 0.502s\n",
      "Generation: 1062 18839.58237392084 175964.8380122144 0.7739797938073355 0.7655126780656747 0.435s\n",
      "Generation: 1063 18838.215675167925 175958.3686047663 0.7739109481764513 0.7655471007494583 0.465s\n",
      "Generation: 1064 18836.056417343523 175940.10028936787 0.7737904683224041 0.7655413636354944 0.423s\n",
      "Generation: 1065 18834.27060779412 175926.28248656972 0.7739109481764513 0.7655528378634223 0.465s\n",
      "Generation: 1066 18832.643828828626 175918.25101618987 0.7739625823996145 0.7655738739479567 0.456s\n",
      "Generation: 1067 18831.330539145925 175907.3167661362 0.7741174850691038 0.7656293327162749 0.531s\n",
      "Generation: 1068 18830.25683062162 175898.37724015958 0.774169119292267 0.7656828791132717 0.43s\n",
      "Generation: 1069 18828.802162715736 175892.8858633178 0.7742207535154301 0.7656771419993077 0.426s\n",
      "Generation: 1070 18827.846308561777 175884.9272027117 0.7742379649231511 0.7656943533411995 0.447s\n",
      "Generation: 1071 18827.2158002753 175882.8717875813 0.774203542107709 0.7656981780838422 0.383s\n",
      "Generation: 1072 18825.960216498042 175878.61393956048 0.7743240219617562 0.7656522811721306 0.352s\n",
      "Generation: 1073 18818.655256768623 175828.36592611865 0.7742207535154301 0.7657345131389471 0.36s\n",
      "Generation: 1074 18818.28393600294 175826.47283405458 0.7742379649231511 0.7657593739661243 0.41s\n",
      "Generation: 1075 18817.302516706393 175818.81666013083 0.7741346964768249 0.7657230389110192 0.561s\n",
      "Generation: 1076 18816.6750086719 175814.14165972575 0.7740486394382197 0.765745987366875 0.44s\n",
      "Generation: 1077 18814.4992449243 175797.5056010114 0.7742379649231511 0.7657345131389471 0.453s\n",
      "Generation: 1078 18814.017666397292 175793.8690144185 0.7742895991463141 0.7657230389110192 0.508s\n",
      "Generation: 1079 18813.510465720843 175788.64232043078 0.7743240219617562 0.7657058275691274 0.533s\n",
      "Generation: 1080 18812.261565831286 175783.97394468202 0.7743928675926404 0.7657326007676258 0.497s\n",
      "Generation: 1081 18811.064488299002 175778.366796624 0.7743068105540352 0.7657689358227309 0.618s\n",
      "Generation: 1082 18810.37782610638 175772.4357821547 0.7742379649231511 0.7657555492234817 0.441s\n",
      "Generation: 1083 18809.86823504393 175768.03827236954 0.7743412333694772 0.7657020028264848 0.47s\n",
      "Generation: 1084 18806.954029721506 175745.17898447055 0.7744100790003614 0.7657995337638719 0.482s\n",
      "Generation: 1085 18806.55951343926 175742.3466986421 0.7745305588544087 0.7657689358227309 0.385s\n",
      "Generation: 1086 18805.208328295932 175735.45594120835 0.7744100790003614 0.7658224822197276 0.498s\n",
      "Generation: 1087 18804.86724437503 175732.96613915992 0.7744961360389666 0.7657957090212293 0.533s\n",
      "Generation: 1088 18804.550767995173 175730.65540165716 0.7745821930775718 0.7657631987087669 0.385s\n",
      "Generation: 1089 18804.27819461896 175728.6480056661 0.7746338273007349 0.7657861471646227 0.476s\n",
      "Generation: 1090 18804.0653658996 175727.2261545758 0.7747026729316191 0.7657957090212293 0.486s\n",
      "Generation: 1091 18803.919337672873 175726.38677098797 0.7745994044852929 0.7657918842785866 0.439s\n",
      "Generation: 1092 18803.675707559152 175724.85502853736 0.7745649816698508 0.7657957090212293 0.482s\n",
      "Generation: 1093 18802.89030759449 175719.39558685376 0.7746166158930139 0.7658205698484063 0.535s\n",
      "Generation: 1094 18798.11517090141 175679.481569333 0.7747026729316191 0.765897064701259 0.561s\n",
      "Generation: 1095 18796.713961122266 175672.6399423931 0.7748747870088295 0.765921925528436 0.476s\n",
      "Generation: 1096 18796.451450230183 175671.59734914816 0.7749436326397136 0.7658951523299375 0.477s\n",
      "Generation: 1097 18796.340306340568 175671.08453912515 0.7748403641933874 0.7659123636718295 0.425s\n",
      "Generation: 1098 18793.97186365815 175659.33751215742 0.7749608440474346 0.7659123636718295 0.526s\n",
      "Generation: 1099 18792.63756976884 175652.80617459695 0.7749264212319925 0.7659525234695771 0.523s\n",
      "Generation: 1100 18790.971116562447 175636.80221695354 0.7750296896783188 0.7659467863556131 0.227s\n",
      "Generation: 1101 18788.977487598597 175620.9706457796 0.7749608440474346 0.7659391368703279 0.237s\n",
      "Generation: 1102 18787.34299681519 175608.13740024052 0.7748403641933874 0.7659945956386459 0.24s\n",
      "Generation: 1103 18782.17767160203 175558.7607673623 0.7750124782705977 0.7661514100869937 0.28s\n",
      "Generation: 1104 18780.44804922203 175546.8428401537 0.7749780554551556 0.7661781832854921 0.277s\n",
      "Generation: 1105 18774.63629805257 175496.50598475753 0.7750469010860398 0.766298662678735 0.301s\n",
      "Generation: 1106 18770.73281055961 175459.87556159383 0.7751673809400871 0.7664134049580139 0.257s\n",
      "Generation: 1107 18767.60321218318 175429.9051225743 0.7750813239014819 0.7665472709505059 0.261s\n",
      "Generation: 1108 18762.46687918712 175387.6180781878 0.775132958124645 0.7665816936342896 0.318s\n",
      "Generation: 1109 18760.339553507853 175371.74376980777 0.7752362265709712 0.7665969926048601 0.237s\n",
      "Generation: 1110 18759.143537527718 175364.3555943864 0.7751673809400871 0.7666161163180732 0.327s\n",
      "Generation: 1111 18757.69401823444 175350.979413291 0.7750985353092029 0.7665491833218272 0.169s\n",
      "Generation: 1112 18755.107854737456 175326.84597433452 0.7750985353092029 0.7666371524026077 0.305s\n",
      "Generation: 1113 18747.27920687403 175254.45880370462 0.7753394950172975 0.7667404204539587 0.267s\n",
      "Generation: 1114 18746.222596154348 175248.01346316864 0.7753739178327396 0.76674233282528 0.327s\n",
      "Generation: 1115 18741.868225950628 175221.27805144512 0.7751673809400871 0.7668666369611655 0.319s\n",
      "Generation: 1116 18737.852634168983 175186.89452957342 0.7753911292404606 0.7669010596449491 0.201s\n",
      "Generation: 1117 18734.80952107028 175163.90838161152 0.7755804547253919 0.7670311008947985 0.22s\n",
      "Generation: 1118 18731.56906154673 175136.415806552 0.775614877540834 0.7670942091484019 0.347s\n",
      "Generation: 1119 18730.45302313139 175129.5750238238 0.7755288205022289 0.7670827349204741 0.348s\n",
      "Generation: 1120 18725.057650703184 175092.18371257506 0.7757181459871603 0.7671592297733266 0.293s\n",
      "Generation: 1121 18724.645920393476 175088.46305477218 0.7757697802103234 0.7671458431740774 0.308s\n",
      "Generation: 1122 18721.808615679034 175064.76062406055 0.7756837231717182 0.7672032143137169 0.247s\n",
      "Generation: 1123 18716.84673962282 175023.1031336325 0.7757697802103234 0.7672261627695727 0.289s\n",
      "Generation: 1124 18715.15327109988 175010.82498863508 0.7759246828798128 0.76722233802693 0.215s\n",
      "Generation: 1125 18709.03462044799 174977.86015852197 0.7759074714720917 0.7673772401039565 0.285s\n",
      "Generation: 1126 18706.781408493698 174959.35560322375 0.7758902600643707 0.7674078380450975 0.258s\n",
      "Generation: 1127 18704.380416918404 174944.3015346065 0.7758386258412076 0.7674116627877402 0.335s\n",
      "Generation: 1128 18703.517825424915 174939.27919549344 0.7758902600643707 0.7674231370156681 0.359s\n",
      "Generation: 1129 18699.261686732363 174915.49922725448 0.7760967969570232 0.7674862452692714 0.316s\n",
      "Generation: 1130 18698.58596427306 174911.4294416466 0.7760623741415811 0.7674881576405927 0.419s\n",
      "Generation: 1131 18694.70974898202 174876.29332390285 0.7760967969570232 0.767490070011914 0.396s\n",
      "Generation: 1132 18691.248649003388 174858.40968196347 0.7760795855493021 0.7675895133206224 0.387s\n",
      "Generation: 1133 18687.999955128304 174837.64428783624 0.7763377566651176 0.7676621834308324 0.495s\n",
      "Generation: 1134 18684.916299537934 174814.91279029055 0.7761140083647442 0.7676449720889406 0.404s\n",
      "Generation: 1135 18681.57496714887 174793.50283670434 0.7762000654033493 0.7677042555999013 0.372s\n",
      "Generation: 1136 18679.158160764233 174773.85976457503 0.7762861224419545 0.7677578019968981 0.308s\n",
      "Generation: 1137 18678.51942373518 174770.36096889077 0.7762516996265124 0.7677731009674686 0.302s\n",
      "Generation: 1138 18675.8677286129 174741.5124191399 0.7763377566651176 0.7677826628240753 0.317s\n",
      "Generation: 1139 18675.298174473603 174738.32648026163 0.7764066022960018 0.7677845751953966 0.347s\n",
      "Generation: 1140 18674.791014116912 174735.8262296638 0.7764066022960018 0.767763539110862 0.346s\n",
      "Generation: 1141 18671.820077509805 174707.25283154237 0.7763205452573966 0.7677425030263276 0.328s\n",
      "Generation: 1142 18663.308227095466 174660.5970838057 0.7766647734118174 0.7679050545886393 0.35s\n",
      "Generation: 1143 18662.842246529217 174655.00585895983 0.7765615049654911 0.7679394772724231 0.34s\n",
      "Generation: 1144 18661.53223479908 174636.94179419882 0.7765959277809332 0.7679471267577083 0.442s\n",
      "Generation: 1145 18656.932980206748 174589.85880102517 0.7766991962272595 0.7680370082098101 0.365s\n",
      "Generation: 1146 18649.71881751996 174549.23739853324 0.7770778471971223 0.7680809927502004 0.374s\n",
      "Generation: 1147 18647.909047896013 174543.39165230404 0.7770778471971223 0.7680867298641643 0.385s\n",
      "Generation: 1148 18645.962605925317 174529.94020781302 0.7771639042357275 0.7680962917207709 0.453s\n",
      "Generation: 1149 18645.497333191248 174527.25517295924 0.7771122700125643 0.768084817492843 0.367s\n",
      "Generation: 1150 18641.500007109866 174505.3095843921 0.7770262129739591 0.7680943793494496 0.311s\n",
      "Generation: 1151 18638.630491852593 174486.98386324698 0.7773188069052168 0.7681307144045545 0.357s\n",
      "Generation: 1152 18636.68518962108 174467.44890842872 0.7773360183129379 0.7681421886324824 0.315s\n",
      "Generation: 1153 18635.232720254204 174457.41461950907 0.7773360183129379 0.7681115906913414 0.385s\n",
      "Generation: 1154 18632.373151934346 174443.91377713825 0.7771294814202854 0.7682301577132629 0.383s\n",
      "Generation: 1155 18631.976089072286 174441.81927646368 0.7771294814202854 0.7682397195698695 0.404s\n",
      "Generation: 1156 18629.68326811461 174432.22377427123 0.7770950586048433 0.7682511937977974 0.263s\n",
      "Generation: 1157 18628.47775925637 174417.88780007703 0.7770090015662381 0.7682626680257252 0.282s\n",
      "Generation: 1158 18621.424334736053 174367.06382330498 0.7773704411283799 0.7683601989631123 0.414s\n",
      "Generation: 1159 18620.11891018943 174356.8340888713 0.7774048639438219 0.7682990030808302 0.335s\n",
      "Generation: 1160 18619.843987324548 174356.01611134651 0.777422075351543 0.7683009154521515 0.488s\n",
      "Generation: 1161 18618.25823237003 174347.73148527057 0.7772671726820537 0.7683716731910402 0.411s\n",
      "Generation: 1162 18609.676862258384 174302.63680609444 0.777439286759264 0.7684921525842829 0.413s\n",
      "Generation: 1163 18601.785801257218 174253.779173327 0.7775769780210323 0.7685399618673159 0.356s\n",
      "Generation: 1164 18593.96026231036 174200.37956945616 0.7776286122441954 0.768668090745844 0.271s\n",
      "Generation: 1165 18587.645438642397 174155.64029761314 0.7779384175831742 0.7688363794221197 0.347s\n",
      "Generation: 1166 18584.05710464444 174126.36762108683 0.7779556289908952 0.7688765392198672 0.402s\n",
      "Generation: 1167 18579.626571838413 174094.0430556089 0.7779900518063373 0.7689549464440412 0.301s\n",
      "Generation: 1168 18576.376741042448 174058.40260340102 0.7781621658835476 0.7688975753044017 0.492s\n",
      "Generation: 1169 18573.519260611494 174034.45491705267 0.7782654343298738 0.7689549464440412 0.444s\n",
      "Generation: 1170 18565.09908149368 173971.11728936346 0.7782310115144317 0.7690601268667134 0.478s\n",
      "Generation: 1171 18561.216183969365 173938.96240461714 0.7784203369993632 0.7691557454327792 0.646s\n",
      "Generation: 1172 18560.045876084485 173931.07565708403 0.7784031255916422 0.7691461835761726 0.413s\n",
      "Generation: 1173 18555.658521190628 173905.39250618988 0.7782654343298738 0.7692628382267728 0.393s\n",
      "Generation: 1174 18554.8546137565 173900.53417260174 0.778317068553037 0.7692781371973433 0.451s\n",
      "Generation: 1175 18548.34704305834 173868.74326751288 0.7784203369993632 0.7693565444215172 0.418s\n",
      "Generation: 1176 18542.626048998296 173831.6698152193 0.7786096624842946 0.7694789361860814 0.421s\n",
      "Generation: 1177 18542.33807698283 173830.73943941903 0.7787129309306208 0.7694789361860814 0.48s\n",
      "Generation: 1178 18542.006384426746 173828.93967006938 0.7787473537460629 0.7694636372155108 0.472s\n",
      "Generation: 1179 18540.536571563192 173816.87506375258 0.7786268738920157 0.7694598124728682 0.477s\n",
      "Generation: 1180 18530.849902261245 173749.070188164 0.7787645651537839 0.7695822042374324 0.512s\n",
      "Generation: 1181 18530.48688293735 173749.6041961502 0.7787129309306208 0.7695382196970422 0.414s\n",
      "Generation: 1182 18529.035855839564 173743.4675393288 0.7786785081151787 0.769555431038934 0.335s\n",
      "Generation: 1183 18528.587404699694 173740.4906233287 0.7786268738920157 0.769586028980075 0.333s\n",
      "Generation: 1184 18525.920188526976 173723.00429947095 0.7788334107846681 0.7695936784653603 0.426s\n",
      "Generation: 1185 18523.374195373795 173697.8495251875 0.7788506221923891 0.7695516062962914 0.411s\n",
      "Generation: 1186 18519.31306140234 173673.13852023784 0.7787645651537839 0.7695076217559011 0.382s\n",
      "Generation: 1187 18518.153688816663 173665.71941087084 0.7787817765615049 0.7695229207264717 0.403s\n",
      "Generation: 1188 18517.381487023147 173658.4687189559 0.7787129309306208 0.7695324825830782 0.336s\n",
      "Generation: 1189 18515.52845750282 173643.85071213834 0.7788850450078312 0.7694540753589043 0.354s\n",
      "Generation: 1190 18513.17319980046 173625.27619553992 0.7789711020464364 0.7694961475279732 0.461s\n",
      "Generation: 1191 18511.308207725535 173612.84037066228 0.7789883134541574 0.769524833097793 0.49s\n",
      "Generation: 1192 18503.294745141808 173549.67648252903 0.7790743704927626 0.7696491372336783 0.422s\n",
      "Generation: 1193 18502.900762612746 173546.54860356354 0.7790743704927626 0.7696491372336783 0.481s\n",
      "Generation: 1194 18502.020012135854 173541.4590204561 0.7791432161236468 0.7696510496049997 0.389s\n",
      "Generation: 1195 18500.580750961315 173530.6484377975 0.779298118793136 0.7696491372336783 0.41s\n",
      "Generation: 1196 18500.04292664888 173525.56628683396 0.7793497530162992 0.769647224862357 0.379s\n",
      "Generation: 1197 18499.89272535338 173524.3265877905 0.7791948503468099 0.7696453124910357 0.455s\n",
      "Generation: 1198 18499.160616407622 173520.31692254104 0.7793497530162992 0.7696338382631078 0.361s\n",
      "Generation: 1199 18493.91845222091 173474.03628592385 0.7791087933082047 0.7697179826012457 0.395s\n",
      "Generation: 1200 18493.072849065073 173473.48515180874 0.7789538906387153 0.7697103331159605 0.22s\n",
      "Generation: 1201 18492.659180930317 173471.0513747488 0.7789883134541574 0.7697237197152097 0.239s\n",
      "Generation: 1202 18490.913343663822 173466.77249011357 0.7791604275313678 0.7697906527114556 0.28s\n",
      "Generation: 1203 18489.645930199953 173457.7477752278 0.7791948503468099 0.7698193382812754 0.233s\n",
      "Generation: 1204 18485.737228356236 173441.64174734778 0.7791260047159257 0.7698384619944886 0.311s\n",
      "Generation: 1205 18484.36036387076 173438.90089783238 0.7793841758317412 0.7699149568473411 0.232s\n",
      "Generation: 1206 18481.82289473543 173415.19456027081 0.7795046556857885 0.7699818898435872 0.183s\n",
      "Generation: 1207 18480.256927036437 173397.27657125753 0.7794702328703464 0.7699857145862298 0.196s\n",
      "Generation: 1208 18475.30227265719 173370.54727430217 0.7794530214626254 0.769968503244338 0.234s\n",
      "Generation: 1209 18470.840201256335 173348.80573045748 0.779229273162252 0.7699704156156593 0.17s\n",
      "Generation: 1210 18470.768247132797 173348.01454365268 0.779280907385415 0.7699952764428364 0.238s\n",
      "Generation: 1211 18464.754770657524 173311.38659890727 0.779246484569973 0.7700163125273708 0.326s\n",
      "Generation: 1212 18462.017624497134 173292.75003918025 0.7794013872394623 0.77002969912662 0.279s\n",
      "Generation: 1213 18461.129952166946 173287.97409584987 0.779298118793136 0.7700220496413348 0.321s\n",
      "Generation: 1214 18460.595525872817 173284.5282310601 0.7793153302008571 0.7700144001560495 0.23s\n",
      "Generation: 1215 18457.895187329173 173269.87986579738 0.7793325416085781 0.7700698589243676 0.3s\n",
      "Generation: 1216 18455.646401767215 173255.5920425095 0.779298118793136 0.7700449980971905 0.259s\n",
      "Generation: 1217 18455.222081111588 173252.94289590573 0.779298118793136 0.7700583846964397 0.27s\n",
      "Generation: 1218 18452.905324977284 173238.47718174115 0.7793669644240202 0.7700775084096528 0.235s\n",
      "Generation: 1219 18451.58109868347 173226.29134122047 0.7795562899089516 0.7701520908911842 0.246s\n",
      "Generation: 1220 18450.632088338574 173221.8284379621 0.7796251355398358 0.7702094620308236 0.256s\n",
      "Generation: 1221 18448.25864852344 173210.30997342913 0.7795390785012306 0.7702113744021449 0.322s\n",
      "Generation: 1222 18442.588321337582 173182.13444206218 0.779298118793136 0.7702438847146073 0.308s\n",
      "Generation: 1223 18437.27122299678 173140.06336392488 0.7792636959776941 0.770303168225568 0.197s\n",
      "Generation: 1224 18433.816043500363 173124.95112274337 0.779229273162252 0.7703127300821746 0.268s\n",
      "Generation: 1225 18432.902664129928 173116.38524817344 0.779298118793136 0.770308905339532 0.333s\n",
      "Generation: 1226 18427.232041103813 173087.92945626832 0.7795046556857885 0.770333766166709 0.291s\n",
      "Generation: 1227 18424.02286886648 173061.5583925228 0.7796079241321148 0.7703834878210633 0.262s\n",
      "Generation: 1228 18421.082914357627 173049.0746583392 0.7795735013166727 0.7704064362769191 0.275s\n",
      "Generation: 1229 18416.095056911712 173022.0880943748 0.7795562899089516 0.7704159981335256 0.32s\n",
      "Generation: 1230 18412.19228281252 172990.33071474437 0.7796251355398358 0.7705192661848765 0.282s\n",
      "Generation: 1231 18405.82743207274 172936.3644079308 0.7796079241321148 0.77058237443848 0.29s\n",
      "Generation: 1232 18405.42918097193 172933.7649071406 0.7796251355398358 0.7705976734090505 0.328s\n",
      "Generation: 1233 18400.98404540432 172897.21610399504 0.779728403986162 0.7707544878573983 0.344s\n",
      "Generation: 1234 18398.03892035965 172876.9999420042 0.7798488838402093 0.7707812610558967 0.359s\n",
      "Generation: 1235 18397.040670583385 172867.21069604054 0.7798488838402093 0.7707430136294704 0.293s\n",
      "Generation: 1236 18395.39913874265 172860.1549024187 0.7798488838402093 0.7707640497140049 0.235s\n",
      "Generation: 1237 18391.185074370056 172820.86780013161 0.7799005180633723 0.770886441478569 0.237s\n",
      "Generation: 1238 18387.846944510973 172790.29582217094 0.7799177294710934 0.7709361631329232 0.327s\n",
      "Generation: 1239 18381.75789046433 172731.1017828814 0.7802619576255142 0.7710719414967365 0.318s\n",
      "Generation: 1240 18376.468795655222 172688.64368816628 0.7803996488872825 0.7712230188311204 0.3s\n",
      "Generation: 1241 18376.233153055782 172687.63740406634 0.7804168602950036 0.771190508518658 0.377s\n",
      "Generation: 1242 18373.510551998024 172670.97089557358 0.7802447462177932 0.7711828590333728 0.247s\n",
      "Generation: 1243 18373.32473747106 172670.15907039098 0.7802791690332352 0.7711752095480875 0.347s\n",
      "Generation: 1244 18368.250928648133 172634.56317112566 0.7802791690332352 0.7711752095480875 0.328s\n",
      "Generation: 1245 18368.13086602464 172634.13243046202 0.7802963804409563 0.7711828590333728 0.204s\n",
      "Generation: 1246 18368.034473986707 172633.9694450369 0.7803135918486773 0.7711771219194088 0.392s\n",
      "Generation: 1247 18360.5533133766 172586.64752586142 0.7806061857799349 0.7714601528749634 0.36s\n",
      "Generation: 1248 18357.645227378147 172558.03754510242 0.780640608595377 0.7714926631874258 0.329s\n",
      "Generation: 1249 18357.148873758397 172555.48748492208 0.7805717629644928 0.7714257301911798 0.271s\n",
      "Generation: 1250 18349.339022561082 172498.1477467414 0.7809332025266347 0.7715748951542423 0.316s\n",
      "Generation: 1251 18341.689880337406 172439.2890411325 0.7810192595652399 0.771681987948236 0.436s\n",
      "Generation: 1252 18340.873211459693 172437.9775759347 0.781088105196124 0.7716877250621998 0.291s\n",
      "Generation: 1253 18338.740447611508 172418.49813611823 0.781139739419287 0.7717603951724098 0.331s\n",
      "Generation: 1254 18334.367612362097 172384.57544986936 0.7811913736424502 0.771810116826764 0.334s\n",
      "Generation: 1255 18332.62326194809 172368.6490948974 0.7813806991273816 0.7718445395105477 0.455s\n",
      "Generation: 1256 18332.681763809283 172369.5219974984 0.7813462763119395 0.7718292405399771 0.363s\n",
      "Generation: 1257 18332.711087899985 172370.14105252372 0.7812774306810554 0.7718120291980853 0.375s\n",
      "Generation: 1258 18332.777753171744 172370.95907408706 0.7812085850501712 0.7718158539407279 0.401s\n",
      "Generation: 1259 18332.384421543877 172367.6052236094 0.7811913736424502 0.7717776065143017 0.275s\n",
      "Generation: 1260 18326.19959044406 172320.92095913555 0.7812257964578923 0.7718636632237609 0.442s\n",
      "Generation: 1261 18326.249410378834 172321.56794903436 0.7812430078656133 0.7718598384811182 0.429s\n",
      "Generation: 1262 18323.29536789883 172292.25592698262 0.781139739419287 0.7720300395287152 0.49s\n",
      "Generation: 1263 18318.6046114192 172254.17878331672 0.7810192595652399 0.7720988848962825 0.406s\n",
      "Generation: 1264 18316.578232986543 172236.72705682248 0.7812257964578923 0.7720931477823186 0.304s\n",
      "Generation: 1265 18316.514277383518 172236.99696995268 0.7811741622347291 0.7720721116977841 0.461s\n",
      "Generation: 1266 18315.133124561704 172224.25044380085 0.7812602192733343 0.7721428694366728 0.411s\n",
      "Generation: 1267 18313.37037958681 172208.04618120016 0.7812430078656133 0.7722002405763122 0.361s\n",
      "Generation: 1268 18313.02162403486 172206.46539313276 0.7812602192733343 0.7721581684072433 0.348s\n",
      "Generation: 1269 18313.017289840027 172207.85434244553 0.7812085850501712 0.7721371323227089 0.468s\n",
      "Generation: 1270 18313.096155543775 172208.61155539728 0.7811913736424502 0.7721409570653515 0.392s\n",
      "Generation: 1271 18313.146429001226 172209.71906098965 0.7812430078656133 0.7721333075800662 0.526s\n",
      "Generation: 1272 18310.96251797691 172193.18855776644 0.7813634877196606 0.7721428694366728 0.406s\n",
      "Generation: 1273 18311.0082918756 172193.68378154113 0.7813634877196606 0.7721390446940302 0.327s\n",
      "Generation: 1274 18305.634825084835 172153.07619978566 0.7814667561659868 0.7722136271755614 0.467s\n",
      "Generation: 1275 18302.52835842554 172135.6392350764 0.7814839675737079 0.7722442251167024 0.4s\n",
      "Generation: 1276 18298.995688147494 172104.20003738426 0.781535601796871 0.772265261201237 0.42s\n",
      "Generation: 1277 18295.969591588324 172077.80324260125 0.7815183903891499 0.7723245447121977 0.318s\n",
      "Generation: 1278 18292.454450546706 172046.2156544221 0.7816216588354762 0.7724259003922274 0.4s\n",
      "Generation: 1279 18290.588347265177 172030.34771610188 0.7814667561659868 0.772460323076011 0.455s\n",
      "Generation: 1280 18290.755095381955 172031.95424479878 0.781552813204592 0.7724526735907258 0.433s\n",
      "Generation: 1281 18290.91725562156 172033.50369132197 0.7817077158740814 0.77242972513487 0.436s\n",
      "Generation: 1282 18288.430023861954 172011.2919045944 0.7816044474277551 0.7724737096752602 0.425s\n",
      "Generation: 1283 18285.557760141233 171991.14233532388 0.7814495447582658 0.7724411993627979 0.337s\n",
      "Generation: 1284 18283.588934192205 171973.68752525517 0.781552813204592 0.7724813591605455 0.356s\n",
      "Generation: 1285 18281.532516064603 171955.21267716776 0.7816732930586393 0.7724679725612963 0.351s\n",
      "Generation: 1286 18279.08176148076 171942.4935820508 0.7816044474277551 0.7724794467892242 0.482s\n",
      "Generation: 1287 18277.674644461655 171928.4247095863 0.7815872360200341 0.7725253437009357 0.368s\n",
      "Generation: 1288 18276.219234056978 171915.31651582406 0.7816216588354762 0.7725291684435783 0.39s\n",
      "Generation: 1289 18272.718296523723 171887.1957471332 0.7816732930586393 0.7725291684435783 0.459s\n",
      "Generation: 1290 18267.807175660302 171856.81210197735 0.7819314641744548 0.7726687715500343 0.442s\n",
      "Generation: 1291 18267.92201150077 171858.51392167064 0.7817593500972444 0.7726247870096441 0.47s\n",
      "Generation: 1292 18262.31256835029 171824.87566683473 0.782034732620781 0.7727242303183525 0.532s\n",
      "Generation: 1293 18262.089226391254 171823.78594905458 0.782034732620781 0.7727012818624966 0.359s\n",
      "Generation: 1294 18262.27135298171 171825.4659876526 0.7818970413590127 0.7726649468073917 0.387s\n",
      "Generation: 1295 18258.75299387571 171802.25373051004 0.7819142527667338 0.7726878952632474 0.516s\n",
      "Generation: 1296 18258.830913187427 171803.30194811735 0.7819314641744548 0.7726840705206048 0.525s\n",
      "Generation: 1297 18258.19346163611 171798.33518503295 0.7818798299512917 0.7726553849507851 0.516s\n",
      "Generation: 1298 18256.404020564492 171785.8560726079 0.7819142527667338 0.7725961014398244 0.429s\n",
      "Generation: 1299 18254.576949853217 171777.75018222767 0.7819142527667338 0.7726152251530375 0.435s\n",
      "Generation: 1300 18253.022740030145 171765.32818040258 0.7819658869898969 0.772661122064749 0.225s\n",
      "Generation: 1301 18249.303448955096 171754.54629729624 0.7822240581057125 0.772599926182467 0.275s\n",
      "Generation: 1302 18246.611636005906 171729.5515895415 0.7822584809211546 0.7726496478368212 0.24s\n",
      "Generation: 1303 18244.046654896003 171718.10680278562 0.7822929037365967 0.7726534725794638 0.299s\n",
      "Generation: 1304 18242.982846155148 171711.15784916704 0.7822412695134335 0.7726859828919261 0.276s\n",
      "Generation: 1305 18241.18202456381 171703.5830909303 0.7822929037365967 0.772697457119854 0.236s\n",
      "Generation: 1306 18239.158352916682 171695.60671781516 0.7823961721829228 0.7727242303183525 0.3s\n",
      "Generation: 1307 18237.907232092173 171684.59926795255 0.7824305949983649 0.7727510035168509 0.256s\n",
      "Generation: 1308 18234.963002667748 171675.6295497179 0.7825338634446911 0.7726802457779622 0.302s\n",
      "Generation: 1309 18231.87760372053 171666.29346353395 0.7825166520369701 0.772661122064749 0.274s\n",
      "Generation: 1310 18229.35874367397 171660.16628063752 0.7823617493674808 0.7727165808330672 0.265s\n",
      "Generation: 1311 18227.280755725187 171651.76644399716 0.7822756923288756 0.7726993694911753 0.266s\n",
      "Generation: 1312 18223.674777813314 171633.8506301274 0.7823617493674808 0.7726878952632474 0.302s\n",
      "Generation: 1313 18222.946080618985 171629.24392497627 0.7822756923288756 0.7726745086639982 0.293s\n",
      "Generation: 1314 18220.530363566766 171626.07512688107 0.7824133835906438 0.7726725962926769 0.297s\n",
      "Generation: 1315 18215.69412872967 171581.45778884983 0.7825338634446911 0.7727892509432771 0.206s\n",
      "Generation: 1316 18214.30919815034 171576.86954349003 0.7824133835906438 0.7727605653734574 0.361s\n",
      "Generation: 1317 18209.12833290866 171532.074190862 0.7826199204832963 0.7727911633145985 0.243s\n",
      "Generation: 1318 18206.691365577193 171524.54188800504 0.7827404003373436 0.7727414416602443 0.365s\n",
      "Generation: 1319 18205.12114385332 171517.5263206874 0.7827231889296226 0.7727070189764607 0.345s\n",
      "Generation: 1320 18203.892264318965 171510.77249089497 0.7827231889296226 0.7727376169176017 0.277s\n",
      "Generation: 1321 18201.558169324104 171506.34073716038 0.7826887661141805 0.7727758643440279 0.293s\n",
      "Generation: 1322 18201.342874006914 171503.96153511916 0.7827059775219015 0.7727529158881722 0.288s\n",
      "Generation: 1323 18199.10810445561 171500.6125215125 0.7826543432987384 0.7728102870278116 0.293s\n",
      "Generation: 1324 18195.95799259456 171486.32183045134 0.7827748231527857 0.7728236736270608 0.253s\n",
      "Generation: 1325 18194.81422724999 171483.29275115643 0.7828436687836698 0.7728083746564903 0.294s\n",
      "Generation: 1326 18193.813584230385 171478.2544039116 0.782929725822275 0.7728447097115952 0.281s\n",
      "Generation: 1327 18192.052653352428 171468.6053841008 0.7829469372299961 0.7729192921931266 0.324s\n",
      "Generation: 1328 18190.316201919915 171464.07589391054 0.7829813600454382 0.7729212045644479 0.249s\n",
      "Generation: 1329 18187.2915200146 171450.501187906 0.7830329942686012 0.7729441530203036 0.288s\n",
      "Generation: 1330 18182.66038657201 171415.08601318556 0.7831190513072064 0.7730627200422252 0.446s\n",
      "Generation: 1331 18180.319303076976 171407.1124221107 0.7832051083458116 0.7730225602444776 0.285s\n",
      "Generation: 1332 18177.771181915912 171395.91402229495 0.7831878969380905 0.7730933179833662 0.329s\n",
      "Generation: 1333 18175.65010497168 171394.74351002392 0.7832739539766957 0.7730416839576907 0.359s\n",
      "Generation: 1334 18174.825432226637 171391.09399719382 0.7832739539766957 0.7730340344724055 0.39s\n",
      "Generation: 1335 18172.183825876626 171381.28178743593 0.7834116452384641 0.7731143540679006 0.334s\n",
      "Generation: 1336 18170.597623116864 171370.9334192401 0.7834288566461851 0.7731047922112941 0.326s\n",
      "Generation: 1337 18168.21261804901 171350.20154370175 0.783377222423022 0.7731143540679006 0.259s\n",
      "Generation: 1338 18167.994719650727 171349.1333826343 0.783377222423022 0.7730894932407235 0.335s\n",
      "Generation: 1339 18167.003713373622 171347.6181244131 0.7832223197535326 0.7731353901524352 0.317s\n",
      "Generation: 1340 18166.362844210315 171346.12196820698 0.7832223197535326 0.7730875808694022 0.332s\n",
      "Generation: 1341 18161.80043297108 171314.8169880346 0.7834804908693482 0.7731315654097924 0.327s\n",
      "Generation: 1342 18160.17496250797 171305.57622159764 0.7834977022770693 0.7731506891230057 0.425s\n",
      "Generation: 1343 18156.52740532703 171281.5500787359 0.7836870277620006 0.7731487767516844 0.322s\n",
      "Generation: 1344 18153.616118064983 171264.1066018863 0.7837042391697217 0.7732080602626451 0.346s\n",
      "Generation: 1345 18149.97360833319 171236.78122222616 0.7836698163542796 0.773141127266399 0.398s\n",
      "Generation: 1346 18138.219191048684 171139.86502507722 0.7836181821311166 0.7733075035713535 0.342s\n",
      "Generation: 1347 18128.717959105394 171062.19440106337 0.7838075076160479 0.773426070593275 0.388s\n",
      "Generation: 1348 18124.81430341244 171054.0588244467 0.7837902962083269 0.7733744365675994 0.303s\n",
      "Generation: 1349 18121.918913131565 171036.40515093948 0.78384193043149 0.7734050345087405 0.317s\n",
      "Generation: 1350 18121.00007382709 171034.32985959342 0.7837902962083269 0.773383998424206 0.415s\n",
      "Generation: 1351 18113.26765358047 170971.70305596775 0.7835665479079534 0.7735083025600915 0.358s\n",
      "Generation: 1352 18108.534277979586 170944.85868874213 0.7838247190237689 0.7735618489570883 0.364s\n",
      "Generation: 1353 18102.252527534238 170894.66831225657 0.783859141839211 0.7736115706114425 0.442s\n",
      "Generation: 1354 18098.477947889925 170860.48960380934 0.7837558733928848 0.7736517304091901 0.412s\n",
      "Generation: 1355 18096.748408212505 170856.39951399324 0.783859141839211 0.7736498180378688 0.336s\n",
      "Generation: 1356 18096.111658059315 170853.28838024268 0.7839968331009793 0.773657467523154 0.446s\n",
      "Generation: 1357 18087.502591301614 170786.89779495992 0.7841001015473056 0.7737282252620427 0.379s\n",
      "Generation: 1358 18085.142233701114 170769.59640526766 0.7841517357704687 0.7737569108318624 0.436s\n",
      "Generation: 1359 18080.33923447817 170728.62186726945 0.7841517357704687 0.7738104572288592 0.458s\n",
      "Generation: 1360 18074.780774227576 170689.94208816334 0.7840140445087004 0.773766472688469 0.413s\n",
      "Generation: 1361 18069.169247289803 170654.14744084334 0.7842033699936318 0.7737569108318624 0.413s\n",
      "Generation: 1362 18068.467981460974 170653.0404846973 0.7842033699936318 0.7737645603171477 0.333s\n",
      "Generation: 1363 18063.92736121841 170625.30864506605 0.7843582726631211 0.77379707062961 0.451s\n",
      "Generation: 1364 18063.578236881534 170623.26134812547 0.7842722156245159 0.7737760345450756 0.449s\n",
      "Generation: 1365 18060.31268326959 170601.2526094313 0.7844615411094473 0.7737741221737542 0.418s\n",
      "Generation: 1366 18058.18877251199 170586.39552023122 0.7844443297017263 0.7738238438281084 0.409s\n",
      "Generation: 1367 18054.980380188543 170561.61995461673 0.7843754840708421 0.7738506170266068 0.357s\n",
      "Generation: 1368 18053.642176131478 170559.6616613645 0.7844099068862842 0.7738678283684987 0.272s\n",
      "Generation: 1369 18053.545345782048 170558.06091745908 0.7844615411094473 0.7738678283684987 0.446s\n",
      "Generation: 1370 18051.691369360728 170540.14272139515 0.7844615411094473 0.7738793025964266 0.259s\n",
      "Generation: 1371 18048.342525359305 170511.38783179846 0.7844271182940052 0.7738965139383184 0.534s\n",
      "Generation: 1372 18047.815478615343 170508.61290905988 0.7844787525171684 0.7738850397103905 0.481s\n",
      "Generation: 1373 18039.825207213657 170446.42797522878 0.7846852894098209 0.7738984263096397 0.433s\n",
      "Generation: 1374 18039.70452375448 170443.33730650047 0.7846680780020998 0.7738563541405707 0.401s\n",
      "Generation: 1375 18036.975585846532 170420.02623768285 0.7848057692638681 0.7739099005375676 0.49s\n",
      "Generation: 1376 18034.803451419764 170401.52334537168 0.7849090377101943 0.773930936622102 0.541s\n",
      "Generation: 1377 18034.746132361928 170400.75247674578 0.7848229806715892 0.7738869520817118 0.465s\n",
      "Generation: 1378 18033.285935684045 170392.61284232012 0.7846852894098209 0.7738984263096397 0.328s\n",
      "Generation: 1379 18029.93023463844 170368.0058096827 0.7846852894098209 0.7739137252802102 0.418s\n",
      "Generation: 1380 18026.855702916902 170352.15365207745 0.784771346448426 0.7739519727066365 0.381s\n",
      "Generation: 1381 18026.836949622113 170351.39880850678 0.7847197122252629 0.7739271118794594 0.41s\n",
      "Generation: 1382 18026.84768111623 170350.94065082786 0.7846680780020998 0.7739462355926725 0.4s\n",
      "Generation: 1383 18024.21949163806 170332.2027069847 0.784736923632984 0.7739997819896693 0.322s\n",
      "Generation: 1384 18023.804371551923 170330.74161722104 0.7847025008175419 0.7739730087911709 0.342s\n",
      "Generation: 1385 18023.202124594452 170329.83638865768 0.7848057692638681 0.7739500603353152 0.433s\n",
      "Generation: 1386 18021.794536159246 170323.05455719604 0.7847025008175419 0.7739557974492791 0.458s\n",
      "Generation: 1387 18020.032433087814 170307.5026394635 0.7846852894098209 0.7740227304455252 0.422s\n",
      "Generation: 1388 18019.940434406053 170307.24731263178 0.7846852894098209 0.7740112562175973 0.434s\n",
      "Generation: 1389 18019.439558070488 170306.8361892143 0.784736923632984 0.7740189057028826 0.41s\n"
     ]
    }
   ],
   "source": [
    "stack = []\n",
    "learning_rate = 0.1\n",
    "max_depth = 3\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "\n",
    "init_log_odds,init_p = initial(y_train_one_hot)\n",
    "log_odds,p = initial_first_bin(init_log_odds,init_p,X_train)\n",
    "residual = y_train_one_hot - p\n",
    "\n",
    "test_log_odds,test_p = initial_first_bin(init_log_odds,init_p,X_test)\n",
    "test_residual = y_test_one_hot - test_p\n",
    "\n",
    "for g in range(10000):\n",
    "    if g % 100 == 0:\n",
    "        eg = update_engine(X_train,y_train,log_odds,p,residual,learning_rate,max_depth)\n",
    "    else:\n",
    "        eg = update_engine(X_train,y_train,log_odds,p,residual,learning_rate,max_depth,eg)\n",
    "\n",
    "    start = time()\n",
    "    node,val,log_odds,p,residual = train_engine(eg)\n",
    "    training_time = time() - start\n",
    "    \n",
    "    residual = y_train_one_hot - p\n",
    "    loss = np.sum(np.power(residual,2))\n",
    "\n",
    "    if not np.isnan(loss):\n",
    "        stack.append(node)\n",
    "\n",
    "        if g % 1000 == 0:\n",
    "            #residual plot\n",
    "            plt.hist(np.max(p,axis=1),bins=100)\n",
    "            plt.show()\n",
    "            \n",
    "        # train_acc.append(loss)\n",
    "        # test_acc.append(test_loss)\n",
    "        \n",
    "        pred = np.argmax(p,axis=1)\n",
    "        train_acc.append(accuracy_score(y_train,pred))\n",
    "        # pred = p[:,1]\n",
    "        # train_acc.append(roc_auc_score(y_train,pred))\n",
    "\n",
    "\n",
    "        # test data prediction\n",
    "        test_log_odds,test_p = predict_single_node(node,X_test,test_log_odds,test_p,learning_rate)\n",
    "        test_residual = y_test_one_hot - test_p\n",
    "        test_loss = np.sum(np.power(test_residual,2))\n",
    "\n",
    "        \n",
    "        # test_pred = predict(X_test,init_log_odds,init_p,learning_rate,stack)\n",
    "        test_pred = np.argmax(test_p,axis=1)\n",
    "        test_acc.append(accuracy_score(y_test,test_pred))\n",
    "        # test_pred = test_p[:,1]\n",
    "        # test_acc.append(roc_auc_score(y_test,test_pred))\n",
    "    \n",
    "        # print(\"Generation:\",g,loss,train_acc[-1],str(round(training_time,3))+'s')\n",
    "        print(\"Generation:\",g,loss,test_loss,train_acc[-1],test_acc[-1],str(round(training_time,3))+'s')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecde1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2093\n",
       "2    1498\n",
       "3    1017\n",
       "4     521\n",
       "5     232\n",
       "6      69\n",
       "7      15\n",
       "8       4\n",
       "9       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts([node.depth for node in stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba84308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     2093\n",
       "5     1336\n",
       "7      864\n",
       "9      511\n",
       "11     294\n",
       "13     149\n",
       "15     105\n",
       "17      41\n",
       "19      30\n",
       "23       7\n",
       "21       7\n",
       "25       7\n",
       "27       2\n",
       "29       2\n",
       "31       1\n",
       "33       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts([node.numNode for node in stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81959aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 2.196240601503759)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([node.depth for node in stack[:130]]),np.average([node.depth for node in stack[130:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7270a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.507692307692308, 5.932706766917293)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([node.numNode for node in stack[:130]]),np.average([node.numNode for node in stack[130:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92     21477\n",
      "           1       0.93      0.94      0.93     28054\n",
      "           2       0.98      0.98      0.98      3626\n",
      "           3       1.00      1.00      1.00       267\n",
      "           4       0.99      0.87      0.92       909\n",
      "           5       0.98      0.98      0.98      1724\n",
      "           6       0.99      0.97      0.98      2044\n",
      "\n",
      "    accuracy                           0.93     58101\n",
      "   macro avg       0.97      0.95      0.96     58101\n",
      "weighted avg       0.93      0.93      0.93     58101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, np.argmax(p,axis=1)))\n",
    "# print(roc_auc_score(y_train,p[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79    190363\n",
      "           1       0.82      0.85      0.84    255247\n",
      "           2       0.81      0.86      0.84     32128\n",
      "           3       0.85      0.67      0.75      2480\n",
      "           4       0.77      0.41      0.54      8584\n",
      "           5       0.73      0.65      0.68     15643\n",
      "           6       0.90      0.79      0.84     18466\n",
      "\n",
      "    accuracy                           0.81    522911\n",
      "   macro avg       0.81      0.72      0.75    522911\n",
      "weighted avg       0.81      0.81      0.81    522911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(test_p,axis=1)))\n",
    "# print(roc_auc_score(y_test,test_p[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c9225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "acc = np.array(train_acc)\n",
    "\n",
    "# w = [np.sum(np.abs(grad_bin)) for node,node_param,grad_bin in stack[:i]]\n",
    "w = [np.sum(node.clf[1][1]) for node in stack]\n",
    "# w = [0]\n",
    "# for node in stack:\n",
    "#     w.append(w[-1]+np.sum(node.clf[1][1]))\n",
    "\n",
    "# w = minmax_scale(w)\n",
    "# m = minmax_scale(acc[1:i+1]-acc[:i])\n",
    "m = acc[:i]\n",
    "plt.plot(w)\n",
    "# plt.xscale('log')\n",
    "plt.show()\n",
    "plt.plot(m[1:]-m[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53192483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27d04b896d0>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7941764c",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f84104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=100)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23865252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.886     0.868     0.877     21477\n",
      "           1      0.891     0.913     0.902     28054\n",
      "           2      0.970     0.972     0.971      3626\n",
      "           3      1.000     1.000     1.000       267\n",
      "           4      0.968     0.738     0.838       909\n",
      "           5      0.958     0.948     0.953      1724\n",
      "           6      0.989     0.977     0.983      2044\n",
      "\n",
      "    accuracy                          0.901     58101\n",
      "   macro avg      0.952     0.917     0.932     58101\n",
      "weighted avg      0.902     0.901     0.901     58101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     0.829     0.833    190363\n",
      "           1      0.853     0.879     0.866    255247\n",
      "           2      0.861     0.886     0.873     32128\n",
      "           3      0.884     0.746     0.809      2480\n",
      "           4      0.847     0.477     0.611      8584\n",
      "           5      0.789     0.738     0.763     15643\n",
      "           6      0.929     0.853     0.890     18466\n",
      "\n",
      "    accuracy                          0.849    522911\n",
      "   macro avg      0.857     0.773     0.806    522911\n",
      "weighted avg      0.849     0.849     0.848    522911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = xgb.predict_proba(X_train)\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_train,pred,digits=3))\n",
    "\n",
    "pred = xgb.predict_proba(X_train)\n",
    "pred = pred[:,1]\n",
    "# print(roc_auc_score(y_train,pred))\n",
    "\n",
    "pred = xgb.predict_proba(X_test)\n",
    "\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_test,pred,digits=3))\n",
    "\n",
    "pred = xgb.predict_proba(X_test)\n",
    "pred = pred[:,1]\n",
    "# print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a73c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree</th>\n",
       "      <th>Node</th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Split</th>\n",
       "      <th>Yes</th>\n",
       "      <th>No</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Gain</th>\n",
       "      <th>Cover</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>f0</td>\n",
       "      <td>3023.5</td>\n",
       "      <td>0-1</td>\n",
       "      <td>0-2</td>\n",
       "      <td>0-1</td>\n",
       "      <td>12462.416000</td>\n",
       "      <td>14228.817400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>f0</td>\n",
       "      <td>2787.5</td>\n",
       "      <td>0-3</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0-3</td>\n",
       "      <td>1230.060670</td>\n",
       "      <td>7725.796390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0-2</td>\n",
       "      <td>f0</td>\n",
       "      <td>3065.5</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0-6</td>\n",
       "      <td>0-5</td>\n",
       "      <td>494.205078</td>\n",
       "      <td>6503.021000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0-3</td>\n",
       "      <td>f5</td>\n",
       "      <td>2835.5</td>\n",
       "      <td>0-7</td>\n",
       "      <td>0-8</td>\n",
       "      <td>0-7</td>\n",
       "      <td>97.239136</td>\n",
       "      <td>3257.387940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>f3</td>\n",
       "      <td>92.5</td>\n",
       "      <td>0-9</td>\n",
       "      <td>0-10</td>\n",
       "      <td>0-9</td>\n",
       "      <td>515.172363</td>\n",
       "      <td>4468.408690</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43615</th>\n",
       "      <td>699</td>\n",
       "      <td>98</td>\n",
       "      <td>699-98</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.146271</td>\n",
       "      <td>9.169310</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43616</th>\n",
       "      <td>699</td>\n",
       "      <td>99</td>\n",
       "      <td>699-99</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.079653</td>\n",
       "      <td>8.600679</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43617</th>\n",
       "      <td>699</td>\n",
       "      <td>100</td>\n",
       "      <td>699-100</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094662</td>\n",
       "      <td>1.463033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43618</th>\n",
       "      <td>699</td>\n",
       "      <td>101</td>\n",
       "      <td>699-101</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058399</td>\n",
       "      <td>13.785927</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43619</th>\n",
       "      <td>699</td>\n",
       "      <td>102</td>\n",
       "      <td>699-102</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.070105</td>\n",
       "      <td>1.846027</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43620 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tree  Node       ID Feature   Split  Yes    No Missing          Gain  \\\n",
       "0         0     0      0-0      f0  3023.5  0-1   0-2     0-1  12462.416000   \n",
       "1         0     1      0-1      f0  2787.5  0-3   0-4     0-3   1230.060670   \n",
       "2         0     2      0-2      f0  3065.5  0-5   0-6     0-5    494.205078   \n",
       "3         0     3      0-3      f5  2835.5  0-7   0-8     0-7     97.239136   \n",
       "4         0     4      0-4      f3    92.5  0-9  0-10     0-9    515.172363   \n",
       "...     ...   ...      ...     ...     ...  ...   ...     ...           ...   \n",
       "43615   699    98   699-98    Leaf     NaN  NaN   NaN     NaN      0.146271   \n",
       "43616   699    99   699-99    Leaf     NaN  NaN   NaN     NaN     -0.079653   \n",
       "43617   699   100  699-100    Leaf     NaN  NaN   NaN     NaN      0.094662   \n",
       "43618   699   101  699-101    Leaf     NaN  NaN   NaN     NaN      0.058399   \n",
       "43619   699   102  699-102    Leaf     NaN  NaN   NaN     NaN     -0.070105   \n",
       "\n",
       "              Cover  Category  \n",
       "0      14228.817400       NaN  \n",
       "1       7725.796390       NaN  \n",
       "2       6503.021000       NaN  \n",
       "3       3257.387940       NaN  \n",
       "4       4468.408690       NaN  \n",
       "...             ...       ...  \n",
       "43615      9.169310       NaN  \n",
       "43616      8.600679       NaN  \n",
       "43617      1.463033       NaN  \n",
       "43618     13.785927       NaN  \n",
       "43619      1.846027       NaN  \n",
       "\n",
       "[43620 rows x 11 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.get_booster().trees_to_dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b222abf9",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d611a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82911c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420b0cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(X_train)\n\u001b[0;32m      2\u001b[0m pred \u001b[39m=\u001b[39m pred[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(roc_auc_score(y_train,pred))\n",
      "File \u001b[1;32mc:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:565\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    559\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPartial AUC computation not available in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmulticlass setting, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_fpr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m set to `None`, received `max_fpr=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(max_fpr)\n\u001b[0;32m    563\u001b[0m         )\n\u001b[0;32m    564\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 565\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    566\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[0;32m    567\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "pred = clf.predict_proba(X_train)\n",
    "pred = pred[:,1]\n",
    "print(roc_auc_score(y_train,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(X_test)\n",
    "pred = pred[:,1]\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d7659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.875     0.856     0.866     21421\n",
      "           1      0.882     0.907     0.894     28284\n",
      "           2      0.967     0.954     0.961      3542\n",
      "           3      1.000     1.000     1.000       278\n",
      "           4      0.975     0.798     0.878       878\n",
      "           5      0.944     0.910     0.927      1671\n",
      "           6      0.982     0.932     0.956      2027\n",
      "\n",
      "    accuracy                          0.891     58101\n",
      "   macro avg      0.946     0.908     0.926     58101\n",
      "weighted avg      0.892     0.891     0.891     58101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "print(classification_report(y_train,pred,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea07ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.830     0.818     0.824    190419\n",
      "           1      0.843     0.874     0.858    255017\n",
      "           2      0.847     0.864     0.855     32212\n",
      "           3      0.823     0.740     0.779      2469\n",
      "           4      0.810     0.475     0.599      8615\n",
      "           5      0.755     0.687     0.719     15696\n",
      "           6      0.909     0.813     0.858     18483\n",
      "\n",
      "    accuracy                          0.838    522911\n",
      "   macro avg      0.831     0.753     0.785    522911\n",
      "weighted avg      0.838     0.838     0.837    522911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print(classification_report(y_test,pred,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0]\n",
    "\n",
    "for e in clf.estimators_:\n",
    "    a = e[0]\n",
    "    w.append(w[-1] + np.sum(a.tree_.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a88b10ee",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f087f87",
   "metadata": {},
   "source": [
    "## CoverType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f584528",
   "metadata": {},
   "source": [
    "## Higgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064f3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     10474\n",
       "2     10162\n",
       "3      9116\n",
       "4      7430\n",
       "5      5444\n",
       "6      3623\n",
       "7      2027\n",
       "8      1045\n",
       "9       455\n",
       "10      147\n",
       "11       56\n",
       "12       15\n",
       "13        2\n",
       "14        2\n",
       "15        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts([node.depth for node in stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3      10474\n",
       "5       8779\n",
       "7       6702\n",
       "9       5288\n",
       "11      3927\n",
       "13      3066\n",
       "15      2501\n",
       "17      1975\n",
       "19      1586\n",
       "21      1180\n",
       "23       935\n",
       "25       768\n",
       "27       592\n",
       "29       443\n",
       "31       330\n",
       "33       276\n",
       "35       209\n",
       "37       178\n",
       "39       146\n",
       "41       110\n",
       "43        96\n",
       "45        76\n",
       "47        73\n",
       "49        62\n",
       "51        41\n",
       "53        39\n",
       "55        38\n",
       "57        25\n",
       "65        15\n",
       "59        11\n",
       "63        10\n",
       "61         8\n",
       "67         6\n",
       "77         6\n",
       "71         6\n",
       "79         4\n",
       "83         3\n",
       "87         3\n",
       "73         3\n",
       "107        2\n",
       "95         1\n",
       "105        1\n",
       "97         1\n",
       "69         1\n",
       "85         1\n",
       "109        1\n",
       "75         1\n",
       "111        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts([node.numNode for node in stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abafc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.495, 3.3331224489795916)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([node.depth for node in stack[:1000]]),np.average([node.depth for node in stack[1000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db0124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.56, 10.449551020408164)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([node.numNode for node in stack[:1000]]),np.average([node.numNode for node in stack[1000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fbd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7862846424520893\n"
     ]
    }
   ],
   "source": [
    "test_pred = predict(X_test,init_log_odds,init_p,learning_rate,stack)\n",
    "# test_pred = np.argmax(test_pred,axis=1)\n",
    "# print(classification_report(y_test,test_pred))\n",
    "\n",
    "test_pred = test_pred[:,1]\n",
    "print(roc_auc_score(y_test,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(X_train,init_log_odds,init_p,learning_rate,stack)\n",
    "# print(classification_report(y_train,pred))\n",
    "pred = pred[:,1]\n",
    "print(roc_auc_score(y_train,pred))\n",
    "\n",
    "pred = predict(X_test,init_log_odds,init_p,learning_rate,stack)\n",
    "# pred = np.argmax(pred,axis=1)\n",
    "# print(classification_report(y_test,pred))\n",
    "\n",
    "pred = pred[:,1]\n",
    "print(roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "acc = np.array(train_acc)\n",
    "\n",
    "# w = [np.sum(np.abs(grad_bin)) for node,node_param,grad_bin in stack[:i]]\n",
    "w = [np.sum(node.clf[1][1]) for node in stack]\n",
    "# w = [0]\n",
    "# for node in stack:\n",
    "#     w.append(w[-1]+np.sum(node.clf[1][1]))\n",
    "\n",
    "# w = minmax_scale(w)\n",
    "# m = minmax_scale(acc[1:i+1]-acc[:i])\n",
    "m = acc[:i]\n",
    "plt.plot(w)\n",
    "# plt.xscale('log')\n",
    "plt.show()\n",
    "plt.plot(m[1:]-m[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf11b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90cddfe2",
   "metadata": {},
   "source": [
    "## kdd99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f799c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhouz\\AppData\\Local\\Temp\\ipykernel_3056\\1286640705.py:1: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  np.sum(n.numNode for n in stack),np.average([n.depth for n in stack])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3619, 2.2007104795737122)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(n.numNode for n in stack),np.average([n.depth for n in stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b629788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000      1102\n",
      "           1      1.000     1.000     1.000        15\n",
      "           2      1.000     1.000     1.000         4\n",
      "           3      1.000     1.000     1.000        27\n",
      "           4      1.000     0.833     0.909         6\n",
      "           5      0.997     0.998     0.998       623\n",
      "           6      1.000     1.000     1.000        11\n",
      "           7      1.000     0.750     0.857         4\n",
      "           8      1.000     1.000     1.000         3\n",
      "           9      1.000     1.000     1.000     53601\n",
      "          10      1.000     0.878     0.935       115\n",
      "          11      0.999     1.000     1.000     48639\n",
      "          12      1.000     1.000     1.000         2\n",
      "          13      1.000     1.000     1.000         2\n",
      "          14      0.992     1.000     0.996       132\n",
      "          15      1.000     0.998     0.999       520\n",
      "          16      1.000     0.800     0.889         5\n",
      "          17      1.000     0.996     0.998       794\n",
      "          18      1.000     1.000     1.000    140395\n",
      "          19      0.000     0.000     0.000         1\n",
      "          20      1.000     1.000     1.000       489\n",
      "          21      0.996     0.975     0.985       510\n",
      "          22      1.000     1.000     1.000        10\n",
      "\n",
      "    accuracy                          1.000    247010\n",
      "   macro avg      0.956     0.923     0.938    247010\n",
      "weighted avg      1.000     1.000     1.000    247010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,p.argmax(axis=1),digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76833e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     1.000     0.997      1101\n",
      "           1      0.714     0.667     0.690        15\n",
      "           2      0.000     0.000     0.000         4\n",
      "           3      0.960     0.923     0.941        26\n",
      "           4      1.000     0.500     0.667         6\n",
      "           5      0.992     0.990     0.991       624\n",
      "           6      0.909     1.000     0.952        10\n",
      "           7      1.000     0.200     0.333         5\n",
      "           8      1.000     0.250     0.400         4\n",
      "           9      1.000     1.000     1.000     53600\n",
      "          10      1.000     0.871     0.931       116\n",
      "          11      0.999     1.000     0.999     48638\n",
      "          12      0.500     1.000     0.667         1\n",
      "          13      1.000     0.500     0.667         2\n",
      "          14      0.992     1.000     0.996       132\n",
      "          15      0.998     0.994     0.996       520\n",
      "          16      1.000     0.200     0.333         5\n",
      "          17      0.999     0.994     0.996       795\n",
      "          18      1.000     1.000     1.000    140395\n",
      "          19      0.000     0.000     0.000         1\n",
      "          20      1.000     1.000     1.000       490\n",
      "          21      0.982     0.969     0.975       510\n",
      "          22      1.000     0.600     0.750        10\n",
      "\n",
      "    accuracy                          1.000    247010\n",
      "   macro avg      0.871     0.724     0.751    247010\n",
      "weighted avg      1.000     1.000     1.000    247010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhouz\\miniconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,test_p.argmax(axis=1),digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "965aa81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2290dece1d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc)\n",
    "# plt.plot(train_grads_acc)\n",
    "plt.plot(test_acc)\n",
    "# plt.plot(test_grads_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814eaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BStackGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "598cefc26d6e5a65b2978c65314d0610ea9dfe34c7d989c4b6d2528d500ccb7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
