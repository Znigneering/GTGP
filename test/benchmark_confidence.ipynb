{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8bc990-ebc5-46bd-bab4-f4091ce5b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../script/\")\n",
    "\n",
    "import Functions\n",
    "from Engine import Engine\n",
    "from GTGP import GTGP\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix,f1_score,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import importlib\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbad47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    print(roc_auc_score(true_y,prob[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114de92",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feedaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/confidence.tsv\",delimiter='\\t')\n",
    "X = df.iloc[:,:-1].to_numpy().astype(\"float\")\n",
    "y = df.iloc[:,-1].to_numpy().astype(\"int\")\n",
    "\n",
    "seeds = [10086, 200,500,30506,30405,30420,10056,7059,40965,5398,869543,83491,823190,\n",
    "         48392,2810,48392,3498210,483902,859032,12890,538920,86954,54309,6504,9840,\n",
    "         219805,548,2981,432890,5438908,219094,5843902,60854,979,12890,2108,4093]\n",
    "train_size = 0.7\n",
    "dataset = 'confidence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745d0030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12\n",
       "4    12\n",
       "2    12\n",
       "1    12\n",
       "5    12\n",
       "3    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2341bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y,yt):\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = le.fit_transform(y)\n",
    "    y_one_hot = ohe.fit_transform(y_train.reshape(-1,1))\n",
    "    \n",
    "    y_test = le.transform(yt)\n",
    "    yt_one_hot = ohe.transform(y_test.reshape(-1,1))\n",
    "    \n",
    "    return y_train,y_test,y_one_hot,yt_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd30c5",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f418f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 21 1\n",
      "11 33 2\n",
      "12 36 3\n",
      "13 39 4\n",
      "14 42 5\n",
      "14 42 6\n",
      "14 42 7\n",
      "15 45 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.98 1.294027078219048 \ttest: 0.8636363636363636 4.926873060249929\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.7228999636073361 \ttest: 0.9090909090909091 4.35406460428511\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.450090708232322 \ttest: 0.9090909090909091 4.115626935895159\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.3007305166037153 \ttest: 0.9090909090909091 3.9789933930487136\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.21277682086841201 \ttest: 0.9090909090909091 3.888380639609937\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.15758483201286605 \ttest: 0.9090909090909091 3.823369495878639\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.12102613709122034 \ttest: 0.9090909090909091 3.7745309921688\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.095696345793826 \ttest: 0.9090909090909091 3.7367795614659327\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.07748128350825897 \ttest: 0.9090909090909091 3.7070547745003974\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.06397084576698484 \ttest: 0.9090909090909091 3.683364202248136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.50000   0.66667         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    0.80000   1.00000   0.88889         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    0.80000   1.00000   0.88889         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.93333   0.91667   0.90741        22\n",
      "weighted avg    0.92727   0.90909   0.89899        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "10 30 2\n",
      "11 33 3\n",
      "13 39 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.2302307859824255 \ttest: 0.8636363636363636 6.745261357951963\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.06965115474978999 \ttest: 0.8181818181818182 7.034237592608827\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.03406596711817999 \ttest: 0.7727272727272727 7.222795278750879\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.02036164891939458 \ttest: 0.7727272727272727 7.354607949270122\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.013605691347332555 \ttest: 0.7727272727272727 7.4531498176750635\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.009763108998385804 \ttest: 0.7727272727272727 7.530567240333755\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.007362184551217675 \ttest: 0.7727272727272727 7.593669262097902\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.005758529001688087 \ttest: 0.7727272727272727 7.646562825683875\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.004632572907805456 \ttest: 0.7727272727272727 7.691877437266508\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0038107474857248186 \ttest: 0.7727272727272727 7.731380068827081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         9\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.25000   0.33333         4\n",
      "           1    0.80000   1.00000   0.88889         4\n",
      "           2    0.57143   1.00000   0.72727         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.66667   0.80000         3\n",
      "           5    1.00000   0.66667   0.80000         3\n",
      "\n",
      "    accuracy                        0.77273        22\n",
      "   macro avg    0.81190   0.76389   0.75825        22\n",
      "weighted avg    0.79481   0.77273   0.75445        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "6 18 1\n",
      "10 30 2\n",
      "11 33 3\n",
      "13 39 4\n",
      "14 42 5\n",
      "15 45 6\n",
      "15 45 7\n",
      "15 45 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.29217411423809936 \ttest: 0.6818181818181818 8.783685290933303\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.09408772691506025 \ttest: 0.6363636363636364 8.882631370279267\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.046570554396319436 \ttest: 0.6363636363636364 8.918343303874213\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.027893251133452316 \ttest: 0.6818181818181818 8.935657901222356\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.018627370323579348 \ttest: 0.6818181818181818 8.945463083615929\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.01334770036889358 \ttest: 0.6818181818181818 8.951796748636834\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.010048728473768274 \ttest: 0.6818181818181818 8.95640794826069\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.007846839062259013 \ttest: 0.6818181818181818 8.960131586571997\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.006302498882751323 \ttest: 0.6818181818181818 8.96339305194234\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.005176656536767305 \ttest: 0.6818181818181818 8.966418513281113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.25000   0.33333   0.28571         3\n",
      "           1    1.00000   0.50000   0.66667         4\n",
      "           2    0.75000   0.75000   0.75000         4\n",
      "           3    0.75000   1.00000   0.85714         3\n",
      "           4    0.66667   0.50000   0.57143         4\n",
      "           5    0.80000   1.00000   0.88889         4\n",
      "\n",
      "    accuracy                        0.68182        22\n",
      "   macro avg    0.70278   0.68056   0.66997        22\n",
      "weighted avg    0.72121   0.68182   0.67893        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "11 33 2\n",
      "15 45 3\n",
      "15 45 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.8597126957986387 \ttest: 0.9545454545454546 4.5996704426734425\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.3500917745670925 \ttest: 0.9090909090909091 4.7228518390626615\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.19362551286433288 \ttest: 0.9090909090909091 4.8262002557187555\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.12381237075566709 \ttest: 0.9090909090909091 4.906328030059245\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.0863105041351574 \ttest: 0.9090909090909091 4.972602465493585\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.063746576222417 \ttest: 0.9090909090909091 5.030151135896972\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.04907916669805523 \ttest: 0.9090909090909091 5.081693173600215\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.03899151239509789 \ttest: 0.9090909090909091 5.128765649803615\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.03174809190130916 \ttest: 0.9090909090909091 5.1723071653841215\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.026366791886985805 \ttest: 0.9090909090909091 5.212935103728658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         9\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66667   1.00000   0.80000         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    1.00000   0.75000   0.85714         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.75000   0.85714         4\n",
      "           5    1.00000   1.00000   1.00000         3\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.94444   0.91667   0.91905        22\n",
      "weighted avg    0.93939   0.90909   0.91169        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "9 27 1\n",
      "10 30 2\n",
      "13 39 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.98 1.3609673222461875 \ttest: 0.6818181818181818 11.46550562783234\n",
      "retrain  2 :\n",
      "\ttrain: 0.98 1.1146943763722386 \ttest: 0.6818181818181818 10.823121649901676\n",
      "retrain  3 :\n",
      "\ttrain: 0.98 1.0563362701304067 \ttest: 0.7272727272727273 10.513745820924038\n",
      "retrain  4 :\n",
      "\ttrain: 0.98 1.0335721646076295 \ttest: 0.7727272727272727 10.314883711042294\n",
      "retrain  5 :\n",
      "\ttrain: 0.98 1.0223315115361242 \ttest: 0.7727272727272727 10.169773191449396\n",
      "retrain  6 :\n",
      "\ttrain: 0.98 1.0159499681416608 \ttest: 0.7727272727272727 10.056379024823293\n",
      "retrain  7 :\n",
      "\ttrain: 0.98 1.0119747637369587 \ttest: 0.7727272727272727 9.963948374040504\n",
      "retrain  8 :\n",
      "\ttrain: 0.98 1.0093286105191202 \ttest: 0.7727272727272727 9.8864240791346\n",
      "retrain  9 :\n",
      "\ttrain: 0.98 1.007477043329711 \ttest: 0.7727272727272727 9.82004548231265\n",
      "retrain  10 :\n",
      "\ttrain: 0.98 1.0061300557877262 \ttest: 0.7727272727272727 9.762309943495111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.90000   1.00000   0.94737         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   0.88889   0.94118         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        0.98000        50\n",
      "   macro avg    0.98333   0.98148   0.98142        50\n",
      "weighted avg    0.98200   0.98000   0.97994        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.33333   0.40000         3\n",
      "           1    0.66667   1.00000   0.80000         4\n",
      "           2    0.60000   1.00000   0.75000         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.75000   0.85714         4\n",
      "           5    1.00000   0.50000   0.66667         4\n",
      "\n",
      "    accuracy                        0.77273        22\n",
      "   macro avg    0.79444   0.76389   0.74563        22\n",
      "weighted avg    0.81667   0.77273   0.76115        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "10 30 1\n",
      "12 36 2\n",
      "14 42 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.3418721143237584 \ttest: 0.7727272727272727 7.922689190188158\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.1172461520464982 \ttest: 0.7727272727272727 7.442588478939187\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.06114536973472395 \ttest: 0.7727272727272727 7.245091297860021\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.03800231002810042 \ttest: 0.7727272727272727 7.140967908357448\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.026088623427829808 \ttest: 0.7727272727272727 7.07909955532875\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.019103544477689402 \ttest: 0.7727272727272727 7.0396852277681425\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.014637855355197925 \ttest: 0.8181818181818182 7.0134709856640285\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.01160024522510814 \ttest: 0.8181818181818182 6.995574596555059\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.009435278786423014 \ttest: 0.8181818181818182 6.98319047522408\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.00783496327985567 \ttest: 0.8181818181818182 6.974600675673225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.75000   0.60000         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    0.50000   0.33333   0.40000         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.75000   0.85714         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.83333   0.80556   0.80952        22\n",
      "weighted avg    0.84091   0.81818   0.81948        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "6 18 1\n",
      "10 30 2\n",
      "15 45 3\n",
      "15 45 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "15 45 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.5717327298367254 \ttest: 0.7727272727272727 8.13542792566877\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.2447712956624863 \ttest: 0.7727272727272727 7.888689248512126\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.14014630938636852 \ttest: 0.8181818181818182 7.7880013313925165\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.09113068789425699 \ttest: 0.8181818181818182 7.730144717974676\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.06408009185275632 \ttest: 0.8181818181818182 7.692294506191504\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.04755076518750696 \ttest: 0.8181818181818182 7.6656627385329195\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.03670529199367457 \ttest: 0.8181818181818182 7.6459893193811785\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.029202465953995747 \ttest: 0.8181818181818182 7.63092845950524\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.023794858773638346 \ttest: 0.8181818181818182 7.619076208184804\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.019767678473029415 \ttest: 0.8181818181818182 7.609540229145179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.33333   0.33333   0.33333         3\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    0.66667   1.00000   0.80000         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.50000   0.66667         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.83333   0.80556   0.80000        22\n",
      "weighted avg    0.84848   0.81818   0.81212        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "8 24 1\n",
      "11 33 2\n",
      "14 42 3\n",
      "16 48 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.5767016130912105 \ttest: 0.8181818181818182 7.728503966627381\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.2411871055962653 \ttest: 0.8181818181818182 7.649205517241919\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.14250003317549584 \ttest: 0.8181818181818182 7.663590441139619\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.09601527730596708 \ttest: 0.8181818181818182 7.68791247756007\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.069649648433789 \ttest: 0.8181818181818182 7.710668731099873\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.05304760378047379 \ttest: 0.8181818181818182 7.7302026459710635\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.041846432784776594 \ttest: 0.8181818181818182 7.746697017626074\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.03390358707420299 \ttest: 0.8181818181818182 7.760649449278391\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.02805365056760124 \ttest: 0.8181818181818182 7.77253839273824\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.02361395129195014 \ttest: 0.8181818181818182 7.782758895326755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.50000   0.66667         4\n",
      "           1    0.75000   0.75000   0.75000         4\n",
      "           2    0.80000   1.00000   0.88889         4\n",
      "           3    0.75000   1.00000   0.85714         3\n",
      "           4    0.75000   1.00000   0.85714         3\n",
      "           5    1.00000   0.75000   0.85714         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.84167   0.83333   0.81283        22\n",
      "weighted avg    0.85000   0.81818   0.80880        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "6 18 1\n",
      "7 21 2\n",
      "12 36 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.6243020985173908 \ttest: 0.7727272727272727 6.190344650651859\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.22026302245911197 \ttest: 0.8181818181818182 6.626079788824796\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.11765212905232628 \ttest: 0.8181818181818182 6.857267962578768\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.07454097882605097 \ttest: 0.8181818181818182 6.991354373070562\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.051959993340974064 \ttest: 0.8181818181818182 7.0768037257686816\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.0385219363933511 \ttest: 0.8181818181818182 7.134679625057992\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.029821449487878203 \ttest: 0.8181818181818182 7.1755228081108715\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.023838976304354657 \ttest: 0.8181818181818182 7.205176457279073\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.01953511199674446 \ttest: 0.8181818181818182 7.22713105228439\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.01632764148712608 \ttest: 0.8181818181818182 7.243593919718924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   1.00000   0.66667         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    1.00000   0.50000   0.66667         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    1.00000   0.50000   0.66667         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.91667   0.83333   0.83333        22\n",
      "weighted avg    0.90909   0.81818   0.81818        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "10 30 2\n",
      "12 36 3\n",
      "12 36 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.98 1.0896290034577996 \ttest: 0.8636363636363636 3.106168613196282\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.575025636160682 \ttest: 0.8636363636363636 2.5115728346563966\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.36376637005162704 \ttest: 0.9545454545454546 2.2575169101146826\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.2505213116872931 \ttest: 0.9545454545454546 2.1131300469341565\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.18261759840189468 \ttest: 0.9545454545454546 2.0196707811988266\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.1388103065990361 \ttest: 0.9545454545454546 1.9541931265476162\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.10897196433376644 \ttest: 0.9545454545454546 1.9057598596165648\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.08776791122185565 \ttest: 0.9545454545454546 1.8684768358427366\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.07217660258859511 \ttest: 0.9545454545454546 1.8388843680069689\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.06038622856413636 \ttest: 0.9545454545454546 1.8148182353824287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.75000   0.85714         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    0.80000   1.00000   0.88889         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    1.00000   1.00000   1.00000         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.95455        22\n",
      "   macro avg    0.96667   0.95833   0.95767        22\n",
      "weighted avg    0.96364   0.95455   0.95382        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "10 30 1\n",
      "12 36 2\n",
      "15 45 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "17 51 6\n",
      "17 51 7\n",
      "17 51 8\n",
      "17 51 9\n",
      "17 51 10\n",
      "17 51 11\n",
      "17 51 12\n",
      "17 51 13\n",
      "17 51 14\n",
      "17 51 15\n",
      "17 51 16\n",
      "17 51 17\n",
      "17 51 18\n",
      "17 51 19\n",
      "17 51 20\n",
      "17 51 21\n",
      "17 51 22\n",
      "17 51 23\n",
      "17 51 24\n",
      "17 51 25\n",
      "17 51 26\n",
      "17 51 27\n",
      "17 51 28\n",
      "17 51 29\n",
      "17 51 30\n",
      "17 51 31\n",
      "17 51 32\n",
      "17 51 33\n",
      "17 51 34\n",
      "17 51 35\n",
      "17 51 36\n",
      "17 51 37\n",
      "17 51 38\n",
      "17 51 39\n",
      "17 51 40\n",
      "17 51 41\n",
      "17 51 42\n",
      "17 51 43\n",
      "17 51 44\n",
      "17 51 45\n",
      "17 51 46\n",
      "17 51 47\n",
      "17 51 48\n",
      "17 51 49\n",
      "17 51 50\n",
      "17 51 51\n",
      "17 51 52\n",
      "17 51 53\n",
      "17 51 54\n",
      "17 51 55\n",
      "17 51 56\n",
      "17 51 57\n",
      "17 51 58\n",
      "17 51 59\n",
      "17 51 60\n",
      "17 51 61\n",
      "17 51 62\n",
      "17 51 63\n",
      "17 51 64\n",
      "17 51 65\n",
      "17 51 66\n",
      "17 51 67\n",
      "17 51 68\n",
      "17 51 69\n",
      "17 51 70\n",
      "17 51 71\n",
      "17 51 72\n",
      "17 51 73\n",
      "17 51 74\n",
      "17 51 75\n",
      "17 51 76\n",
      "17 51 77\n",
      "17 51 78\n",
      "17 51 79\n",
      "17 51 80\n",
      "17 51 81\n",
      "17 51 82\n",
      "17 51 83\n",
      "17 51 84\n",
      "17 51 85\n",
      "17 51 86\n",
      "17 51 87\n",
      "17 51 88\n",
      "17 51 89\n",
      "17 51 90\n",
      "17 51 91\n",
      "17 51 92\n",
      "17 51 93\n",
      "17 51 94\n",
      "17 51 95\n",
      "17 51 96\n",
      "17 51 97\n",
      "17 51 98\n",
      "17 51 99\n",
      "17 51 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.6007636798134364 \ttest: 0.8181818181818182 7.6607199406898\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.22573728119125197 \ttest: 0.8181818181818182 7.693054972920148\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.12008458265508434 \ttest: 0.8181818181818182 7.727859602206607\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.07493457834630086 \ttest: 0.8181818181818182 7.755544269952697\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.051396857580048985 \ttest: 0.8181818181818182 7.77838765708428\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.037538092537040234 \ttest: 0.8181818181818182 7.797842342417027\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.028672365691342197 \ttest: 0.8181818181818182 7.8147738741967085\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0226480332896044 \ttest: 0.8181818181818182 7.829738976879668\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.01836218566495121 \ttest: 0.8181818181818182 7.843121874049821\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.015201058589854013 \ttest: 0.8181818181818182 7.855202004367193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.25000   0.33333         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    0.50000   0.75000   0.60000         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    1.00000   1.00000   1.00000         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.83333   0.83333   0.82222        22\n",
      "weighted avg    0.81818   0.81818   0.80606        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 17\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 51\n",
      "7 21 1\n",
      "12 36 2\n",
      "12 36 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.4199300624755712 \ttest: 0.8636363636363636 5.990363616695297\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.12945631329942237 \ttest: 0.8181818181818182 6.209359503317023\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.06447268021360514 \ttest: 0.8181818181818182 6.29343687167643\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.03894803361702757 \ttest: 0.8181818181818182 6.340533677392406\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.026204744543412026 \ttest: 0.8181818181818182 6.371212983555093\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.018895264169553914 \ttest: 0.8181818181818182 6.392931888402149\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.014300555650823957 \ttest: 0.8181818181818182 6.409154289314998\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.011217651012923432 \ttest: 0.8181818181818182 6.421737140162801\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.009045362069485271 \ttest: 0.8181818181818182 6.431775174929169\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0074552336490185415 \ttest: 0.8181818181818182 6.439959412149557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.00000   0.00000   0.00000         3\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    0.75000   0.75000   0.75000         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    0.60000   1.00000   0.75000         3\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.72500   0.79167   0.75000        22\n",
      "weighted avg    0.76364   0.81818   0.78409        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "8 24 1\n",
      "13 39 2\n",
      "14 42 3\n",
      "16 48 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.47685820589104067 \ttest: 0.8181818181818182 6.758425372930439\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.16251025923479412 \ttest: 0.8181818181818182 6.784757319153714\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.08256704686205434 \ttest: 0.8181818181818182 6.845204089471548\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.05004916257579155 \ttest: 0.8181818181818182 6.9002379601737385\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.03362700448686854 \ttest: 0.8181818181818182 6.94907555386597\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.024176856784600926 \ttest: 0.8181818181818182 6.993051374523416\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.018237422030274977 \ttest: 0.8181818181818182 7.03322581664196\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.014258757886610799 \ttest: 0.8181818181818182 7.0703326398984006\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.011461692390865131 \ttest: 0.8181818181818182 7.104887863884351\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.009419410489677317 \ttest: 0.8181818181818182 7.137268855085766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.66667   0.57143         3\n",
      "           1    1.00000   0.75000   0.85714         4\n",
      "           2    1.00000   0.75000   0.85714         4\n",
      "           3    0.75000   1.00000   0.85714         3\n",
      "           4    0.75000   0.75000   0.75000         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.83333   0.81944   0.81548        22\n",
      "weighted avg    0.85227   0.81818   0.82468        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "11 33 2\n",
      "13 39 3\n",
      "14 42 4\n",
      "15 45 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "17 51 9\n",
      "17 51 10\n",
      "17 51 11\n",
      "17 51 12\n",
      "17 51 13\n",
      "17 51 14\n",
      "17 51 15\n",
      "17 51 16\n",
      "17 51 17\n",
      "17 51 18\n",
      "17 51 19\n",
      "17 51 20\n",
      "17 51 21\n",
      "17 51 22\n",
      "17 51 23\n",
      "17 51 24\n",
      "17 51 25\n",
      "17 51 26\n",
      "17 51 27\n",
      "17 51 28\n",
      "17 51 29\n",
      "17 51 30\n",
      "17 51 31\n",
      "17 51 32\n",
      "17 51 33\n",
      "17 51 34\n",
      "17 51 35\n",
      "17 51 36\n",
      "17 51 37\n",
      "17 51 38\n",
      "17 51 39\n",
      "17 51 40\n",
      "17 51 41\n",
      "17 51 42\n",
      "17 51 43\n",
      "17 51 44\n",
      "17 51 45\n",
      "17 51 46\n",
      "17 51 47\n",
      "17 51 48\n",
      "17 51 49\n",
      "17 51 50\n",
      "17 51 51\n",
      "17 51 52\n",
      "17 51 53\n",
      "17 51 54\n",
      "17 51 55\n",
      "17 51 56\n",
      "17 51 57\n",
      "17 51 58\n",
      "17 51 59\n",
      "17 51 60\n",
      "17 51 61\n",
      "17 51 62\n",
      "17 51 63\n",
      "17 51 64\n",
      "17 51 65\n",
      "17 51 66\n",
      "17 51 67\n",
      "17 51 68\n",
      "17 51 69\n",
      "17 51 70\n",
      "17 51 71\n",
      "17 51 72\n",
      "17 51 73\n",
      "17 51 74\n",
      "17 51 75\n",
      "17 51 76\n",
      "17 51 77\n",
      "17 51 78\n",
      "17 51 79\n",
      "17 51 80\n",
      "17 51 81\n",
      "17 51 82\n",
      "17 51 83\n",
      "17 51 84\n",
      "17 51 85\n",
      "17 51 86\n",
      "17 51 87\n",
      "17 51 88\n",
      "17 51 89\n",
      "17 51 90\n",
      "17 51 91\n",
      "17 51 92\n",
      "17 51 93\n",
      "17 51 94\n",
      "17 51 95\n",
      "17 51 96\n",
      "17 51 97\n",
      "17 51 98\n",
      "17 51 99\n",
      "17 51 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.985647612928752 \ttest: 0.7727272727272727 6.250041550213588\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.43046272242870337 \ttest: 0.8636363636363636 5.741157404241807\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.24375119070137918 \ttest: 0.9090909090909091 5.571792571715918\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.15683780283730714 \ttest: 0.9090909090909091 5.496646613278163\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.10935961295437871 \ttest: 0.9090909090909091 5.460711485996156\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.08062067272263389 \ttest: 0.9090909090909091 5.44409607357411\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.06191526199574786 \ttest: 0.9090909090909091 5.437955777694019\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.04906164934050771 \ttest: 0.9090909090909091 5.437846358312909\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.03984912817116399 \ttest: 0.9090909090909091 5.441339597107737\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.03302023541979961 \ttest: 0.9090909090909091 5.447025252538128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.60000   1.00000   0.75000         3\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    1.00000   0.50000   0.66667         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    1.00000   1.00000   1.00000         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.93333   0.91667   0.90278        22\n",
      "weighted avg    0.94545   0.90909   0.90530        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 17\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 51\n",
      "8 24 1\n",
      "10 30 2\n",
      "13 39 3\n",
      "14 42 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.940983145423569 \ttest: 0.9545454545454546 4.245234495190354\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.3623723478975541 \ttest: 0.9090909090909091 3.683003378890954\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.19834313144417748 \ttest: 0.9090909090909091 3.5173681965698638\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.12645384639333035 \ttest: 0.9090909090909091 3.4513803798219302\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.08808237941147982 \ttest: 0.9090909090909091 3.423726664181416\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.06506113347816997 \ttest: 0.9090909090909091 3.413983548794862\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.05011706247542899 \ttest: 0.9090909090909091 3.4135912473329997\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.039845507763881446 \ttest: 0.9090909090909091 3.4184226069669967\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.032471359681513816 \ttest: 0.9090909090909091 3.426290765703177\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.026992392501329537 \ttest: 0.9090909090909091 3.4359557786968478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         9\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.75000   0.85714         4\n",
      "           1    0.75000   1.00000   0.85714         3\n",
      "           2    0.80000   1.00000   0.88889         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   1.00000   1.00000         4\n",
      "           5    1.00000   0.66667   0.80000         3\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.92500   0.90278   0.90053        22\n",
      "weighted avg    0.92955   0.90909   0.90707        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "11 33 2\n",
      "13 39 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "17 51 6\n",
      "17 51 7\n",
      "17 51 8\n",
      "17 51 9\n",
      "17 51 10\n",
      "17 51 11\n",
      "17 51 12\n",
      "17 51 13\n",
      "17 51 14\n",
      "17 51 15\n",
      "17 51 16\n",
      "17 51 17\n",
      "17 51 18\n",
      "17 51 19\n",
      "17 51 20\n",
      "17 51 21\n",
      "17 51 22\n",
      "17 51 23\n",
      "17 51 24\n",
      "17 51 25\n",
      "17 51 26\n",
      "17 51 27\n",
      "17 51 28\n",
      "17 51 29\n",
      "17 51 30\n",
      "17 51 31\n",
      "17 51 32\n",
      "17 51 33\n",
      "17 51 34\n",
      "17 51 35\n",
      "17 51 36\n",
      "17 51 37\n",
      "17 51 38\n",
      "17 51 39\n",
      "17 51 40\n",
      "17 51 41\n",
      "17 51 42\n",
      "17 51 43\n",
      "17 51 44\n",
      "17 51 45\n",
      "17 51 46\n",
      "17 51 47\n",
      "17 51 48\n",
      "17 51 49\n",
      "17 51 50\n",
      "17 51 51\n",
      "17 51 52\n",
      "17 51 53\n",
      "17 51 54\n",
      "17 51 55\n",
      "17 51 56\n",
      "17 51 57\n",
      "17 51 58\n",
      "17 51 59\n",
      "17 51 60\n",
      "17 51 61\n",
      "17 51 62\n",
      "17 51 63\n",
      "17 51 64\n",
      "17 51 65\n",
      "17 51 66\n",
      "17 51 67\n",
      "17 51 68\n",
      "17 51 69\n",
      "17 51 70\n",
      "17 51 71\n",
      "17 51 72\n",
      "17 51 73\n",
      "17 51 74\n",
      "17 51 75\n",
      "17 51 76\n",
      "17 51 77\n",
      "17 51 78\n",
      "17 51 79\n",
      "17 51 80\n",
      "17 51 81\n",
      "17 51 82\n",
      "17 51 83\n",
      "17 51 84\n",
      "17 51 85\n",
      "17 51 86\n",
      "17 51 87\n",
      "17 51 88\n",
      "17 51 89\n",
      "17 51 90\n",
      "17 51 91\n",
      "17 51 92\n",
      "17 51 93\n",
      "17 51 94\n",
      "17 51 95\n",
      "17 51 96\n",
      "17 51 97\n",
      "17 51 98\n",
      "17 51 99\n",
      "17 51 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.976237296907184 \ttest: 0.7727272727272727 6.259439693946916\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.42537445574778865 \ttest: 0.8636363636363636 5.747132878186801\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.24057742194732257 \ttest: 0.9090909090909091 5.578722774369148\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.1546782259781294 \ttest: 0.9090909090909091 5.504896352665792\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.10780277782455541 \ttest: 0.9090909090909091 5.470270917814595\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.07944917322244904 \ttest: 0.9090909090909091 5.454881766324133\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.061003873820214397 \ttest: 0.9090909090909091 5.449862392304428\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.04833354792385528 \ttest: 0.9090909090909091 5.450763555268719\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.03925475629580006 \ttest: 0.9090909090909091 5.455160073550841\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.03252627995867087 \ttest: 0.9090909090909091 5.461648763195967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.60000   1.00000   0.75000         3\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    1.00000   0.50000   0.66667         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    1.00000   1.00000   1.00000         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.93333   0.91667   0.90278        22\n",
      "weighted avg    0.94545   0.90909   0.90530        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 17\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 51\n",
      "8 24 1\n",
      "13 39 2\n",
      "14 42 3\n",
      "14 42 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.98 1.6021042307803706 \ttest: 0.7727272727272727 5.604151069544114\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.9607277849151893 \ttest: 0.8636363636363636 5.387604641187437\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.6533103668952431 \ttest: 0.8636363636363636 5.2055611840664495\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.47156989735582416 \ttest: 0.8636363636363636 5.058199039248736\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.3548428412917035 \ttest: 0.9090909090909091 4.9384478573925925\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.27573276628962795 \ttest: 0.9090909090909091 4.840295581328817\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.219874842199843 \ttest: 0.9090909090909091 4.759098935597396\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.17910533518273103 \ttest: 0.9090909090909091 4.691338256934236\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.14851634699547744 \ttest: 0.9090909090909091 4.634343335874275\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.12502378640675454 \ttest: 0.9090909090909091 4.586072081137965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.50000   0.66667         4\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    0.66667   1.00000   0.80000         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    1.00000   1.00000   1.00000         3\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.94444   0.91667   0.91111        22\n",
      "weighted avg    0.93939   0.90909   0.90303        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "11 33 2\n",
      "11 33 3\n",
      "14 42 4\n",
      "15 45 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 1.018006074770167 \ttest: 0.9090909090909091 2.6340317446488175\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.45354481260258606 \ttest: 0.9090909090909091 2.5439081386895985\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.25760723800043606 \ttest: 0.9090909090909091 2.552696779948349\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.165474365627007 \ttest: 0.9090909090909091 2.5793007780717536\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.11504862803213697 \ttest: 0.9090909090909091 2.6094074273745687\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.08455668708770192 \ttest: 0.9090909090909091 2.6390424563566945\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.06475099084212174 \ttest: 0.9090909090909091 2.667000573951304\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.05117324027919271 \ttest: 0.9090909090909091 2.692982783321913\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.041464595306776125 \ttest: 0.9090909090909091 2.7170121753796095\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.03428409893065998 \ttest: 0.9090909090909091 2.739224485523573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         9\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.75000   0.75000   0.75000         4\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    0.80000   1.00000   0.88889         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.66667   0.80000         3\n",
      "           5    1.00000   1.00000   1.00000         3\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.92500   0.90278   0.90648        22\n",
      "weighted avg    0.91818   0.90909   0.90707        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "12 36 2\n",
      "14 42 3\n",
      "15 45 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.98 1.5744675056892428 \ttest: 0.8181818181818182 6.597091433159825\n",
      "retrain  2 :\n",
      "\ttrain: 0.98 1.227202605082997 \ttest: 0.8636363636363636 5.992943966987641\n",
      "retrain  3 :\n",
      "\ttrain: 0.98 1.1252221601325187 \ttest: 0.8636363636363636 5.680890992854943\n",
      "retrain  4 :\n",
      "\ttrain: 0.98 1.0804175426915927 \ttest: 0.9090909090909091 5.464647911782255\n",
      "retrain  5 :\n",
      "\ttrain: 0.98 1.0564505740543166 \ttest: 0.9090909090909091 5.298390732455618\n",
      "retrain  6 :\n",
      "\ttrain: 0.98 1.0420083815624892 \ttest: 0.9090909090909091 5.163827958760015\n",
      "retrain  7 :\n",
      "\ttrain: 0.98 1.0325814005331138 \ttest: 0.9090909090909091 5.051486038151152\n",
      "retrain  8 :\n",
      "\ttrain: 0.98 1.026063914030545 \ttest: 0.9090909090909091 4.955684782644121\n",
      "retrain  9 :\n",
      "\ttrain: 0.98 1.0213579399148598 \ttest: 0.9090909090909091 4.872695550760355\n",
      "retrain  10 :\n",
      "\ttrain: 0.98 1.017842338840801 \ttest: 0.9090909090909091 4.799917656150034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.87500   0.93333         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    0.88889   1.00000   0.94118         8\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         9\n",
      "\n",
      "    accuracy                        0.98000        50\n",
      "   macro avg    0.98148   0.97917   0.97908        50\n",
      "weighted avg    0.98222   0.98000   0.97992        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.75000   0.75000   0.75000         4\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    1.00000   1.00000   1.00000         4\n",
      "           3    1.00000   1.00000   1.00000         3\n",
      "           4    0.75000   0.75000   0.75000         4\n",
      "           5    1.00000   1.00000   1.00000         3\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.91667   0.91667   0.91667        22\n",
      "weighted avg    0.90909   0.90909   0.90909        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "6 18 1\n",
      "11 33 2\n",
      "12 36 3\n",
      "13 39 4\n",
      "14 42 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.98 1.1207414001885814 \ttest: 0.7727272727272727 6.567133802085193\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.58380573283756 \ttest: 0.8181818181818182 6.193832844813757\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.356437366043906 \ttest: 0.8181818181818182 6.02417662163127\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.2372447434746058 \ttest: 0.8181818181818182 5.939565459924639\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.1680734979343994 \ttest: 0.8181818181818182 5.893496323566792\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.12481198849436331 \ttest: 0.8181818181818182 5.866731458255087\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.09613168765004257 \ttest: 0.8181818181818182 5.850449102810265\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.07621577004193927 \ttest: 0.8181818181818182 5.840230921347942\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.061856880103020956 \ttest: 0.8181818181818182 5.833699628257223\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0511797238220597 \ttest: 0.8181818181818182 5.829499697394349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.50000   0.66667         4\n",
      "           1    1.00000   0.75000   0.85714         4\n",
      "           2    0.50000   1.00000   0.66667         3\n",
      "           3    0.80000   1.00000   0.88889         4\n",
      "           4    1.00000   0.66667   0.80000         3\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.88333   0.81944   0.81323        22\n",
      "weighted avg    0.89545   0.81818   0.82049        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "8 24 1\n",
      "12 36 2\n",
      "13 39 3\n",
      "13 39 4\n",
      "14 42 5\n",
      "14 42 6\n",
      "14 42 7\n",
      "15 45 8\n",
      "15 45 9\n",
      "15 45 10\n",
      "15 45 11\n",
      "15 45 12\n",
      "15 45 13\n",
      "15 45 14\n",
      "15 45 15\n",
      "15 45 16\n",
      "15 45 17\n",
      "15 45 18\n",
      "15 45 19\n",
      "15 45 20\n",
      "15 45 21\n",
      "15 45 22\n",
      "15 45 23\n",
      "15 45 24\n",
      "15 45 25\n",
      "15 45 26\n",
      "15 45 27\n",
      "15 45 28\n",
      "15 45 29\n",
      "15 45 30\n",
      "15 45 31\n",
      "15 45 32\n",
      "15 45 33\n",
      "15 45 34\n",
      "15 45 35\n",
      "15 45 36\n",
      "15 45 37\n",
      "15 45 38\n",
      "15 45 39\n",
      "15 45 40\n",
      "15 45 41\n",
      "15 45 42\n",
      "15 45 43\n",
      "15 45 44\n",
      "15 45 45\n",
      "15 45 46\n",
      "15 45 47\n",
      "15 45 48\n",
      "15 45 49\n",
      "15 45 50\n",
      "15 45 51\n",
      "15 45 52\n",
      "15 45 53\n",
      "15 45 54\n",
      "15 45 55\n",
      "15 45 56\n",
      "15 45 57\n",
      "15 45 58\n",
      "15 45 59\n",
      "15 45 60\n",
      "15 45 61\n",
      "15 45 62\n",
      "15 45 63\n",
      "15 45 64\n",
      "15 45 65\n",
      "15 45 66\n",
      "15 45 67\n",
      "15 45 68\n",
      "15 45 69\n",
      "15 45 70\n",
      "15 45 71\n",
      "15 45 72\n",
      "15 45 73\n",
      "15 45 74\n",
      "15 45 75\n",
      "15 45 76\n",
      "15 45 77\n",
      "15 45 78\n",
      "15 45 79\n",
      "15 45 80\n",
      "15 45 81\n",
      "15 45 82\n",
      "15 45 83\n",
      "15 45 84\n",
      "15 45 85\n",
      "15 45 86\n",
      "15 45 87\n",
      "15 45 88\n",
      "15 45 89\n",
      "15 45 90\n",
      "15 45 91\n",
      "15 45 92\n",
      "15 45 93\n",
      "15 45 94\n",
      "15 45 95\n",
      "15 45 96\n",
      "15 45 97\n",
      "15 45 98\n",
      "15 45 99\n",
      "15 45 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.502221652594276 \ttest: 0.8636363636363636 5.824653917165362\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.19895302696595699 \ttest: 0.8181818181818182 5.515035270594629\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.11206839482843314 \ttest: 0.8181818181818182 5.51580241972794\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.07282375562256556 \ttest: 0.8181818181818182 5.570784752393697\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.051403553532983134 \ttest: 0.8181818181818182 5.636800284500897\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.038341730020614084 \ttest: 0.7727272727272727 5.702724475208911\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.02975783337330247 \ttest: 0.7727272727272727 5.7652799669886186\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.023800650642899292 \ttest: 0.7727272727272727 5.823607551294207\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.019490433318195027 \ttest: 0.7727272727272727 5.8776798910634565\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.016267285551315014 \ttest: 0.7727272727272727 5.9277594882074585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.33333   0.40000         3\n",
      "           1    1.00000   0.50000   0.66667         4\n",
      "           2    0.60000   1.00000   0.75000         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.75000   0.85714         4\n",
      "           5    0.66667   1.00000   0.80000         4\n",
      "\n",
      "    accuracy                        0.77273        22\n",
      "   macro avg    0.79444   0.76389   0.74563        22\n",
      "weighted avg    0.81667   0.77273   0.76115        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 15\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 45\n",
      "7 21 1\n",
      "11 33 2\n",
      "12 36 3\n",
      "13 39 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.5128140917053444 \ttest: 0.8181818181818182 6.157610785676705\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.1803603306535389 \ttest: 0.8636363636363636 5.731108302989353\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.0942202430707941 \ttest: 0.8636363636363636 5.555156932026829\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.05844962139526182 \ttest: 0.8636363636363636 5.461964833257849\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.040001437751562785 \ttest: 0.8636363636363636 5.406918797212099\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.029187210767079124 \ttest: 0.8636363636363636 5.372394626203367\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.022282470203051477 \ttest: 0.8636363636363636 5.349963184801872\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.017594830474012904 \ttest: 0.8636363636363636 5.335108873749854\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.014261427785790867 \ttest: 0.8636363636363636 5.325219479931204\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.011803430216144647 \ttest: 0.8636363636363636 5.3186960791484745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66667   0.50000   0.57143         4\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    0.66667   0.66667   0.66667         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    0.75000   1.00000   0.85714         3\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.86364        22\n",
      "   macro avg    0.84722   0.86111   0.84921        22\n",
      "weighted avg    0.85985   0.86364   0.85714        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "8 24 1\n",
      "11 33 2\n",
      "13 39 3\n",
      "14 42 4\n",
      "14 42 5\n",
      "14 42 6\n",
      "15 45 7\n",
      "15 45 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.6180045558347833 \ttest: 0.8636363636363636 3.618826481998536\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.21409080534839087 \ttest: 0.9090909090909091 3.864206512076512\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.11013415013292761 \ttest: 0.9090909090909091 3.949504135192869\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.0676611018755999 \ttest: 0.9090909090909091 3.9842978879312603\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.04604659104217778 \ttest: 0.9090909090909091 3.9996947696214944\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.033493346173644196 \ttest: 0.9090909090909091 4.0066569532935095\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.025527614678766603 \ttest: 0.9090909090909091 4.009665254671539\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.020141486639782623 \ttest: 0.9090909090909091 4.010740825885016\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.016321378246016933 \ttest: 0.9090909090909091 4.010859511031712\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.013509141317511596 \ttest: 0.9090909090909091 4.010515007815721\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66667   1.00000   0.80000         4\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    1.00000   0.66667   0.80000         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.75000   0.85714         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.90909        22\n",
      "   macro avg    0.94444   0.90278   0.90952        22\n",
      "weighted avg    0.93939   0.90909   0.91039        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "10 30 2\n",
      "14 42 3\n",
      "15 45 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.9983067303452905 \ttest: 0.8636363636363636 5.048149705715046\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.3640565011013239 \ttest: 0.8636363636363636 5.194269823306242\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.18961666237498573 \ttest: 0.8636363636363636 5.38523894517246\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.1168209838600581 \ttest: 0.8636363636363636 5.547853969276408\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.07938418474493639 \ttest: 0.8636363636363636 5.683160143740047\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.05754309750449715 \ttest: 0.8636363636363636 5.7969239597778195\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.04366906783034405 \ttest: 0.8636363636363636 5.8940234333849\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0342969196070456 \ttest: 0.8636363636363636 5.978091933701719\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.02766377478771216 \ttest: 0.8636363636363636 6.051794910107797\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.022794202073860258 \ttest: 0.8636363636363636 6.117109192816795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.60000   1.00000   0.75000         3\n",
      "           1    0.75000   1.00000   0.85714         3\n",
      "           2    1.00000   0.75000   0.85714         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   0.75000   0.85714         4\n",
      "           5    1.00000   0.75000   0.85714         4\n",
      "\n",
      "    accuracy                        0.86364        22\n",
      "   macro avg    0.89167   0.87500   0.86310        22\n",
      "weighted avg    0.91136   0.86364   0.86851        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "7 21 1\n",
      "10 30 2\n",
      "14 42 3\n",
      "16 48 4\n",
      "16 48 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.4244005490088718 \ttest: 0.7272727272727273 6.7553650775762595\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.15051152574482513 \ttest: 0.7727272727272727 6.316150867132537\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.08101549967951255 \ttest: 0.8181818181818182 6.190273555735837\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.05146570973660787 \ttest: 0.8181818181818182 6.136102013339807\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.035866951061850595 \ttest: 0.8181818181818182 6.110120779187996\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.026544666738283745 \ttest: 0.8181818181818182 6.097722519002479\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.02049753483810574 \ttest: 0.8181818181818182 6.0925908121294645\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.01633794442927436 \ttest: 0.8181818181818182 6.091593145984443\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.013347270900253172 \ttest: 0.8181818181818182 6.093034077361482\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.011121193997899062 \ttest: 0.8181818181818182 6.0959383138285546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.40000   0.66667   0.50000         3\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "           2    1.00000   0.50000   0.66667         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    0.75000   0.75000   0.75000         4\n",
      "           5    1.00000   1.00000   1.00000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.85833   0.81944   0.81944        22\n",
      "weighted avg    0.87273   0.81818   0.82576        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "8 24 1\n",
      "14 42 2\n",
      "15 45 3\n",
      "15 45 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "15 45 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.49546895368206767 \ttest: 0.7727272727272727 10.014245386556315\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.20708348739405968 \ttest: 0.7272727272727273 10.262016726522592\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.11380651791286836 \ttest: 0.7272727272727273 10.430698060020756\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.07202779745472668 \ttest: 0.7272727272727273 10.556119277539842\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.04972520541576099 \ttest: 0.7272727272727273 10.653994203317648\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.03641191935479576 \ttest: 0.6818181818181818 10.733110154359675\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.027823957933683946 \ttest: 0.6818181818181818 10.798812041759032\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.021959113754992435 \ttest: 0.6818181818181818 10.854542717380859\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.017774883480911458 \ttest: 0.6818181818181818 10.902624761949646\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.014684465261273743 \ttest: 0.6818181818181818 10.94468647122568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhouz\\anaconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zhouz\\anaconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\zhouz\\anaconda3\\envs\\BStackGP\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.00000   0.00000   0.00000         4\n",
      "           1    1.00000   0.33333   0.50000         3\n",
      "           2    0.50000   1.00000   0.66667         3\n",
      "           3    0.80000   1.00000   0.88889         4\n",
      "           4    0.60000   0.75000   0.66667         4\n",
      "           5    0.80000   1.00000   0.88889         4\n",
      "\n",
      "    accuracy                        0.68182        22\n",
      "   macro avg    0.61667   0.68056   0.60185        22\n",
      "weighted avg    0.60455   0.68182   0.60354        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "9 27 1\n",
      "14 42 2\n",
      "14 42 3\n",
      "15 45 4\n",
      "15 45 5\n",
      "15 45 6\n",
      "15 45 7\n",
      "15 45 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.4728609983181895 \ttest: 0.7727272727272727 5.414656190047451\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.150923300051632 \ttest: 0.7727272727272727 5.7788151611551175\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.07518623504774852 \ttest: 0.7727272727272727 5.931497259009118\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.04537720121082929 \ttest: 0.8181818181818182 6.014809387785659\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.03048370911635591 \ttest: 0.8181818181818182 6.068261228669574\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.0219372428588557 \ttest: 0.8181818181818182 6.106193340513468\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.016565590161411195 \ttest: 0.8181818181818182 6.134998867656425\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.012963589718418343 \ttest: 0.8181818181818182 6.157960012576525\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.010428044628383117 \ttest: 0.8181818181818182 6.176938352503766\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.008574333273763779 \ttest: 0.8181818181818182 6.1930702961026025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         9\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.60000   1.00000   0.75000         3\n",
      "           1    1.00000   0.50000   0.66667         4\n",
      "           2    1.00000   0.33333   0.50000         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    1.00000   1.00000   1.00000         4\n",
      "           5    0.66667   1.00000   0.80000         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.87778   0.80556   0.78611        22\n",
      "weighted avg    0.88485   0.81818   0.80076        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "8 24 1\n",
      "12 36 2\n",
      "13 39 3\n",
      "13 39 4\n",
      "13 39 5\n",
      "14 42 6\n",
      "14 42 7\n",
      "15 45 8\n",
      "15 45 9\n",
      "15 45 10\n",
      "15 45 11\n",
      "15 45 12\n",
      "15 45 13\n",
      "15 45 14\n",
      "15 45 15\n",
      "15 45 16\n",
      "15 45 17\n",
      "15 45 18\n",
      "15 45 19\n",
      "15 45 20\n",
      "15 45 21\n",
      "15 45 22\n",
      "15 45 23\n",
      "15 45 24\n",
      "15 45 25\n",
      "15 45 26\n",
      "15 45 27\n",
      "15 45 28\n",
      "15 45 29\n",
      "15 45 30\n",
      "15 45 31\n",
      "15 45 32\n",
      "15 45 33\n",
      "15 45 34\n",
      "15 45 35\n",
      "15 45 36\n",
      "15 45 37\n",
      "15 45 38\n",
      "15 45 39\n",
      "15 45 40\n",
      "15 45 41\n",
      "15 45 42\n",
      "15 45 43\n",
      "15 45 44\n",
      "15 45 45\n",
      "15 45 46\n",
      "15 45 47\n",
      "15 45 48\n",
      "15 45 49\n",
      "15 45 50\n",
      "15 45 51\n",
      "15 45 52\n",
      "15 45 53\n",
      "15 45 54\n",
      "15 45 55\n",
      "15 45 56\n",
      "15 45 57\n",
      "15 45 58\n",
      "15 45 59\n",
      "15 45 60\n",
      "15 45 61\n",
      "15 45 62\n",
      "15 45 63\n",
      "15 45 64\n",
      "15 45 65\n",
      "15 45 66\n",
      "15 45 67\n",
      "15 45 68\n",
      "15 45 69\n",
      "15 45 70\n",
      "15 45 71\n",
      "15 45 72\n",
      "15 45 73\n",
      "15 45 74\n",
      "15 45 75\n",
      "15 45 76\n",
      "15 45 77\n",
      "15 45 78\n",
      "15 45 79\n",
      "15 45 80\n",
      "15 45 81\n",
      "15 45 82\n",
      "15 45 83\n",
      "15 45 84\n",
      "15 45 85\n",
      "15 45 86\n",
      "15 45 87\n",
      "15 45 88\n",
      "15 45 89\n",
      "15 45 90\n",
      "15 45 91\n",
      "15 45 92\n",
      "15 45 93\n",
      "15 45 94\n",
      "15 45 95\n",
      "15 45 96\n",
      "15 45 97\n",
      "15 45 98\n",
      "15 45 99\n",
      "15 45 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.7943291625191851 \ttest: 0.8181818181818182 6.399836624231346\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.27350155508457025 \ttest: 0.8181818181818182 6.493130646538527\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.14215692185568507 \ttest: 0.8181818181818182 6.56321529160618\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.0889035717008785 \ttest: 0.8181818181818182 6.617906330680486\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.06151207542992194 \ttest: 0.8181818181818182 6.663066967015705\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.04537154427990032 \ttest: 0.8181818181818182 6.701604511885063\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.03498505315440874 \ttest: 0.8181818181818182 6.735228204273927\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.02787370633120506 \ttest: 0.8181818181818182 6.765049008194115\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.022774512238217255 \ttest: 0.8181818181818182 6.791835723438327\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.018984663322569813 \ttest: 0.8181818181818182 6.816144297554844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         9\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66667   0.50000   0.57143         4\n",
      "           1    0.75000   1.00000   0.85714         3\n",
      "           2    0.75000   1.00000   0.85714         3\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    0.75000   0.75000   0.75000         4\n",
      "           5    1.00000   0.75000   0.85714         4\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.81944   0.83333   0.81548        22\n",
      "weighted avg    0.82576   0.81818   0.81169        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 15\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 45\n",
      "5 15 1\n",
      "9 27 2\n",
      "12 36 3\n",
      "14 42 4\n",
      "15 45 5\n",
      "16 48 6\n",
      "16 48 7\n",
      "16 48 8\n",
      "16 48 9\n",
      "16 48 10\n",
      "16 48 11\n",
      "16 48 12\n",
      "16 48 13\n",
      "16 48 14\n",
      "16 48 15\n",
      "16 48 16\n",
      "16 48 17\n",
      "16 48 18\n",
      "16 48 19\n",
      "16 48 20\n",
      "16 48 21\n",
      "16 48 22\n",
      "16 48 23\n",
      "16 48 24\n",
      "16 48 25\n",
      "16 48 26\n",
      "16 48 27\n",
      "16 48 28\n",
      "16 48 29\n",
      "16 48 30\n",
      "16 48 31\n",
      "16 48 32\n",
      "16 48 33\n",
      "16 48 34\n",
      "16 48 35\n",
      "16 48 36\n",
      "16 48 37\n",
      "16 48 38\n",
      "16 48 39\n",
      "16 48 40\n",
      "16 48 41\n",
      "16 48 42\n",
      "16 48 43\n",
      "16 48 44\n",
      "16 48 45\n",
      "16 48 46\n",
      "16 48 47\n",
      "16 48 48\n",
      "16 48 49\n",
      "16 48 50\n",
      "16 48 51\n",
      "16 48 52\n",
      "16 48 53\n",
      "16 48 54\n",
      "16 48 55\n",
      "16 48 56\n",
      "16 48 57\n",
      "16 48 58\n",
      "16 48 59\n",
      "16 48 60\n",
      "16 48 61\n",
      "16 48 62\n",
      "16 48 63\n",
      "16 48 64\n",
      "16 48 65\n",
      "16 48 66\n",
      "16 48 67\n",
      "16 48 68\n",
      "16 48 69\n",
      "16 48 70\n",
      "16 48 71\n",
      "16 48 72\n",
      "16 48 73\n",
      "16 48 74\n",
      "16 48 75\n",
      "16 48 76\n",
      "16 48 77\n",
      "16 48 78\n",
      "16 48 79\n",
      "16 48 80\n",
      "16 48 81\n",
      "16 48 82\n",
      "16 48 83\n",
      "16 48 84\n",
      "16 48 85\n",
      "16 48 86\n",
      "16 48 87\n",
      "16 48 88\n",
      "16 48 89\n",
      "16 48 90\n",
      "16 48 91\n",
      "16 48 92\n",
      "16 48 93\n",
      "16 48 94\n",
      "16 48 95\n",
      "16 48 96\n",
      "16 48 97\n",
      "16 48 98\n",
      "16 48 99\n",
      "16 48 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.7007035404493416 \ttest: 0.7727272727272727 7.511456724144342\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.2692084193452099 \ttest: 0.7727272727272727 7.510751525905347\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.1461982766450444 \ttest: 0.7727272727272727 7.4903834415342505\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.0931497566503118 \ttest: 0.7727272727272727 7.466668975769851\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.06506984876128111 \ttest: 0.7727272727272727 7.443265175575507\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.048267653373011477 \ttest: 0.7727272727272727 7.421369740614573\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.03735472990440064 \ttest: 0.7727272727272727 7.401264242427421\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.029837174686208175 \ttest: 0.7727272727272727 7.382898798754105\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.024423517572616505 \ttest: 0.7727272727272727 7.3661171228676\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.02038720416391896 \ttest: 0.7727272727272727 7.35074138401195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   1.00000   1.00000         8\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.50000   0.25000   0.33333         4\n",
      "           1    0.75000   1.00000   0.85714         3\n",
      "           2    0.60000   0.75000   0.66667         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    0.75000   1.00000   0.85714         3\n",
      "           5    1.00000   0.75000   0.85714         4\n",
      "\n",
      "    accuracy                        0.77273        22\n",
      "   macro avg    0.76667   0.79167   0.76190        22\n",
      "weighted avg    0.76818   0.77273   0.75325        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 16\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 48\n",
      "6 18 1\n",
      "10 30 2\n",
      "11 33 3\n",
      "15 45 4\n",
      "15 45 5\n",
      "16 48 6\n",
      "17 51 7\n",
      "17 51 8\n",
      "17 51 9\n",
      "17 51 10\n",
      "17 51 11\n",
      "17 51 12\n",
      "17 51 13\n",
      "17 51 14\n",
      "17 51 15\n",
      "17 51 16\n",
      "17 51 17\n",
      "17 51 18\n",
      "17 51 19\n",
      "17 51 20\n",
      "17 51 21\n",
      "17 51 22\n",
      "17 51 23\n",
      "17 51 24\n",
      "17 51 25\n",
      "17 51 26\n",
      "17 51 27\n",
      "17 51 28\n",
      "17 51 29\n",
      "17 51 30\n",
      "17 51 31\n",
      "17 51 32\n",
      "17 51 33\n",
      "17 51 34\n",
      "17 51 35\n",
      "17 51 36\n",
      "17 51 37\n",
      "17 51 38\n",
      "17 51 39\n",
      "17 51 40\n",
      "17 51 41\n",
      "17 51 42\n",
      "17 51 43\n",
      "17 51 44\n",
      "17 51 45\n",
      "17 51 46\n",
      "17 51 47\n",
      "17 51 48\n",
      "17 51 49\n",
      "17 51 50\n",
      "17 51 51\n",
      "17 51 52\n",
      "17 51 53\n",
      "17 51 54\n",
      "17 51 55\n",
      "17 51 56\n",
      "17 51 57\n",
      "17 51 58\n",
      "17 51 59\n",
      "17 51 60\n",
      "17 51 61\n",
      "17 51 62\n",
      "17 51 63\n",
      "17 51 64\n",
      "17 51 65\n",
      "17 51 66\n",
      "17 51 67\n",
      "17 51 68\n",
      "17 51 69\n",
      "17 51 70\n",
      "17 51 71\n",
      "17 51 72\n",
      "17 51 73\n",
      "17 51 74\n",
      "17 51 75\n",
      "17 51 76\n",
      "17 51 77\n",
      "17 51 78\n",
      "17 51 79\n",
      "17 51 80\n",
      "17 51 81\n",
      "17 51 82\n",
      "17 51 83\n",
      "17 51 84\n",
      "17 51 85\n",
      "17 51 86\n",
      "17 51 87\n",
      "17 51 88\n",
      "17 51 89\n",
      "17 51 90\n",
      "17 51 91\n",
      "17 51 92\n",
      "17 51 93\n",
      "17 51 94\n",
      "17 51 95\n",
      "17 51 96\n",
      "17 51 97\n",
      "17 51 98\n",
      "17 51 99\n",
      "17 51 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.8819069817321565 \ttest: 0.8181818181818182 7.32808473645454\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.3379079623785317 \ttest: 0.8181818181818182 7.283955740842389\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.18925044417944603 \ttest: 0.8181818181818182 7.283746244152263\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.12278787707074124 \ttest: 0.8181818181818182 7.294897113669508\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.08671712950467954 \ttest: 0.8181818181818182 7.309054779076097\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.06478455138615154 \ttest: 0.8181818181818182 7.323825920392231\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.05038768755987084 \ttest: 0.8181818181818182 7.338420692060871\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.04039772949968645 \ttest: 0.8181818181818182 7.352563959983144\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.033166207701085124 \ttest: 0.8181818181818182 7.3661684865679895\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.02775383094516972 \ttest: 0.8181818181818182 7.379221161167704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000         8\n",
      "           1    1.00000   1.00000   1.00000         9\n",
      "           2    1.00000   1.00000   1.00000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   1.00000   1.00000         9\n",
      "\n",
      "    accuracy                        1.00000        50\n",
      "   macro avg    1.00000   1.00000   1.00000        50\n",
      "weighted avg    1.00000   1.00000   1.00000        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.60000   0.75000   0.66667         4\n",
      "           1    0.75000   1.00000   0.85714         3\n",
      "           2    1.00000   1.00000   1.00000         4\n",
      "           3    1.00000   1.00000   1.00000         4\n",
      "           4    0.66667   0.50000   0.57143         4\n",
      "           5    1.00000   0.66667   0.80000         3\n",
      "\n",
      "    accuracy                        0.81818        22\n",
      "   macro avg    0.83611   0.81944   0.81587        22\n",
      "weighted avg    0.83258   0.81818   0.81472        22\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 17\n",
      "Average of depth: 1.0\n",
      "Number of nodes: 51\n"
     ]
    }
   ],
   "source": [
    "def fit_trees():\n",
    "    learning_rate=10\n",
    "    max_depth=3\n",
    "    bins=5\n",
    "    lam=10\n",
    "\n",
    "    gtgp = GTGP(learning_rate=learning_rate,max_depth=max_depth,bins=bins,lam=lam)\n",
    "\n",
    "    total_size=10\n",
    "    elite_size = 10\n",
    "    epoch= 100\n",
    "    gp_epoch= 1\n",
    "    verbose = 1\n",
    "    tolerance= 0.1\n",
    "\n",
    "    gtgp.fit(X_train,y_train,total_size=total_size,elite_size = elite_size,epoch=epoch,gp_epoch=gp_epoch,tolerance=tolerance,verbose=verbose)\n",
    "\n",
    "    retrain_epoch= 10\n",
    "    alpha=0\n",
    "    beta=1\n",
    "    gammer=0\n",
    "\n",
    "    verbose=1\n",
    "    gtgp.lam = 10\n",
    "    gtgp.retrain_estimators(X_test,y_test,retrain_epoch=retrain_epoch,alpha=alpha,beta=beta,gammer=gammer,verbose=verbose)\n",
    "\n",
    "    return gtgp\n",
    "\n",
    "with open('./benchmark/'+dataset+'.csv','w') as f:\n",
    "        f.writelines(\"\")\n",
    "\n",
    "for i in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "    y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "    gtgp = fit_trees()\n",
    "\n",
    "    print(classification_report(y_train,np.argmax(gtgp.train_p,axis=1),digits=5))\n",
    "    print(classification_report(y_test,np.argmax(gtgp.test_p,axis=1),digits=5))\n",
    "\n",
    "    num_trees,depth,num_nodes = gtgp.print_model()\n",
    "\n",
    "    train_acc = accuracy_score(y_train,np.argmax(gtgp.train_p,axis=1))\n",
    "    test_acc = accuracy_score(y_test,np.argmax(gtgp.test_p,axis=1))\n",
    "    train_f1 = f1_score(y_train,np.argmax(gtgp.train_p,axis=1),average='macro')\n",
    "    test_f1 = f1_score(y_test,np.argmax(gtgp.test_p,axis=1),average='macro')\n",
    "\n",
    "    \n",
    "    # y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "    # train_f1 = mean_squared_error(y_train_one_hot.toarray(),gtgp.train_p)\n",
    "    # test_f1 = mean_squared_error(y_test_one_hot.toarray(),gtgp.test_p)\n",
    "\n",
    "\n",
    "    # train_f1 = roc_auc_score(gtgp.y_one_hot.toarray(),(gtgp.train_p.T/np.sum(gtgp.train_p,axis=1)).T)\n",
    "    # test_f1 = roc_auc_score(y_test_one_hot.toarray(),(gtgp.test_p.T/np.sum(gtgp.test_p,axis=1)).T)\n",
    "\n",
    "    # train_f1 = roc_auc_score(gtgp.y_one_hot.toarray(),gtgp.train_p)\n",
    "    # test_f1 = roc_auc_score(y_test_one_hot.toarray(),gtgp.test_p)\n",
    "\n",
    "\n",
    "    with open('./benchmark/'+dataset+'.csv','a') as f:\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e91a5",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ed15d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "661630ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    num_trees = 1\n",
    "    depth = clf.tree_.max_depth\n",
    "    num_nodes = clf.tree_.node_count\n",
    "\n",
    "    train_acc = accuracy_score(y_train,clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test,clf.predict(X_test))\n",
    "    train_f1 = f1_score(y_train,clf.predict(X_train),average='macro')\n",
    "    test_f1 = f1_score(y_test,clf.predict(X_test),average='macro')\n",
    "\n",
    "    # y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "    # train_roc = roc_auc_score(y_train_one_hot.toarray(),clf.predict_proba(X_train))\n",
    "    # test_roc = roc_auc_score(y_test_one_hot.toarray(),clf.predict_proba(X_test))\n",
    "\n",
    "    with open('./benchmark_DC/'+dataset+'.csv','a') as f:\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afbae1",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28fa8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acfdd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./benchmark_xgb/'+dataset+'.csv','w') as f:\n",
    "    for i in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "        # xgb = xgboost.XGBClassifier(n_estimators=100,max_depth=6)\n",
    "        # xgb = xgboost.XGBClassifier(min_child_weight=0,max_depth=4)\n",
    "        xgb = xgboost.XGBClassifier(n_estimators=10)\n",
    "        xgb.fit(X_train,y_train)\n",
    "\n",
    "        import json\n",
    "\n",
    "        def item_generator(json_input, lookup_key):\n",
    "            if isinstance(json_input, dict):\n",
    "                for k, v in json_input.items():\n",
    "                    if k == lookup_key:\n",
    "                        yield v\n",
    "                    else:\n",
    "                        yield from item_generator(v, lookup_key)\n",
    "            elif isinstance(json_input, list):\n",
    "                for item in json_input:\n",
    "                    yield from item_generator(item, lookup_key)\n",
    "\n",
    "        def tree_depth(json_text):\n",
    "            json_input = json.loads(json_text)\n",
    "            depths = list(item_generator(json_input, 'depth'))\n",
    "            return max(depths) + 1 if len(depths) != 0 else 1\n",
    "\n",
    "        train_acc = accuracy_score(y_train,xgb.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test,xgb.predict(X_test))\n",
    "\n",
    "        train_f1 = f1_score(y_train,xgb.predict(X_train),average='macro')\n",
    "        test_f1 = f1_score(y_test,xgb.predict(X_test),average='macro')\n",
    "\n",
    "        y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        # train_f1 = roc_auc_score(y_train_one_hot.toarray(),xgb.predict_proba(X_train))\n",
    "        # test_f1 = roc_auc_score(y_test_one_hot.toarray(),xgb.predict_proba(X_test))\n",
    "\n",
    "        booster = xgb.get_booster()\n",
    "\n",
    "        tree_df = booster.trees_to_dataframe()\n",
    "        depths = [tree_depth(x) for x in booster.get_dump(dump_format = \"json\")]\n",
    "        num_trees = len(depths)\n",
    "        depth = np.average(depths)\n",
    "        num_nodes = len(tree_df)\n",
    "\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc363f99",
   "metadata": {},
   "source": [
    "# GDBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e6d3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef69c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./benchmark_GBDT/'+dataset+'.csv','w') as f:\n",
    "    for i in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "        # clf = GradientBoostingClassifier(n_estimators=1000,max_depth=1)\n",
    "        clf = GradientBoostingClassifier(n_estimators=1000)\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        train_acc = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test,clf.predict(X_test))\n",
    "        train_f1 = f1_score(y_train,clf.predict(X_train),average='macro')\n",
    "        test_f1 = f1_score(y_test,clf.predict(X_test),average='macro')\n",
    "\n",
    "        \n",
    "        # y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        # train_f1 = mean_squared_error(y_train_one_hot.toarray(),clf.predict_proba(X_train))\n",
    "        # test_f1 = mean_squared_error(y_test_one_hot.toarray(),clf.predict_proba(X_test))\n",
    "        \n",
    "        # y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        # train_f1 = roc_auc_score(y_train_one_hot.toarray(),clf.predict_proba(X_train))\n",
    "        # test_f1 = roc_auc_score(y_test_one_hot.toarray(),clf.predict_proba(X_test))\n",
    "\n",
    "\n",
    "        num_trees = len([ est for ests in clf.estimators_ for est in ests])\n",
    "        depth = np.average([ max(1,est.tree_.max_depth) for ests in clf.estimators_ for est in ests])\n",
    "        num_nodes = sum([ est.tree_.node_count for ests in clf.estimators_ for est in ests])\n",
    "\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ef54f",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8623bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3edfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./benchmark_RF/'+dataset+'.csv','w') as f:\n",
    "    for i in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "        # rfc = RandomForestClassifier(n_estimators=100)\n",
    "        rfc = RandomForestClassifier(n_estimators=1000)\n",
    "        rfc.fit(X_train,y_train)\n",
    "\n",
    "        train_acc = accuracy_score(y_train,rfc.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test,rfc.predict(X_test))\n",
    "        train_f1 = f1_score(y_train,rfc.predict(X_train),average='macro')\n",
    "        test_f1 = f1_score(y_test,rfc.predict(X_test),average='macro')\n",
    "        \n",
    "        # y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        # train_f1 = roc_auc_score(y_train_one_hot.toarray(),rfc.predict_proba(X_train))\n",
    "        # test_f1 = roc_auc_score(y_test_one_hot.toarray(),rfc.predict_proba(X_test))\n",
    "\n",
    "        num_trees = len(rfc.estimators_)\n",
    "        depth = np.average([est.tree_.max_depth for est in rfc.estimators_])\n",
    "        num_nodes = sum([est.tree_.node_count for est in rfc.estimators_])\n",
    "\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3fc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BStackGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "598cefc26d6e5a65b2978c65314d0610ea9dfe34c7d989c4b6d2528d500ccb7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
