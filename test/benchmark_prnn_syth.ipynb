{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8bc990-ebc5-46bd-bab4-f4091ce5b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../script/\")\n",
    "\n",
    "import Functions\n",
    "from Engine import Engine\n",
    "from GTGP import GTGP\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import importlib\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbad47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    print(roc_auc_score(true_y,prob[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114de92",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feedaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prnn_synth.tsv\",delimiter='\\t')\n",
    "X = df.iloc[:,:-1].to_numpy().astype(\"float\")\n",
    "y = df.iloc[:,-1].to_numpy().astype(\"int\")\n",
    "\n",
    "seeds = [10086, 200,500,30506,30405,30420,10056,7059,40965,5398,869543,83491,823190,\n",
    "         48392,2810,48392,3498210,483902,859032,12890,538920,86954,54309,6504,9840,\n",
    "         219805,548,2981,432890,5438908,219094,5843902,60854,979,12890,2108,4093]\n",
    "train_size = 0.7\n",
    "dataset = 'prnn_synth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9290c7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    125\n",
       "1    125\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2341bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y,yt):\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = le.fit_transform(y)\n",
    "    y_one_hot = ohe.fit_transform(y_train.reshape(-1,1))\n",
    "    \n",
    "    y_test = le.transform(yt)\n",
    "    yt_one_hot = ohe.transform(y_test.reshape(-1,1))\n",
    "    \n",
    "    return y_train,y_test,y_one_hot,yt_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd30c5",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f418f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 61 1\n",
      "26 168 2\n",
      "29 187 3\n",
      "34 234 4\n",
      "38 266 5\n",
      "42 294 6\n",
      "45 321 7\n",
      "52 374 8\n",
      "58 428 9\n",
      "59 435 10\n",
      "61 455 11\n",
      "63 467 12\n",
      "65 487 13\n",
      "69 515 14\n",
      "75 565 15\n",
      "76 572 16\n",
      "77 583 17\n",
      "78 594 18\n",
      "82 624 19\n",
      "84 642 20\n",
      "87 667 21\n",
      "88 666 22\n",
      "90 680 23\n",
      "90 674 24\n",
      "93 703 25\n",
      "97 737 26\n",
      "99 751 27\n",
      "100 760 28\n",
      "103 797 29\n",
      "104 808 30\n",
      "107 833 31\n",
      "107 829 32\n",
      "109 839 33\n",
      "109 839 34\n",
      "111 847 35\n",
      "112 856 36\n",
      "112 856 37\n",
      "113 861 38\n",
      "116 894 39\n",
      "117 909 40\n",
      "117 905 41\n",
      "117 905 42\n",
      "117 895 43\n",
      "116 884 44\n",
      "116 880 45\n",
      "117 889 46\n",
      "117 885 47\n",
      "117 885 48\n",
      "120 918 49\n",
      "120 916 50\n",
      "120 914 51\n",
      "121 925 52\n",
      "121 925 53\n",
      "123 951 54\n",
      "124 960 55\n",
      "126 978 56\n",
      "125 959 57\n",
      "125 953 58\n",
      "125 951 59\n",
      "127 969 60\n",
      "127 969 61\n",
      "127 969 62\n",
      "128 980 63\n",
      "128 978 64\n",
      "131 1003 65\n",
      "131 995 66\n",
      "132 1004 67\n",
      "133 1013 68\n",
      "133 1009 69\n",
      "135 1025 70\n",
      "135 1023 71\n",
      "136 1026 72\n",
      "136 1026 73\n",
      "138 1040 74\n",
      "138 1036 75\n",
      "138 1032 76\n",
      "138 1030 77\n",
      "139 1039 78\n",
      "139 1039 79\n",
      "139 1039 80\n",
      "140 1050 81\n",
      "140 1050 82\n",
      "143 1079 83\n",
      "143 1077 84\n",
      "143 1077 85\n",
      "144 1092 86\n",
      "144 1092 87\n",
      "147 1125 88\n",
      "148 1132 89\n",
      "149 1139 90\n",
      "150 1146 91\n",
      "150 1146 92\n",
      "150 1140 93\n",
      "150 1140 94\n",
      "150 1140 95\n",
      "150 1140 96\n",
      "151 1151 97\n",
      "152 1158 98\n",
      "152 1158 99\n",
      "152 1158 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.2846777295703645 \ttest: 0.8933333333333333 13.136021035845278\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.08220947274847294 \ttest: 0.8933333333333333 13.062814410628857\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.03967899588000278 \ttest: 0.9066666666666666 13.007290563069509\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.023728346657749692 \ttest: 0.9066666666666666 12.97213466097184\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.01594420017875368 \ttest: 0.9066666666666666 12.95013686548377\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.011523559495160273 \ttest: 0.9066666666666666 12.936208612792612\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.008754088554274682 \ttest: 0.9066666666666666 12.927366491458775\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.006895710124649207 \ttest: 0.9066666666666666 12.921854207819681\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.005583786401351004 \ttest: 0.9066666666666666 12.918599096270313\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.00462075672565147 \ttest: 0.9066666666666666 12.916918831533778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.87805   0.94737   0.91139        38\n",
      "           1    0.94118   0.86486   0.90141        37\n",
      "\n",
      "    accuracy                        0.90667        75\n",
      "   macro avg    0.90961   0.90612   0.90640        75\n",
      "weighted avg    0.90919   0.90667   0.90647        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 152\n",
      "Average of depth: 2.289473684210526\n",
      "Number of nodes: 1158\n",
      "15 81 1\n",
      "24 152 2\n",
      "26 168 3\n",
      "31 199 4\n",
      "36 238 5\n",
      "36 238 6\n",
      "40 274 7\n",
      "48 340 8\n",
      "53 379 9\n",
      "54 382 10\n",
      "58 420 11\n",
      "63 461 12\n",
      "66 496 13\n",
      "68 512 14\n",
      "70 530 15\n",
      "71 537 16\n",
      "75 575 17\n",
      "75 571 18\n",
      "78 602 19\n",
      "80 614 20\n",
      "83 641 21\n",
      "86 664 22\n",
      "87 669 23\n",
      "88 676 24\n",
      "89 683 25\n",
      "90 678 26\n",
      "92 700 27\n",
      "92 700 28\n",
      "97 745 29\n",
      "98 758 30\n",
      "98 754 31\n",
      "98 752 32\n",
      "99 759 33\n",
      "100 754 34\n",
      "101 763 35\n",
      "102 772 36\n",
      "102 772 37\n",
      "103 781 38\n",
      "104 788 39\n",
      "107 827 40\n",
      "107 827 41\n",
      "108 840 42\n",
      "110 864 43\n",
      "112 870 44\n",
      "112 870 45\n",
      "113 869 46\n",
      "114 880 47\n",
      "115 891 48\n",
      "117 911 49\n",
      "117 909 50\n",
      "119 931 51\n",
      "120 938 52\n",
      "124 982 53\n",
      "124 980 54\n",
      "124 978 55\n",
      "124 966 56\n",
      "125 975 57\n",
      "127 993 58\n",
      "127 983 59\n",
      "127 973 60\n",
      "129 979 61\n",
      "132 1000 62\n",
      "133 1003 63\n",
      "133 999 64\n",
      "134 1008 65\n",
      "134 1008 66\n",
      "135 1013 67\n",
      "136 1024 68\n",
      "138 1042 69\n",
      "139 1047 70\n",
      "142 1082 71\n",
      "144 1104 72\n",
      "144 1102 73\n",
      "147 1131 74\n",
      "148 1140 75\n",
      "148 1138 76\n",
      "148 1138 77\n",
      "150 1164 78\n",
      "153 1195 79\n",
      "153 1195 80\n",
      "153 1195 81\n",
      "154 1204 82\n",
      "154 1204 83\n",
      "155 1215 84\n",
      "155 1215 85\n",
      "155 1215 86\n",
      "155 1215 87\n",
      "155 1215 88\n",
      "155 1215 89\n",
      "156 1226 90\n",
      "156 1226 91\n",
      "156 1224 92\n",
      "157 1229 93\n",
      "158 1240 94\n",
      "158 1240 95\n",
      "158 1240 96\n",
      "158 1236 97\n",
      "158 1234 98\n",
      "159 1241 99\n",
      "161 1257 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.6496875392439799 \ttest: 0.88 14.205757687270552\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.2484942913805467 \ttest: 0.88 14.309923615881685\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.13254842268626307 \ttest: 0.88 14.349132643088918\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.0820892102905168 \ttest: 0.8933333333333333 14.380188850914195\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.05567030674143026 \ttest: 0.8933333333333333 14.406567632081588\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.04016618042972876 \ttest: 0.8933333333333333 14.42952627292238\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.03031598006784835 \ttest: 0.8933333333333333 14.449878667690754\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.023679040366059573 \ttest: 0.8933333333333333 14.468160802042153\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.018999645987268304 \ttest: 0.8933333333333333 14.484739010463873\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.015579215349905626 \ttest: 0.8933333333333333 14.499878115579211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91667   0.86842   0.89189        38\n",
      "           1    0.87179   0.91892   0.89474        37\n",
      "\n",
      "    accuracy                        0.89333        75\n",
      "   macro avg    0.89423   0.89367   0.89331        75\n",
      "weighted avg    0.89453   0.89333   0.89330        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 161\n",
      "Average of depth: 2.3043478260869565\n",
      "Number of nodes: 1257\n",
      "12 62 1\n",
      "24 152 2\n",
      "30 216 3\n",
      "35 259 4\n",
      "40 278 5\n",
      "52 374 6\n",
      "54 394 7\n",
      "57 417 8\n",
      "59 427 9\n",
      "63 455 10\n",
      "68 490 11\n",
      "70 502 12\n",
      "72 508 13\n",
      "74 522 14\n",
      "77 549 15\n",
      "77 549 16\n",
      "79 569 17\n",
      "79 569 18\n",
      "82 586 19\n",
      "82 584 20\n",
      "85 609 21\n",
      "88 632 22\n",
      "89 635 23\n",
      "93 665 24\n",
      "94 674 25\n",
      "94 674 26\n",
      "98 712 27\n",
      "98 712 28\n",
      "98 712 29\n",
      "100 728 30\n",
      "103 757 31\n",
      "106 776 32\n",
      "107 783 33\n",
      "107 783 34\n",
      "109 799 35\n",
      "110 802 36\n",
      "111 807 37\n",
      "111 807 38\n",
      "111 807 39\n",
      "113 833 40\n",
      "115 849 41\n",
      "117 871 42\n",
      "120 904 43\n",
      "120 904 44\n",
      "121 915 45\n",
      "121 915 46\n",
      "121 915 47\n",
      "123 939 48\n",
      "123 937 49\n",
      "123 933 50\n",
      "124 938 51\n",
      "126 960 52\n",
      "127 969 53\n",
      "130 1000 54\n",
      "130 1000 55\n",
      "130 1000 56\n",
      "130 1000 57\n",
      "130 1000 58\n",
      "131 1007 59\n",
      "131 1001 60\n",
      "132 1010 61\n",
      "133 1015 62\n",
      "133 1015 63\n",
      "133 1011 64\n",
      "134 1012 65\n",
      "136 1024 66\n",
      "136 1024 67\n",
      "136 1024 68\n",
      "137 1035 69\n",
      "138 1042 70\n",
      "139 1053 71\n",
      "140 1064 72\n",
      "141 1075 73\n",
      "142 1088 74\n",
      "143 1099 75\n",
      "143 1095 76\n",
      "143 1095 77\n",
      "143 1091 78\n",
      "143 1091 79\n",
      "143 1091 80\n",
      "143 1083 81\n",
      "143 1083 82\n",
      "143 1075 83\n",
      "144 1088 84\n",
      "144 1088 85\n",
      "144 1088 86\n",
      "145 1095 87\n",
      "146 1102 88\n",
      "148 1126 89\n",
      "151 1163 90\n",
      "154 1198 91\n",
      "154 1198 92\n",
      "154 1198 93\n",
      "157 1219 94\n",
      "158 1232 95\n",
      "159 1241 96\n",
      "159 1241 97\n",
      "159 1241 98\n",
      "159 1241 99\n",
      "160 1252 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.1524430980352756 \ttest: 0.7866666666666666 23.98234057788672\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.039301460017710274 \ttest: 0.8 23.912290855292905\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.017759047395407124 \ttest: 0.8 23.744714095561676\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.010117561246035193 \ttest: 0.8 23.59453621824352\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.0065440772176540984 \ttest: 0.8133333333333334 23.46811900476378\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.004585714559212052 \ttest: 0.8266666666666667 23.361466884705283\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.003395638290304533 \ttest: 0.8266666666666667 23.270371521251505\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0026178212864284424 \ttest: 0.8266666666666667 23.191554024166784\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.0020811785155899964 \ttest: 0.8266666666666667 23.122567478225886\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0016951118564847664 \ttest: 0.8133333333333334 23.061579544939274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.83333   0.78947   0.81081        38\n",
      "           1    0.79487   0.83784   0.81579        37\n",
      "\n",
      "    accuracy                        0.81333        75\n",
      "   macro avg    0.81410   0.81366   0.81330        75\n",
      "weighted avg    0.81436   0.81333   0.81327        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 160\n",
      "Average of depth: 2.325\n",
      "Number of nodes: 1252\n",
      "18 110 1\n",
      "29 189 2\n",
      "38 248 3\n",
      "42 282 4\n",
      "46 310 5\n",
      "50 340 6\n",
      "51 345 7\n",
      "54 372 8\n",
      "56 376 9\n",
      "58 388 10\n",
      "62 444 11\n",
      "62 438 12\n",
      "62 438 13\n",
      "66 478 14\n",
      "66 478 15\n",
      "68 488 16\n",
      "69 495 17\n",
      "70 510 18\n",
      "71 509 19\n",
      "72 512 20\n",
      "74 526 21\n",
      "75 527 22\n",
      "75 527 23\n",
      "79 559 24\n",
      "82 586 25\n",
      "83 591 26\n",
      "86 616 27\n",
      "87 621 28\n",
      "87 621 29\n",
      "87 619 30\n",
      "91 647 31\n",
      "93 671 32\n",
      "94 676 33\n",
      "94 674 34\n",
      "95 683 35\n",
      "97 689 36\n",
      "97 689 37\n",
      "98 698 38\n",
      "99 705 39\n",
      "103 751 40\n",
      "106 784 41\n",
      "106 782 42\n",
      "108 796 43\n",
      "111 825 44\n",
      "111 825 45\n",
      "111 825 46\n",
      "111 821 47\n",
      "111 821 48\n",
      "111 819 49\n",
      "112 828 50\n",
      "112 828 51\n",
      "113 831 52\n",
      "114 842 53\n",
      "115 849 54\n",
      "115 849 55\n",
      "115 849 56\n",
      "116 856 57\n",
      "116 850 58\n",
      "117 853 59\n",
      "117 853 60\n",
      "118 860 61\n",
      "118 860 62\n",
      "119 869 63\n",
      "120 880 64\n",
      "122 902 65\n",
      "123 903 66\n",
      "123 903 67\n",
      "123 903 68\n",
      "124 912 69\n",
      "125 915 70\n",
      "124 888 71\n",
      "126 906 72\n",
      "126 906 73\n",
      "127 921 74\n",
      "127 921 75\n",
      "129 937 76\n",
      "129 937 77\n",
      "129 937 78\n",
      "130 952 79\n",
      "131 957 80\n",
      "131 957 81\n",
      "132 966 82\n",
      "132 962 83\n",
      "133 969 84\n",
      "134 974 85\n",
      "134 974 86\n",
      "134 974 87\n",
      "134 974 88\n",
      "136 996 89\n",
      "136 996 90\n",
      "136 996 91\n",
      "136 996 92\n",
      "138 1016 93\n",
      "138 1016 94\n",
      "138 1016 95\n",
      "138 1016 96\n",
      "138 1016 97\n",
      "138 1016 98\n",
      "138 1016 99\n",
      "139 1025 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.3081558952916634 \ttest: 0.84 19.20509097556748\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.0710032447431813 \ttest: 0.84 19.39649664913894\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.03182991769858357 \ttest: 0.84 19.483037393741093\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.018231181563949445 \ttest: 0.8533333333333334 19.54489922475726\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.011891364046258937 \ttest: 0.8533333333333334 19.597542541754358\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.008406911957425486 \ttest: 0.8533333333333334 19.644874905895705\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.006278514022612133 \ttest: 0.8533333333333334 19.688258286356806\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.004879056784833248 \ttest: 0.8533333333333334 19.72839188384092\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.003907488704778233 \ttest: 0.8533333333333334 19.765747471763277\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0032042003192761003 \ttest: 0.8533333333333334 19.800688433606975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.81395   0.92105   0.86420        38\n",
      "           1    0.90625   0.78378   0.84058        37\n",
      "\n",
      "    accuracy                        0.85333        75\n",
      "   macro avg    0.86010   0.85242   0.85239        75\n",
      "weighted avg    0.85949   0.85333   0.85255        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 139\n",
      "Average of depth: 2.237410071942446\n",
      "Number of nodes: 1025\n",
      "15 87 1\n",
      "24 142 2\n",
      "27 171 3\n",
      "30 190 4\n",
      "34 214 5\n",
      "36 226 6\n",
      "40 254 7\n",
      "44 288 8\n",
      "48 330 9\n",
      "50 350 10\n",
      "52 368 11\n",
      "56 402 12\n",
      "62 446 13\n",
      "64 460 14\n",
      "66 484 15\n",
      "68 496 16\n",
      "71 529 17\n",
      "72 536 18\n",
      "74 552 19\n",
      "78 584 20\n",
      "79 597 21\n",
      "82 624 22\n",
      "85 655 23\n",
      "90 696 24\n",
      "90 692 25\n",
      "91 695 26\n",
      "91 695 27\n",
      "92 702 28\n",
      "94 720 29\n",
      "94 720 30\n",
      "94 720 31\n",
      "96 738 32\n",
      "99 763 33\n",
      "99 763 34\n",
      "100 766 35\n",
      "103 791 36\n",
      "104 798 37\n",
      "104 798 38\n",
      "105 807 39\n",
      "105 807 40\n",
      "105 805 41\n",
      "105 805 42\n",
      "106 816 43\n",
      "106 816 44\n",
      "107 817 45\n",
      "107 817 46\n",
      "109 841 47\n",
      "110 848 48\n",
      "117 933 49\n",
      "117 931 50\n",
      "118 928 51\n",
      "118 924 52\n",
      "120 950 53\n",
      "121 951 54\n",
      "121 951 55\n",
      "122 960 56\n",
      "122 958 57\n",
      "122 958 58\n",
      "122 958 59\n",
      "124 976 60\n",
      "124 976 61\n",
      "124 976 62\n",
      "124 972 63\n",
      "125 981 64\n",
      "127 997 65\n",
      "127 993 66\n",
      "127 993 67\n",
      "127 987 68\n",
      "128 998 69\n",
      "128 992 70\n",
      "128 990 71\n",
      "129 995 72\n",
      "130 994 73\n",
      "132 1012 74\n",
      "133 1019 75\n",
      "134 1028 76\n",
      "135 1039 77\n",
      "135 1039 78\n",
      "135 1039 79\n",
      "137 1055 80\n",
      "137 1051 81\n",
      "138 1062 82\n",
      "138 1058 83\n",
      "138 1058 84\n",
      "139 1065 85\n",
      "139 1063 86\n",
      "140 1072 87\n",
      "142 1090 88\n",
      "143 1103 89\n",
      "143 1103 90\n",
      "143 1097 91\n",
      "145 1117 92\n",
      "147 1137 93\n",
      "147 1133 94\n",
      "147 1133 95\n",
      "147 1133 96\n",
      "148 1146 97\n",
      "149 1155 98\n",
      "150 1164 99\n",
      "150 1160 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.5075728274045787 \ttest: 0.84 21.222602780029888\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.1644196574176904 \ttest: 0.8266666666666667 21.59323712894561\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.0804583226264465 \ttest: 0.8266666666666667 21.75344926995713\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.04736542894974281 \ttest: 0.8266666666666667 21.851154625927723\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.031089252827077453 \ttest: 0.8266666666666667 21.920915611108537\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.021928342565078942 \ttest: 0.8266666666666667 21.975345196042518\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.01627760050247803 \ttest: 0.8266666666666667 22.020127906667913\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.012552740265580026 \ttest: 0.8266666666666667 22.05825587728754\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.009970528257910708 \ttest: 0.8266666666666667 22.09149756629102\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.008108253177653128 \ttest: 0.8266666666666667 22.12098916766393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.83333   0.81081   0.82192        37\n",
      "           1    0.82051   0.84211   0.83117        38\n",
      "\n",
      "    accuracy                        0.82667        75\n",
      "   macro avg    0.82692   0.82646   0.82654        75\n",
      "weighted avg    0.82684   0.82667   0.82660        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 150\n",
      "Average of depth: 2.3266666666666667\n",
      "Number of nodes: 1160\n",
      "12 78 1\n",
      "19 123 2\n",
      "26 180 3\n",
      "30 210 4\n",
      "36 264 5\n",
      "41 303 6\n",
      "48 354 7\n",
      "54 398 8\n",
      "57 427 9\n",
      "59 443 10\n",
      "61 457 11\n",
      "65 487 12\n",
      "69 527 13\n",
      "75 587 14\n",
      "76 584 15\n",
      "79 611 16\n",
      "80 592 17\n",
      "83 615 18\n",
      "84 618 19\n",
      "85 631 20\n",
      "90 674 21\n",
      "92 690 22\n",
      "94 704 23\n",
      "94 704 24\n",
      "95 719 25\n",
      "97 737 26\n",
      "99 755 27\n",
      "99 755 28\n",
      "100 762 29\n",
      "101 769 30\n",
      "101 769 31\n",
      "101 767 32\n",
      "103 781 33\n",
      "104 788 34\n",
      "104 786 35\n",
      "104 786 36\n",
      "107 821 37\n",
      "108 828 38\n",
      "108 828 39\n",
      "112 864 40\n",
      "113 873 41\n",
      "114 888 42\n",
      "115 893 43\n",
      "120 944 44\n",
      "121 953 45\n",
      "121 953 46\n",
      "122 968 47\n",
      "123 979 48\n",
      "123 979 49\n",
      "124 990 50\n",
      "126 1008 51\n",
      "126 1004 52\n",
      "127 1011 53\n",
      "127 1005 54\n",
      "127 1001 55\n",
      "129 1017 56\n",
      "129 1009 57\n",
      "131 1027 58\n",
      "132 1028 59\n",
      "132 1028 60\n",
      "134 1046 61\n",
      "136 1060 62\n",
      "136 1058 63\n",
      "136 1058 64\n",
      "136 1058 65\n",
      "137 1069 66\n",
      "139 1089 67\n",
      "141 1099 68\n",
      "141 1095 69\n",
      "142 1100 70\n",
      "143 1109 71\n",
      "143 1109 72\n",
      "143 1109 73\n",
      "143 1109 74\n",
      "143 1109 75\n",
      "144 1120 76\n",
      "144 1120 77\n",
      "145 1127 78\n",
      "147 1145 79\n",
      "147 1143 80\n",
      "147 1139 81\n",
      "147 1139 82\n",
      "149 1159 83\n",
      "150 1174 84\n",
      "150 1174 85\n",
      "150 1172 86\n",
      "150 1170 87\n",
      "150 1170 88\n",
      "150 1168 89\n",
      "150 1160 90\n",
      "151 1171 91\n",
      "151 1165 92\n",
      "152 1174 93\n",
      "153 1179 94\n",
      "154 1188 95\n",
      "154 1186 96\n",
      "154 1186 97\n",
      "154 1178 98\n",
      "154 1178 99\n",
      "155 1187 100\n",
      "retrain  1 :\n",
      "\ttrain: 0.9942857142857143 1.7762667615498424 \ttest: 0.7733333333333333 29.070499176793703\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.26421381113105147 \ttest: 0.7866666666666666 28.27180220283491\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.10306104612512765 \ttest: 0.7866666666666666 28.337433334626823\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.05564845686889306 \ttest: 0.7866666666666666 28.421477390010715\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.0351542888538019 \ttest: 0.7866666666666666 28.488512715758734\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.02436014771274954 \ttest: 0.7866666666666666 28.54040279965198\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.017942466391269694 \ttest: 0.7866666666666666 28.581026456960668\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.013800280702547161 \ttest: 0.7866666666666666 28.61337534753503\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.010963247806020895 \ttest: 0.7866666666666666 28.639559044883217\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.008930792022265585 \ttest: 0.7866666666666666 28.661060848861027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.77500   0.81579   0.79487        38\n",
      "           1    0.80000   0.75676   0.77778        37\n",
      "\n",
      "    accuracy                        0.78667        75\n",
      "   macro avg    0.78750   0.78627   0.78632        75\n",
      "weighted avg    0.78733   0.78667   0.78644        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 155\n",
      "Average of depth: 2.2774193548387096\n",
      "Number of nodes: 1187\n",
      "21 127 1\n",
      "29 183 2\n",
      "33 211 3\n",
      "42 286 4\n",
      "44 298 5\n",
      "45 307 6\n",
      "50 348 7\n",
      "52 370 8\n",
      "56 396 9\n",
      "58 398 10\n",
      "61 427 11\n",
      "67 475 12\n",
      "70 494 13\n",
      "76 546 14\n",
      "77 551 15\n",
      "78 564 16\n",
      "79 567 17\n",
      "81 581 18\n",
      "83 597 19\n",
      "86 622 20\n",
      "87 621 21\n",
      "87 619 22\n",
      "90 648 23\n",
      "93 673 24\n",
      "96 692 25\n",
      "96 692 26\n",
      "96 692 27\n",
      "97 703 28\n",
      "97 699 29\n",
      "97 693 30\n",
      "98 694 31\n",
      "100 710 32\n",
      "101 721 33\n",
      "103 739 34\n",
      "103 739 35\n",
      "104 750 36\n",
      "105 753 37\n",
      "105 747 38\n",
      "108 768 39\n",
      "108 768 40\n",
      "109 781 41\n",
      "110 790 42\n",
      "111 801 43\n",
      "113 817 44\n",
      "113 817 45\n",
      "113 813 46\n",
      "113 813 47\n",
      "113 813 48\n",
      "116 850 49\n",
      "118 866 50\n",
      "119 877 51\n",
      "119 877 52\n",
      "119 877 53\n",
      "119 877 54\n",
      "119 877 55\n",
      "120 880 56\n",
      "120 880 57\n",
      "122 906 58\n",
      "122 904 59\n",
      "122 904 60\n",
      "125 933 61\n",
      "126 944 62\n",
      "128 964 63\n",
      "128 954 64\n",
      "128 954 65\n",
      "128 954 66\n",
      "128 954 67\n",
      "129 969 68\n",
      "129 969 69\n",
      "129 969 70\n",
      "130 980 71\n",
      "130 980 72\n",
      "130 980 73\n",
      "130 980 74\n",
      "130 980 75\n",
      "131 995 76\n",
      "131 991 77\n",
      "133 1011 78\n",
      "133 1011 79\n",
      "133 1011 80\n",
      "134 1022 81\n",
      "136 1046 82\n",
      "138 1070 83\n",
      "139 1077 84\n",
      "139 1077 85\n",
      "140 1086 86\n",
      "140 1086 87\n",
      "141 1099 88\n",
      "145 1133 89\n",
      "146 1138 90\n",
      "148 1164 91\n",
      "148 1158 92\n",
      "149 1169 93\n",
      "149 1167 94\n",
      "149 1167 95\n",
      "149 1167 96\n",
      "151 1185 97\n",
      "151 1185 98\n",
      "153 1203 99\n",
      "153 1201 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.22999164801488384 \ttest: 0.8666666666666667 15.209298831805288\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.06405329182097831 \ttest: 0.8666666666666667 14.758846662163432\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.030934258375582554 \ttest: 0.8666666666666667 14.425507312416887\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.018578229643772294 \ttest: 0.88 14.169925836813295\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.012536891386250883 \ttest: 0.88 13.965749002287724\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.009096343186762335 \ttest: 0.8933333333333333 13.79793907032996\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.00693477639409815 \ttest: 0.8933333333333333 13.6571998444679\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.005480349436025022 \ttest: 0.8933333333333333 13.537340246220033\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.00445090292288958 \ttest: 0.8933333333333333 13.43400728978754\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.00369332963649308 \ttest: 0.8933333333333333 13.3440201035184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.85366   0.94595   0.89744        37\n",
      "           1    0.94118   0.84211   0.88889        38\n",
      "\n",
      "    accuracy                        0.89333        75\n",
      "   macro avg    0.89742   0.89403   0.89316        75\n",
      "weighted avg    0.89800   0.89333   0.89311        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 153\n",
      "Average of depth: 2.3137254901960786\n",
      "Number of nodes: 1201\n",
      "12 64 1\n",
      "20 116 2\n",
      "23 137 3\n",
      "30 182 4\n",
      "37 241 5\n",
      "44 300 6\n",
      "50 354 7\n",
      "56 412 8\n",
      "60 440 9\n",
      "62 446 10\n",
      "67 479 11\n",
      "70 502 12\n",
      "72 510 13\n",
      "72 508 14\n",
      "72 504 15\n",
      "77 553 16\n",
      "79 573 17\n",
      "81 585 18\n",
      "84 614 19\n",
      "83 601 20\n",
      "83 601 21\n",
      "87 631 22\n",
      "89 649 23\n",
      "91 673 24\n",
      "93 691 25\n",
      "93 691 26\n",
      "95 705 27\n",
      "97 727 28\n",
      "100 746 29\n",
      "102 762 30\n",
      "103 773 31\n",
      "105 793 32\n",
      "106 802 33\n",
      "106 802 34\n",
      "107 803 35\n",
      "108 814 36\n",
      "111 833 37\n",
      "111 833 38\n",
      "112 838 39\n",
      "112 836 40\n",
      "112 832 41\n",
      "112 828 42\n",
      "113 839 43\n",
      "112 820 44\n",
      "114 838 45\n",
      "114 838 46\n",
      "115 851 47\n",
      "116 864 48\n",
      "116 854 49\n",
      "116 854 50\n",
      "117 863 51\n",
      "118 876 52\n",
      "119 885 53\n",
      "119 881 54\n",
      "121 905 55\n",
      "123 923 56\n",
      "123 923 57\n",
      "123 923 58\n",
      "123 919 59\n",
      "124 930 60\n",
      "128 982 61\n",
      "129 989 62\n",
      "129 989 63\n",
      "129 989 64\n",
      "130 996 65\n",
      "130 994 66\n",
      "131 1005 67\n",
      "132 1012 68\n",
      "132 1012 69\n",
      "132 1012 70\n",
      "133 1021 71\n",
      "133 1021 72\n",
      "134 1028 73\n",
      "134 1026 74\n",
      "136 1056 75\n",
      "137 1071 76\n",
      "138 1082 77\n",
      "140 1102 78\n",
      "141 1113 79\n",
      "141 1109 80\n",
      "143 1129 81\n",
      "144 1134 82\n",
      "144 1128 83\n",
      "144 1128 84\n",
      "145 1139 85\n",
      "146 1150 86\n",
      "149 1183 87\n",
      "150 1190 88\n",
      "150 1190 89\n",
      "150 1190 90\n",
      "150 1190 91\n",
      "150 1190 92\n",
      "151 1199 93\n",
      "152 1214 94\n",
      "152 1212 95\n",
      "152 1212 96\n",
      "153 1223 97\n",
      "154 1236 98\n",
      "154 1236 99\n",
      "154 1236 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.2521125565956541 \ttest: 0.8266666666666667 19.44508181215448\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.0663028458482233 \ttest: 0.8133333333333334 20.205630956244455\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.030106272613281103 \ttest: 0.8133333333333334 20.690101880122896\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.017160267426398184 \ttest: 0.8133333333333334 21.041241347789835\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.011084451220806242 \ttest: 0.8133333333333334 21.314171267183166\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.007750752415790593 \ttest: 0.8133333333333334 21.53600228823084\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.005725231540603395 \ttest: 0.8133333333333334 21.722044811036305\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.004402621849355854 \ttest: 0.8133333333333334 21.881704094290892\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.003491357247835911 \ttest: 0.8133333333333334 22.021137974396936\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0028368325944611045 \ttest: 0.8 22.1445880744398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.78947   0.81081   0.80000        37\n",
      "           1    0.81081   0.78947   0.80000        38\n",
      "\n",
      "    accuracy                        0.80000        75\n",
      "   macro avg    0.80014   0.80014   0.80000        75\n",
      "weighted avg    0.80028   0.80000   0.80000        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 154\n",
      "Average of depth: 2.344155844155844\n",
      "Number of nodes: 1236\n",
      "14 82 1\n",
      "23 143 2\n",
      "31 193 3\n",
      "38 256 4\n",
      "49 353 5\n",
      "50 360 6\n",
      "52 364 7\n",
      "60 434 8\n",
      "65 475 9\n",
      "68 504 10\n",
      "71 527 11\n",
      "74 554 12\n",
      "77 575 13\n",
      "82 618 14\n",
      "88 666 15\n",
      "89 673 16\n",
      "92 704 17\n",
      "94 724 18\n",
      "96 752 19\n",
      "98 770 20\n",
      "98 766 21\n",
      "100 768 22\n",
      "100 768 23\n",
      "101 775 24\n",
      "102 782 25\n",
      "102 782 26\n",
      "103 789 27\n",
      "106 822 28\n",
      "106 820 29\n",
      "108 828 30\n",
      "109 841 31\n",
      "110 852 32\n",
      "111 863 33\n",
      "112 872 34\n",
      "112 868 35\n",
      "112 864 36\n",
      "115 891 37\n",
      "116 900 38\n",
      "117 915 39\n",
      "117 915 40\n",
      "118 928 41\n",
      "119 931 42\n",
      "119 931 43\n",
      "120 938 44\n",
      "120 930 45\n",
      "121 939 46\n",
      "121 939 47\n",
      "124 974 48\n",
      "125 979 49\n",
      "127 991 50\n",
      "128 1002 51\n",
      "128 1000 52\n",
      "129 1005 53\n",
      "128 990 54\n",
      "130 1004 55\n",
      "133 1037 56\n",
      "133 1037 57\n",
      "133 1037 58\n",
      "133 1037 59\n",
      "133 1035 60\n",
      "134 1050 61\n",
      "135 1055 62\n",
      "138 1090 63\n",
      "139 1101 64\n",
      "139 1101 65\n",
      "141 1121 66\n",
      "144 1166 67\n",
      "145 1179 68\n",
      "145 1177 69\n",
      "145 1173 70\n",
      "147 1189 71\n",
      "149 1207 72\n",
      "149 1203 73\n",
      "149 1203 74\n",
      "149 1195 75\n",
      "149 1195 76\n",
      "151 1219 77\n",
      "151 1219 78\n",
      "151 1219 79\n",
      "152 1230 80\n",
      "151 1209 81\n",
      "151 1207 82\n",
      "152 1222 83\n",
      "152 1222 84\n",
      "152 1222 85\n",
      "153 1227 86\n",
      "153 1227 87\n",
      "153 1227 88\n",
      "154 1238 89\n",
      "154 1236 90\n",
      "154 1232 91\n",
      "155 1239 92\n",
      "155 1239 93\n",
      "155 1239 94\n",
      "155 1233 95\n",
      "155 1233 96\n",
      "156 1240 97\n",
      "157 1245 98\n",
      "158 1258 99\n",
      "159 1269 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.17461638514863326 \ttest: 0.8533333333333334 18.80249881104487\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.04974051551411093 \ttest: 0.8533333333333334 18.718817676141374\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.025578269774985726 \ttest: 0.8533333333333334 18.54870585135715\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.015890583331324672 \ttest: 0.8533333333333334 18.417503641644075\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.010915912476997351 \ttest: 0.8666666666666667 18.32825809863916\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.007993880611774692 \ttest: 0.8666666666666667 18.270679429220383\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.00612171661592978 \ttest: 0.8666666666666667 18.23506471396741\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.004846127354275901 \ttest: 0.8666666666666667 18.214272280112358\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.003936032853537439 \ttest: 0.8666666666666667 18.203397664590447\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0032629634392980188 \ttest: 0.8666666666666667 18.19914562100398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.85000   0.89474   0.87179        38\n",
      "           1    0.88571   0.83784   0.86111        37\n",
      "\n",
      "    accuracy                        0.86667        75\n",
      "   macro avg    0.86786   0.86629   0.86645        75\n",
      "weighted avg    0.86762   0.86667   0.86652        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 159\n",
      "Average of depth: 2.289308176100629\n",
      "Number of nodes: 1269\n",
      "16 106 1\n",
      "24 148 2\n",
      "27 171 3\n",
      "32 204 4\n",
      "35 233 5\n",
      "36 240 6\n",
      "43 287 7\n",
      "48 326 8\n",
      "49 325 9\n",
      "55 383 10\n",
      "55 377 11\n",
      "57 403 12\n",
      "59 417 13\n",
      "60 424 14\n",
      "61 431 15\n",
      "62 432 16\n",
      "64 444 17\n",
      "67 473 18\n",
      "69 497 19\n",
      "71 521 20\n",
      "71 521 21\n",
      "71 521 22\n",
      "72 510 23\n",
      "73 525 24\n",
      "73 517 25\n",
      "74 524 26\n",
      "78 564 27\n",
      "78 564 28\n",
      "79 571 29\n",
      "82 602 30\n",
      "84 616 31\n",
      "85 615 32\n",
      "85 613 33\n",
      "86 618 34\n",
      "88 640 35\n",
      "89 655 36\n",
      "93 689 37\n",
      "96 712 38\n",
      "97 721 39\n",
      "98 732 40\n",
      "98 732 41\n",
      "101 759 42\n",
      "101 755 43\n",
      "102 770 44\n",
      "103 773 45\n",
      "104 786 46\n",
      "105 795 47\n",
      "105 795 48\n",
      "105 795 49\n",
      "106 800 50\n",
      "108 820 51\n",
      "110 834 52\n",
      "112 852 53\n",
      "114 874 54\n",
      "115 881 55\n",
      "117 899 56\n",
      "118 910 57\n",
      "118 910 58\n",
      "118 910 59\n",
      "118 910 60\n",
      "118 910 61\n",
      "118 910 62\n",
      "119 913 63\n",
      "120 928 64\n",
      "120 928 65\n",
      "120 928 66\n",
      "121 941 67\n",
      "122 956 68\n",
      "123 963 69\n",
      "124 950 70\n",
      "125 959 71\n",
      "125 957 72\n",
      "125 957 73\n",
      "126 964 74\n",
      "126 964 75\n",
      "126 964 76\n",
      "128 982 77\n",
      "128 982 78\n",
      "128 982 79\n",
      "129 983 80\n",
      "130 988 81\n",
      "129 975 82\n",
      "130 988 83\n",
      "130 986 84\n",
      "130 986 85\n",
      "132 1004 86\n",
      "133 1013 87\n",
      "134 1026 88\n",
      "134 1026 89\n",
      "135 1033 90\n",
      "135 1033 91\n",
      "135 1031 92\n",
      "136 1038 93\n",
      "137 1053 94\n",
      "138 1064 95\n",
      "138 1064 96\n",
      "139 1073 97\n",
      "142 1102 98\n",
      "142 1100 99\n",
      "142 1092 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.3288937594902284 \ttest: 0.8933333333333333 14.89480447085693\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.08082262387803477 \ttest: 0.8933333333333333 15.081875362445139\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.037075416959663536 \ttest: 0.88 15.140338962244694\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.021437118062843895 \ttest: 0.88 15.168130860283636\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.014025932813763195 \ttest: 0.88 15.18523361525327\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.009918107441299218 \ttest: 0.88 15.197759630669761\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.007398765081434281 \ttest: 0.88 15.207981524905504\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.005739619628190411 \ttest: 0.88 15.216870067807763\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.004587553667658137 \ttest: 0.88 15.224891180157254\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.003754140469467972 \ttest: 0.88 15.232292578603612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.89189   0.86842   0.88000        38\n",
      "           1    0.86842   0.89189   0.88000        37\n",
      "\n",
      "    accuracy                        0.88000        75\n",
      "   macro avg    0.88016   0.88016   0.88000        75\n",
      "weighted avg    0.88031   0.88000   0.88000        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 142\n",
      "Average of depth: 2.295774647887324\n",
      "Number of nodes: 1092\n",
      "20 114 1\n",
      "26 150 2\n",
      "33 211 3\n",
      "40 278 4\n",
      "46 326 5\n",
      "49 359 6\n",
      "49 353 7\n",
      "53 381 8\n",
      "54 388 9\n",
      "60 434 10\n",
      "65 491 11\n",
      "65 479 12\n",
      "67 497 13\n",
      "72 536 14\n",
      "74 546 15\n",
      "76 562 16\n",
      "77 567 17\n",
      "78 574 18\n",
      "81 599 19\n",
      "85 633 20\n",
      "89 665 21\n",
      "92 686 22\n",
      "92 684 23\n",
      "91 673 24\n",
      "92 676 25\n",
      "93 677 26\n",
      "99 727 27\n",
      "101 745 28\n",
      "103 767 29\n",
      "107 805 30\n",
      "108 804 31\n",
      "109 809 32\n",
      "110 810 33\n",
      "111 817 34\n",
      "112 822 35\n",
      "113 829 36\n",
      "113 825 37\n",
      "113 823 38\n",
      "115 843 39\n",
      "117 861 40\n",
      "118 866 41\n",
      "118 866 42\n",
      "119 881 43\n",
      "121 901 44\n",
      "122 912 45\n",
      "122 912 46\n",
      "122 912 47\n",
      "124 932 48\n",
      "124 930 49\n",
      "125 941 50\n",
      "125 929 51\n",
      "125 929 52\n",
      "126 936 53\n",
      "126 936 54\n",
      "126 936 55\n",
      "126 936 56\n",
      "127 943 57\n",
      "128 948 58\n",
      "130 978 59\n",
      "130 978 60\n",
      "130 978 61\n",
      "130 978 62\n",
      "131 989 63\n",
      "132 1000 64\n",
      "133 1011 65\n",
      "134 1016 66\n",
      "136 1036 67\n",
      "136 1036 68\n",
      "137 1045 69\n",
      "137 1045 70\n",
      "137 1045 71\n",
      "138 1042 72\n",
      "138 1042 73\n",
      "139 1057 74\n",
      "142 1088 75\n",
      "141 1079 76\n",
      "141 1079 77\n",
      "145 1119 78\n",
      "145 1119 79\n",
      "145 1119 80\n",
      "145 1115 81\n",
      "146 1126 82\n",
      "146 1124 83\n",
      "146 1124 84\n",
      "146 1124 85\n",
      "147 1135 86\n",
      "145 1113 87\n",
      "145 1111 88\n",
      "146 1122 89\n",
      "146 1122 90\n",
      "147 1129 91\n",
      "147 1123 92\n",
      "148 1130 93\n",
      "149 1139 94\n",
      "149 1139 95\n",
      "149 1139 96\n",
      "149 1139 97\n",
      "150 1150 98\n",
      "150 1150 99\n",
      "150 1150 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.4372535855320674 \ttest: 0.8666666666666667 15.390009042553181\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.11568234540992638 \ttest: 0.8666666666666667 15.974514019249408\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.051705327752543585 \ttest: 0.8666666666666667 16.29257902611025\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.029211448038091586 \ttest: 0.8666666666666667 16.499565196215215\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.01878033107138663 \ttest: 0.8666666666666667 16.65055647675199\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.013100555165469614 \ttest: 0.8666666666666667 16.76780190904129\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.009666497045206918 \ttest: 0.8666666666666667 16.86247489306349\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.007431366251684689 \ttest: 0.8666666666666667 16.941033498717033\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.005894691970975556 \ttest: 0.8666666666666667 17.00756876323155\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.004792546398021316 \ttest: 0.8666666666666667 17.06483921432728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.81395   0.94595   0.87500        37\n",
      "           1    0.93750   0.78947   0.85714        38\n",
      "\n",
      "    accuracy                        0.86667        75\n",
      "   macro avg    0.87573   0.86771   0.86607        75\n",
      "weighted avg    0.87655   0.86667   0.86595        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 150\n",
      "Average of depth: 2.2733333333333334\n",
      "Number of nodes: 1150\n",
      "8 30 1\n",
      "16 98 2\n",
      "19 115 3\n",
      "24 148 4\n",
      "26 156 5\n",
      "35 235 6\n",
      "40 270 7\n",
      "45 315 8\n",
      "47 333 9\n",
      "56 394 10\n",
      "61 441 11\n",
      "62 448 12\n",
      "63 455 13\n",
      "66 482 14\n",
      "69 507 15\n",
      "74 544 16\n",
      "74 530 17\n",
      "74 530 18\n",
      "76 544 19\n",
      "80 586 20\n",
      "82 604 21\n",
      "87 651 22\n",
      "89 671 23\n",
      "88 662 24\n",
      "90 686 25\n",
      "91 689 26\n",
      "91 685 27\n",
      "93 699 28\n",
      "93 699 29\n",
      "93 695 30\n",
      "93 695 31\n",
      "93 691 32\n",
      "94 686 33\n",
      "94 684 34\n",
      "94 676 35\n",
      "100 722 36\n",
      "102 736 37\n",
      "104 754 38\n",
      "104 754 39\n",
      "104 752 40\n",
      "104 750 41\n",
      "104 750 42\n",
      "103 739 43\n",
      "104 748 44\n",
      "106 766 45\n",
      "109 791 46\n",
      "109 787 47\n",
      "109 787 48\n",
      "112 822 49\n",
      "113 831 50\n",
      "113 827 51\n",
      "114 834 52\n",
      "114 834 53\n",
      "115 847 54\n",
      "115 843 55\n",
      "117 853 56\n",
      "117 851 57\n",
      "118 860 58\n",
      "119 869 59\n",
      "120 882 60\n",
      "121 885 61\n",
      "122 896 62\n",
      "123 907 63\n",
      "124 920 64\n",
      "125 933 65\n",
      "126 940 66\n",
      "126 940 67\n",
      "127 947 68\n",
      "128 954 69\n",
      "128 954 70\n",
      "129 963 71\n",
      "130 972 72\n",
      "132 998 73\n",
      "134 1008 74\n",
      "134 1008 75\n",
      "136 1030 76\n",
      "137 1039 77\n",
      "139 1061 78\n",
      "139 1059 79\n",
      "139 1059 80\n",
      "141 1075 81\n",
      "142 1086 82\n",
      "142 1086 83\n",
      "143 1095 84\n",
      "145 1115 85\n",
      "145 1111 86\n",
      "145 1111 87\n",
      "145 1111 88\n",
      "148 1134 89\n",
      "149 1141 90\n",
      "150 1148 91\n",
      "151 1159 92\n",
      "151 1159 93\n",
      "151 1159 94\n",
      "152 1170 95\n",
      "152 1170 96\n",
      "153 1179 97\n",
      "154 1190 98\n",
      "154 1190 99\n",
      "154 1184 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.40863807553475096 \ttest: 0.88 13.318743432644792\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.1197046910366488 \ttest: 0.88 13.362739329330797\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.05623381831738697 \ttest: 0.88 13.404726214704226\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.03252084262679674 \ttest: 0.88 13.436941453278372\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.021164813741863783 \ttest: 0.88 13.462167908089102\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.014864792530254719 \ttest: 0.88 13.48259856908925\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.011010948191232389 \ttest: 0.88 13.499599631729291\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.008483171330492149 \ttest: 0.88 13.514048140829676\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.006736093378947037 \ttest: 0.88 13.526536760970277\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0054783881021389265 \ttest: 0.8933333333333333 13.537483103206997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.89474   0.89474   0.89474        38\n",
      "           1    0.89189   0.89189   0.89189        37\n",
      "\n",
      "    accuracy                        0.89333        75\n",
      "   macro avg    0.89331   0.89331   0.89331        75\n",
      "weighted avg    0.89333   0.89333   0.89333        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 154\n",
      "Average of depth: 2.357142857142857\n",
      "Number of nodes: 1184\n",
      "15 105 1\n",
      "24 154 2\n",
      "31 215 3\n",
      "36 246 4\n",
      "38 258 5\n",
      "40 288 6\n",
      "44 310 7\n",
      "47 323 8\n",
      "50 340 9\n",
      "51 345 10\n",
      "53 355 11\n",
      "58 386 12\n",
      "59 401 13\n",
      "60 408 14\n",
      "63 435 15\n",
      "63 431 16\n",
      "63 431 17\n",
      "63 429 18\n",
      "65 449 19\n",
      "68 470 20\n",
      "69 479 21\n",
      "70 486 22\n",
      "70 486 23\n",
      "70 482 24\n",
      "72 496 25\n",
      "73 503 26\n",
      "75 519 27\n",
      "77 541 28\n",
      "78 552 29\n",
      "80 568 30\n",
      "80 566 31\n",
      "81 573 32\n",
      "83 597 33\n",
      "84 602 34\n",
      "86 620 35\n",
      "87 629 36\n",
      "89 649 37\n",
      "89 649 38\n",
      "91 665 39\n",
      "93 687 40\n",
      "94 698 41\n",
      "96 710 42\n",
      "97 721 43\n",
      "98 732 44\n",
      "100 748 45\n",
      "102 764 46\n",
      "103 769 47\n",
      "105 787 48\n",
      "106 782 49\n",
      "107 793 50\n",
      "108 798 51\n",
      "108 798 52\n",
      "108 796 53\n",
      "108 796 54\n",
      "108 796 55\n",
      "108 796 56\n",
      "108 796 57\n",
      "108 796 58\n",
      "108 796 59\n",
      "108 792 60\n",
      "108 790 61\n",
      "109 795 62\n",
      "110 806 63\n",
      "111 815 64\n",
      "111 815 65\n",
      "112 828 66\n",
      "112 828 67\n",
      "112 828 68\n",
      "116 870 69\n",
      "118 888 70\n",
      "119 891 71\n",
      "119 891 72\n",
      "119 891 73\n",
      "119 889 74\n",
      "120 898 75\n",
      "120 894 76\n",
      "121 907 77\n",
      "122 914 78\n",
      "122 908 79\n",
      "122 908 80\n",
      "123 919 81\n",
      "123 919 82\n",
      "126 956 83\n",
      "127 963 84\n",
      "130 998 85\n",
      "130 994 86\n",
      "130 994 87\n",
      "130 990 88\n",
      "131 1001 89\n",
      "131 1001 90\n",
      "134 1032 91\n",
      "135 1045 92\n",
      "136 1056 93\n",
      "136 1056 94\n",
      "136 1056 95\n",
      "136 1056 96\n",
      "136 1056 97\n",
      "137 1065 98\n",
      "137 1065 99\n",
      "137 1057 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.17533554089529793 \ttest: 0.84 22.115169144416722\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.051178266612513 \ttest: 0.84 22.560986243559455\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.024569282218553368 \ttest: 0.84 22.743191548929893\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.014492761974038822 \ttest: 0.8266666666666667 22.84484020566767\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.009587146184416594 \ttest: 0.8266666666666667 22.90973361023387\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.006825237408023273 \ttest: 0.8266666666666667 22.954406953350325\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.005114006398733395 \ttest: 0.8266666666666667 22.986680075962536\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.003979044989508256 \ttest: 0.8266666666666667 23.01078044365032\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.003186907914807343 \ttest: 0.8266666666666667 23.029207944795335\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.002611680206775113 \ttest: 0.8266666666666667 23.043540207157484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.81579   0.83784   0.82667        37\n",
      "           1    0.83784   0.81579   0.82667        38\n",
      "\n",
      "    accuracy                        0.82667        75\n",
      "   macro avg    0.82681   0.82681   0.82667        75\n",
      "weighted avg    0.82696   0.82667   0.82667        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 137\n",
      "Average of depth: 2.321167883211679\n",
      "Number of nodes: 1057\n",
      "19 115 1\n",
      "25 157 2\n",
      "29 185 3\n",
      "32 208 4\n",
      "36 238 5\n",
      "38 254 6\n",
      "42 290 7\n",
      "47 333 8\n",
      "48 328 9\n",
      "54 374 10\n",
      "64 452 11\n",
      "65 459 12\n",
      "68 478 13\n",
      "73 537 14\n",
      "73 531 15\n",
      "75 551 16\n",
      "75 535 17\n",
      "77 551 18\n",
      "79 565 19\n",
      "79 565 20\n",
      "80 568 21\n",
      "83 593 22\n",
      "86 616 23\n",
      "89 641 24\n",
      "89 639 25\n",
      "92 668 26\n",
      "94 692 27\n",
      "94 690 28\n",
      "94 690 29\n",
      "99 735 30\n",
      "99 733 31\n",
      "102 756 32\n",
      "102 752 33\n",
      "102 752 34\n",
      "104 766 35\n",
      "106 780 36\n",
      "106 778 37\n",
      "110 816 38\n",
      "111 823 39\n",
      "111 823 40\n",
      "111 823 41\n",
      "113 839 42\n",
      "113 839 43\n",
      "113 833 44\n",
      "114 840 45\n",
      "114 840 46\n",
      "114 840 47\n",
      "116 864 48\n",
      "117 871 49\n",
      "119 891 50\n",
      "119 891 51\n",
      "119 885 52\n",
      "120 896 53\n",
      "121 907 54\n",
      "122 922 55\n",
      "123 935 56\n",
      "124 942 57\n",
      "125 949 58\n",
      "126 960 59\n",
      "126 958 60\n",
      "126 958 61\n",
      "128 976 62\n",
      "130 1000 63\n",
      "130 998 64\n",
      "130 994 65\n",
      "130 994 66\n",
      "131 1005 67\n",
      "132 1014 68\n",
      "134 1034 69\n",
      "134 1030 70\n",
      "134 1030 71\n",
      "134 1028 72\n",
      "134 1026 73\n",
      "134 1026 74\n",
      "135 1037 75\n",
      "136 1050 76\n",
      "136 1050 77\n",
      "137 1059 78\n",
      "137 1059 79\n",
      "136 1036 80\n",
      "137 1051 81\n",
      "139 1073 82\n",
      "139 1069 83\n",
      "139 1067 84\n",
      "139 1067 85\n",
      "139 1067 86\n",
      "139 1067 87\n",
      "139 1067 88\n",
      "139 1057 89\n",
      "139 1055 90\n",
      "141 1077 91\n",
      "141 1077 92\n",
      "141 1077 93\n",
      "142 1088 94\n",
      "142 1088 95\n",
      "142 1082 96\n",
      "142 1078 97\n",
      "143 1087 98\n",
      "143 1083 99\n",
      "143 1079 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.38290001215670455 \ttest: 0.8666666666666667 17.87881318715921\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.0891056723177791 \ttest: 0.8666666666666667 17.99777409556595\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.04015305061921666 \ttest: 0.8533333333333334 18.069973175418816\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.02308056993457657 \ttest: 0.8533333333333334 18.121480756399002\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.015071427283284956 \ttest: 0.8533333333333334 18.160659786161204\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.010649922554681238 \ttest: 0.84 18.190960276392506\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.007942093317364333 \ttest: 0.84 18.214605770744367\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.006159474890708297 \ttest: 0.84 18.233188875438124\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.0049216132158937465 \ttest: 0.84 18.24787586626591\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.004025978331193426 \ttest: 0.84 18.259529707532945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.86111   0.81579   0.83784        38\n",
      "           1    0.82051   0.86486   0.84211        37\n",
      "\n",
      "    accuracy                        0.84000        75\n",
      "   macro avg    0.84081   0.84033   0.83997        75\n",
      "weighted avg    0.84108   0.84000   0.83994        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 143\n",
      "Average of depth: 2.300699300699301\n",
      "Number of nodes: 1079\n",
      "11 61 1\n",
      "19 113 2\n",
      "26 160 3\n",
      "34 218 4\n",
      "40 270 5\n",
      "43 293 6\n",
      "47 345 7\n",
      "53 403 8\n",
      "59 435 9\n",
      "62 460 10\n",
      "63 469 11\n",
      "63 467 12\n",
      "65 475 13\n",
      "67 497 14\n",
      "72 538 15\n",
      "72 538 16\n",
      "73 541 17\n",
      "74 544 18\n",
      "74 544 19\n",
      "74 542 20\n",
      "75 549 21\n",
      "79 583 22\n",
      "79 577 23\n",
      "82 614 24\n",
      "82 612 25\n",
      "82 608 26\n",
      "84 620 27\n",
      "88 664 28\n",
      "91 699 29\n",
      "91 699 30\n",
      "92 702 31\n",
      "93 709 32\n",
      "93 695 33\n",
      "93 691 34\n",
      "93 691 35\n",
      "94 696 36\n",
      "96 708 37\n",
      "96 708 38\n",
      "96 700 39\n",
      "97 707 40\n",
      "100 730 41\n",
      "104 758 42\n",
      "105 773 43\n",
      "107 783 44\n",
      "109 787 45\n",
      "109 783 46\n",
      "110 794 47\n",
      "112 816 48\n",
      "112 812 49\n",
      "116 844 50\n",
      "118 868 51\n",
      "118 868 52\n",
      "119 873 53\n",
      "120 882 54\n",
      "120 878 55\n",
      "121 881 56\n",
      "121 881 57\n",
      "124 918 58\n",
      "125 923 59\n",
      "126 934 60\n",
      "126 934 61\n",
      "126 928 62\n",
      "127 935 63\n",
      "129 957 64\n",
      "129 957 65\n",
      "130 966 66\n",
      "132 992 67\n",
      "133 1003 68\n",
      "134 1010 69\n",
      "134 1008 70\n",
      "136 1032 71\n",
      "137 1041 72\n",
      "138 1048 73\n",
      "138 1048 74\n",
      "140 1068 75\n",
      "140 1064 76\n",
      "140 1064 77\n",
      "141 1077 78\n",
      "141 1077 79\n",
      "141 1077 80\n",
      "141 1075 81\n",
      "144 1112 82\n",
      "145 1123 83\n",
      "145 1121 84\n",
      "145 1121 85\n",
      "145 1121 86\n",
      "145 1121 87\n",
      "145 1121 88\n",
      "145 1105 89\n",
      "146 1112 90\n",
      "147 1115 91\n",
      "147 1111 92\n",
      "147 1107 93\n",
      "150 1126 94\n",
      "151 1133 95\n",
      "151 1133 96\n",
      "151 1133 97\n",
      "151 1133 98\n",
      "151 1131 99\n",
      "152 1136 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.543276711456302 \ttest: 0.84 19.075381871340824\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.19474854573698594 \ttest: 0.8533333333333334 19.194261976674788\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.10002289794878072 \ttest: 0.84 19.42334098102817\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.060263438647295414 \ttest: 0.84 19.617796691525875\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.040031954007539006 \ttest: 0.84 19.77796145722472\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.028420174267132572 \ttest: 0.84 19.911547570834752\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.021172600120615163 \ttest: 0.84 20.024892942151396\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.016359602901580676 \ttest: 0.8266666666666667 20.122625970391162\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.013007090698987214 \ttest: 0.8266666666666667 20.208096628133106\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.010581736589177496 \ttest: 0.8266666666666667 20.28376046522121\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.82051   0.84211   0.83117        38\n",
      "           1    0.83333   0.81081   0.82192        37\n",
      "\n",
      "    accuracy                        0.82667        75\n",
      "   macro avg    0.82692   0.82646   0.82654        75\n",
      "weighted avg    0.82684   0.82667   0.82660        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 152\n",
      "Average of depth: 2.1907894736842106\n",
      "Number of nodes: 1136\n",
      "16 104 1\n",
      "22 154 2\n",
      "24 166 3\n",
      "30 202 4\n",
      "36 244 5\n",
      "41 283 6\n",
      "46 322 7\n",
      "48 342 8\n",
      "49 355 9\n",
      "51 369 10\n",
      "53 379 11\n",
      "56 394 12\n",
      "59 427 13\n",
      "63 457 14\n",
      "64 460 15\n",
      "67 489 16\n",
      "68 496 17\n",
      "70 510 18\n",
      "73 541 19\n",
      "76 564 20\n",
      "78 586 21\n",
      "82 624 22\n",
      "85 657 23\n",
      "88 686 24\n",
      "87 669 25\n",
      "88 672 26\n",
      "89 687 27\n",
      "90 694 28\n",
      "92 724 29\n",
      "94 740 30\n",
      "94 740 31\n",
      "94 736 32\n",
      "95 743 33\n",
      "96 744 34\n",
      "100 784 35\n",
      "103 817 36\n",
      "104 828 37\n",
      "106 842 38\n",
      "108 866 39\n",
      "109 877 40\n",
      "109 877 41\n",
      "111 893 42\n",
      "113 889 43\n",
      "114 892 44\n",
      "114 886 45\n",
      "114 886 46\n",
      "116 904 47\n",
      "116 902 48\n",
      "117 911 49\n",
      "117 909 50\n",
      "117 901 51\n",
      "120 938 52\n",
      "120 938 53\n",
      "121 947 54\n",
      "121 947 55\n",
      "121 947 56\n",
      "121 947 57\n",
      "122 950 58\n",
      "123 961 59\n",
      "126 994 60\n",
      "126 992 61\n",
      "126 992 62\n",
      "126 992 63\n",
      "126 992 64\n",
      "126 988 65\n",
      "127 1003 66\n",
      "127 993 67\n",
      "128 1000 68\n",
      "128 1000 69\n",
      "130 1010 70\n",
      "131 1013 71\n",
      "131 1011 72\n",
      "131 1007 73\n",
      "131 1003 74\n",
      "131 1003 75\n",
      "132 1008 76\n",
      "132 1008 77\n",
      "134 1030 78\n",
      "136 1048 79\n",
      "137 1049 80\n",
      "137 1049 81\n",
      "139 1073 82\n",
      "139 1071 83\n",
      "139 1069 84\n",
      "139 1069 85\n",
      "140 1078 86\n",
      "140 1076 87\n",
      "141 1089 88\n",
      "142 1096 89\n",
      "143 1111 90\n",
      "143 1105 91\n",
      "143 1103 92\n",
      "143 1103 93\n",
      "144 1114 94\n",
      "145 1125 95\n",
      "146 1132 96\n",
      "147 1135 97\n",
      "149 1161 98\n",
      "149 1161 99\n",
      "150 1172 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.18163295761243564 \ttest: 0.8666666666666667 18.17400430772793\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.046136442377960316 \ttest: 0.8666666666666667 18.1737963350758\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.021301660765490706 \ttest: 0.8666666666666667 18.24170723260775\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.012403593257213966 \ttest: 0.8666666666666667 18.32109192704957\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.008172721463084445 \ttest: 0.8533333333333334 18.39993885990379\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.005815538092054266 \ttest: 0.84 18.47482125460731\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.004361593932913183 \ttest: 0.84 18.544861887418104\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.003398763930520636 \ttest: 0.84 18.61005682519746\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.002726799243545522 \ttest: 0.84 18.67070687130508\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.002238486527457219 \ttest: 0.84 18.727203417652042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.82500   0.86842   0.84615        38\n",
      "           1    0.85714   0.81081   0.83333        37\n",
      "\n",
      "    accuracy                        0.84000        75\n",
      "   macro avg    0.84107   0.83962   0.83974        75\n",
      "weighted avg    0.84086   0.84000   0.83983        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 150\n",
      "Average of depth: 2.3266666666666667\n",
      "Number of nodes: 1172\n",
      "13 59 1\n",
      "20 112 2\n",
      "32 208 3\n",
      "40 280 4\n",
      "48 350 5\n",
      "54 396 6\n",
      "57 433 7\n",
      "60 458 8\n",
      "61 455 9\n",
      "61 455 10\n",
      "63 469 11\n",
      "68 498 12\n",
      "69 505 13\n",
      "72 528 14\n",
      "71 511 15\n",
      "75 547 16\n",
      "76 558 17\n",
      "79 581 18\n",
      "82 618 19\n",
      "85 655 20\n",
      "86 656 21\n",
      "87 665 22\n",
      "91 703 23\n",
      "93 723 24\n",
      "95 737 25\n",
      "95 737 26\n",
      "95 733 27\n",
      "96 744 28\n",
      "98 762 29\n",
      "99 767 30\n",
      "99 765 31\n",
      "100 776 32\n",
      "100 768 33\n",
      "102 784 34\n",
      "102 784 35\n",
      "102 780 36\n",
      "105 809 37\n",
      "107 835 38\n",
      "109 857 39\n",
      "110 864 40\n",
      "111 871 41\n",
      "111 869 42\n",
      "111 869 43\n",
      "112 880 44\n",
      "113 879 45\n",
      "114 886 46\n",
      "116 890 47\n",
      "117 891 48\n",
      "121 931 49\n",
      "121 927 50\n",
      "124 958 51\n",
      "125 963 52\n",
      "126 968 53\n",
      "127 975 54\n",
      "127 975 55\n",
      "127 973 56\n",
      "127 955 57\n",
      "127 955 58\n",
      "129 977 59\n",
      "131 997 60\n",
      "131 995 61\n",
      "131 991 62\n",
      "131 991 63\n",
      "131 991 64\n",
      "134 1026 65\n",
      "135 1041 66\n",
      "136 1056 67\n",
      "136 1056 68\n",
      "136 1056 69\n",
      "137 1067 70\n",
      "137 1067 71\n",
      "138 1078 72\n",
      "139 1081 73\n",
      "140 1090 74\n",
      "140 1090 75\n",
      "140 1090 76\n",
      "140 1088 77\n",
      "141 1101 78\n",
      "143 1117 79\n",
      "143 1115 80\n",
      "143 1115 81\n",
      "143 1113 82\n",
      "143 1109 83\n",
      "143 1109 84\n",
      "145 1129 85\n",
      "146 1142 86\n",
      "146 1142 87\n",
      "146 1142 88\n",
      "147 1151 89\n",
      "147 1151 90\n",
      "148 1164 91\n",
      "149 1175 92\n",
      "151 1189 93\n",
      "151 1189 94\n",
      "153 1211 95\n",
      "153 1207 96\n",
      "154 1214 97\n",
      "154 1214 98\n",
      "154 1214 99\n",
      "154 1214 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.2583228032802272 \ttest: 0.84 18.241951378338733\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.08242863483254984 \ttest: 0.84 18.236357434239416\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.04048266497394754 \ttest: 0.8533333333333334 18.21968048880912\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.02403714886153557 \ttest: 0.8533333333333334 18.203205994222124\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.015916797724454437 \ttest: 0.8533333333333334 18.192203263682554\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.011316644397886355 \ttest: 0.8533333333333334 18.186572058009332\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.008459583453008646 \ttest: 0.8533333333333334 18.185111755110913\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0065638426126692915 \ttest: 0.8666666666666667 18.186682348010834\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.005241627642808897 \ttest: 0.8666666666666667 18.190412294634847\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.004282745267426894 \ttest: 0.8666666666666667 18.195673295663447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.86842   0.86842   0.86842        38\n",
      "           1    0.86486   0.86486   0.86486        37\n",
      "\n",
      "    accuracy                        0.86667        75\n",
      "   macro avg    0.86664   0.86664   0.86664        75\n",
      "weighted avg    0.86667   0.86667   0.86667        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 154\n",
      "Average of depth: 2.344155844155844\n",
      "Number of nodes: 1214\n",
      "15 109 1\n",
      "24 164 2\n",
      "30 212 3\n",
      "34 238 4\n",
      "38 266 5\n",
      "41 301 6\n",
      "43 319 7\n",
      "47 349 8\n",
      "48 364 9\n",
      "51 385 10\n",
      "57 439 11\n",
      "57 435 12\n",
      "61 459 13\n",
      "68 510 14\n",
      "72 550 15\n",
      "79 607 16\n",
      "81 619 17\n",
      "83 645 18\n",
      "82 636 19\n",
      "84 646 20\n",
      "87 665 21\n",
      "88 680 22\n",
      "92 728 23\n",
      "94 738 24\n",
      "95 739 25\n",
      "95 735 26\n",
      "97 749 27\n",
      "98 762 28\n",
      "98 758 29\n",
      "101 777 30\n",
      "102 784 31\n",
      "102 788 32\n",
      "104 806 33\n",
      "105 815 34\n",
      "106 824 35\n",
      "108 836 36\n",
      "108 832 37\n",
      "110 856 38\n",
      "110 852 39\n",
      "111 859 40\n",
      "112 870 41\n",
      "112 870 42\n",
      "113 873 43\n",
      "113 863 44\n",
      "115 887 45\n",
      "117 903 46\n",
      "118 910 47\n",
      "118 908 48\n",
      "118 902 49\n",
      "120 922 50\n",
      "122 944 51\n",
      "124 962 52\n",
      "124 962 53\n",
      "125 973 54\n",
      "125 973 55\n",
      "126 980 56\n",
      "126 980 57\n",
      "127 991 58\n",
      "128 1000 59\n",
      "130 1022 60\n",
      "130 1016 61\n",
      "130 1006 62\n",
      "134 1048 63\n",
      "135 1059 64\n",
      "135 1059 65\n",
      "135 1057 66\n",
      "135 1057 67\n",
      "137 1075 68\n",
      "137 1075 69\n",
      "138 1084 70\n",
      "139 1095 71\n",
      "141 1117 72\n",
      "142 1128 73\n",
      "142 1124 74\n",
      "142 1120 75\n",
      "142 1120 76\n",
      "143 1127 77\n",
      "144 1130 78\n",
      "146 1160 79\n",
      "146 1160 80\n",
      "146 1154 81\n",
      "146 1154 82\n",
      "146 1154 83\n",
      "147 1163 84\n",
      "147 1163 85\n",
      "147 1161 86\n",
      "148 1168 87\n",
      "149 1173 88\n",
      "152 1196 89\n",
      "153 1205 90\n",
      "153 1205 91\n",
      "153 1205 92\n",
      "153 1205 93\n",
      "153 1201 94\n",
      "156 1222 95\n",
      "156 1222 96\n",
      "157 1233 97\n",
      "157 1233 98\n",
      "157 1233 99\n",
      "158 1246 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.23380699877533578 \ttest: 0.88 14.74886157997034\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.07948903815050107 \ttest: 0.88 14.940789807405663\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.039959252337784405 \ttest: 0.88 15.069574825315492\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.023992577587316717 \ttest: 0.88 15.169020289787621\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.015986451468449216 \ttest: 0.88 15.249206124162537\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.011409798929949087 \ttest: 0.88 15.315635430038263\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.008550864800786014 \ttest: 0.88 15.371750489462443\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.006646437631090431 \ttest: 0.88 15.419876900206816\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.005314468058565506 \ttest: 0.88 15.461665742299187\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.004346533956536289 \ttest: 0.88 15.498330693277019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.85366   0.92105   0.88608        38\n",
      "           1    0.91176   0.83784   0.87324        37\n",
      "\n",
      "    accuracy                        0.88000        75\n",
      "   macro avg    0.88271   0.87945   0.87966        75\n",
      "weighted avg    0.88232   0.88000   0.87974        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 158\n",
      "Average of depth: 2.3607594936708862\n",
      "Number of nodes: 1246\n",
      "8 36 1\n",
      "13 65 2\n",
      "15 71 3\n",
      "19 93 4\n",
      "29 179 5\n",
      "40 256 6\n",
      "42 272 7\n",
      "48 304 8\n",
      "52 342 9\n",
      "53 349 10\n",
      "55 363 11\n",
      "58 386 12\n",
      "62 414 13\n",
      "67 457 14\n",
      "69 471 15\n",
      "71 485 16\n",
      "74 512 17\n",
      "76 520 18\n",
      "79 551 19\n",
      "79 551 20\n",
      "79 551 21\n",
      "80 558 22\n",
      "82 580 23\n",
      "83 583 24\n",
      "88 618 25\n",
      "88 618 26\n",
      "90 644 27\n",
      "90 644 28\n",
      "93 675 29\n",
      "94 690 30\n",
      "94 690 31\n",
      "94 688 32\n",
      "95 689 33\n",
      "95 689 34\n",
      "95 689 35\n",
      "96 696 36\n",
      "97 699 37\n",
      "97 699 38\n",
      "99 713 39\n",
      "101 739 40\n",
      "102 740 41\n",
      "102 740 42\n",
      "102 740 43\n",
      "102 738 44\n",
      "103 749 45\n",
      "104 754 46\n",
      "104 754 47\n",
      "104 754 48\n",
      "105 765 49\n",
      "105 765 50\n",
      "105 765 51\n",
      "106 776 52\n",
      "108 792 53\n",
      "109 803 54\n",
      "110 812 55\n",
      "110 810 56\n",
      "110 800 57\n",
      "111 799 58\n",
      "111 799 59\n",
      "111 799 60\n",
      "113 819 61\n",
      "113 819 62\n",
      "117 855 63\n",
      "117 855 64\n",
      "118 866 65\n",
      "119 877 66\n",
      "120 888 67\n",
      "120 888 68\n",
      "120 888 69\n",
      "120 888 70\n",
      "123 921 71\n",
      "123 921 72\n",
      "123 915 73\n",
      "123 907 74\n",
      "123 907 75\n",
      "123 907 76\n",
      "124 914 77\n",
      "123 901 78\n",
      "124 914 79\n",
      "127 947 80\n",
      "127 947 81\n",
      "128 960 82\n",
      "129 969 83\n",
      "130 980 84\n",
      "131 991 85\n",
      "131 991 86\n",
      "131 985 87\n",
      "132 994 88\n",
      "132 988 89\n",
      "133 1003 90\n",
      "134 1018 91\n",
      "135 1025 92\n",
      "135 1025 93\n",
      "136 1030 94\n",
      "137 1033 95\n",
      "139 1051 96\n",
      "139 1045 97\n",
      "140 1056 98\n",
      "142 1080 99\n",
      "144 1102 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.1967123238898308 \ttest: 0.8266666666666667 22.291565101754138\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.05557574858656563 \ttest: 0.8266666666666667 23.07408397936114\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.02706380234691943 \ttest: 0.8266666666666667 23.435408534452364\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.016242245190250363 \ttest: 0.8266666666666667 23.65217023909535\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.010900291854633683 \ttest: 0.8266666666666667 23.79969360461007\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.007848886963255312 \ttest: 0.8266666666666667 23.90775824877816\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.005934263922015782 \ttest: 0.8266666666666667 23.990859934605457\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.004650931116742703 \ttest: 0.8266666666666667 24.057016465674977\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.003747354327658741 \ttest: 0.8266666666666667 24.111066244899135\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.003086378221979065 \ttest: 0.8266666666666667 24.156121853805466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.85714   0.78947   0.82192        38\n",
      "           1    0.80000   0.86486   0.83117        37\n",
      "\n",
      "    accuracy                        0.82667        75\n",
      "   macro avg    0.82857   0.82717   0.82654        75\n",
      "weighted avg    0.82895   0.82667   0.82648        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 144\n",
      "Average of depth: 2.326388888888889\n",
      "Number of nodes: 1102\n",
      "17 115 1\n",
      "24 174 2\n",
      "28 204 3\n",
      "35 259 4\n",
      "40 290 5\n",
      "42 304 6\n",
      "46 344 7\n",
      "49 369 8\n",
      "52 382 9\n",
      "53 387 10\n",
      "54 394 11\n",
      "54 394 12\n",
      "61 449 13\n",
      "61 441 14\n",
      "62 446 15\n",
      "65 465 16\n",
      "67 479 17\n",
      "68 488 18\n",
      "68 480 19\n",
      "71 505 20\n",
      "77 571 21\n",
      "78 576 22\n",
      "79 583 23\n",
      "81 605 24\n",
      "82 604 25\n",
      "83 611 26\n",
      "85 635 27\n",
      "85 627 28\n",
      "87 641 29\n",
      "89 661 30\n",
      "91 679 31\n",
      "92 690 32\n",
      "94 702 33\n",
      "94 702 34\n",
      "94 700 35\n",
      "96 704 36\n",
      "96 704 37\n",
      "97 715 38\n",
      "97 715 39\n",
      "99 731 40\n",
      "100 736 41\n",
      "101 743 42\n",
      "101 739 43\n",
      "101 737 44\n",
      "102 746 45\n",
      "102 746 46\n",
      "103 753 47\n",
      "106 764 48\n",
      "107 779 49\n",
      "108 786 50\n",
      "108 786 51\n",
      "111 819 52\n",
      "111 819 53\n",
      "113 849 54\n",
      "114 864 55\n",
      "115 879 56\n",
      "116 890 57\n",
      "118 918 58\n",
      "119 927 59\n",
      "120 938 60\n",
      "122 956 61\n",
      "122 956 62\n",
      "122 952 63\n",
      "122 952 64\n",
      "123 959 65\n",
      "123 959 66\n",
      "123 959 67\n",
      "124 966 68\n",
      "124 962 69\n",
      "126 984 70\n",
      "126 984 71\n",
      "129 1019 72\n",
      "130 1032 73\n",
      "130 1032 74\n",
      "130 1032 75\n",
      "131 1043 76\n",
      "132 1056 77\n",
      "133 1071 78\n",
      "133 1071 79\n",
      "136 1100 80\n",
      "136 1100 81\n",
      "136 1100 82\n",
      "136 1100 83\n",
      "137 1109 84\n",
      "139 1137 85\n",
      "139 1137 86\n",
      "139 1133 87\n",
      "139 1131 88\n",
      "139 1131 89\n",
      "139 1131 90\n",
      "141 1153 91\n",
      "141 1149 92\n",
      "141 1149 93\n",
      "141 1149 94\n",
      "142 1156 95\n",
      "142 1156 96\n",
      "143 1169 97\n",
      "144 1176 98\n",
      "144 1176 99\n",
      "144 1176 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.40039973584482835 \ttest: 0.8933333333333333 11.995582513798388\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.11647559639429664 \ttest: 0.9066666666666666 12.03307619298426\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.0600065696640741 \ttest: 0.9066666666666666 12.064623281285575\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.038118411869917095 \ttest: 0.9066666666666666 12.093878876061492\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.026957808357668978 \ttest: 0.9066666666666666 12.119962802983787\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.02034356175913376 \ttest: 0.9066666666666666 12.14324159240675\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.016034018341013437 \ttest: 0.9066666666666666 12.164226256603504\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.013037494377094616 \ttest: 0.8933333333333333 12.183329469140382\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.01085300600402251 \ttest: 0.8933333333333333 12.200863569601609\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.009202193669401854 \ttest: 0.8933333333333333 12.217067537219899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.89474   0.89474   0.89474        38\n",
      "           1    0.89189   0.89189   0.89189        37\n",
      "\n",
      "    accuracy                        0.89333        75\n",
      "   macro avg    0.89331   0.89331   0.89331        75\n",
      "weighted avg    0.89333   0.89333   0.89333        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 144\n",
      "Average of depth: 2.375\n",
      "Number of nodes: 1176\n",
      "19 135 1\n",
      "23 165 2\n",
      "26 192 3\n",
      "31 227 4\n",
      "36 264 5\n",
      "39 279 6\n",
      "43 315 7\n",
      "45 333 8\n",
      "49 363 9\n",
      "55 415 10\n",
      "58 440 11\n",
      "61 455 12\n",
      "65 485 13\n",
      "66 492 14\n",
      "73 543 15\n",
      "76 558 16\n",
      "79 587 17\n",
      "80 596 18\n",
      "84 620 19\n",
      "85 627 20\n",
      "86 638 21\n",
      "90 682 22\n",
      "92 704 23\n",
      "93 711 24\n",
      "95 735 25\n",
      "95 731 26\n",
      "97 741 27\n",
      "98 744 28\n",
      "100 754 29\n",
      "102 758 30\n",
      "104 780 31\n",
      "104 770 32\n",
      "105 775 33\n",
      "106 786 34\n",
      "106 786 35\n",
      "107 791 36\n",
      "107 791 37\n",
      "109 809 38\n",
      "110 814 39\n",
      "112 822 40\n",
      "112 822 41\n",
      "113 827 42\n",
      "114 836 43\n",
      "116 854 44\n",
      "116 852 45\n",
      "117 861 46\n",
      "117 861 47\n",
      "119 883 48\n",
      "120 882 49\n",
      "120 880 50\n",
      "121 887 51\n",
      "123 909 52\n",
      "124 916 53\n",
      "124 916 54\n",
      "125 923 55\n",
      "125 919 56\n",
      "129 951 57\n",
      "130 962 58\n",
      "131 971 59\n",
      "132 972 60\n",
      "132 970 61\n",
      "134 996 62\n",
      "136 1014 63\n",
      "137 1021 64\n",
      "138 1034 65\n",
      "139 1043 66\n",
      "139 1043 67\n",
      "142 1080 68\n",
      "144 1096 69\n",
      "144 1096 70\n",
      "144 1090 71\n",
      "144 1082 72\n",
      "144 1080 73\n",
      "144 1080 74\n",
      "144 1080 75\n",
      "144 1080 76\n",
      "144 1078 77\n",
      "148 1120 78\n",
      "148 1120 79\n",
      "150 1136 80\n",
      "151 1147 81\n",
      "152 1158 82\n",
      "152 1154 83\n",
      "153 1167 84\n",
      "155 1185 85\n",
      "155 1185 86\n",
      "157 1215 87\n",
      "158 1228 88\n",
      "158 1228 89\n",
      "161 1261 90\n",
      "161 1255 91\n",
      "161 1255 92\n",
      "161 1251 93\n",
      "161 1247 94\n",
      "162 1246 95\n",
      "163 1261 96\n",
      "164 1276 97\n",
      "164 1276 98\n",
      "164 1276 99\n",
      "164 1276 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.5683201664314128 \ttest: 0.84 21.48589080446655\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.20328298898732144 \ttest: 0.84 21.390147971910235\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.10602585464645135 \ttest: 0.84 21.499615595033514\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.06495963148739176 \ttest: 0.84 21.600987374541567\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.043785053258597974 \ttest: 0.84 21.680183296752844\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.0314652347238457 \ttest: 0.84 21.741493596093413\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.023679873240830807 \ttest: 0.84 21.789635376127436\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.018453186788283255 \ttest: 0.84 21.828063905634703\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.014777868530413012 \ttest: 0.84 21.859186498166988\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.012096941304535723 \ttest: 0.84 21.88469148632702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.77778   0.94595   0.85366        37\n",
      "           1    0.93333   0.73684   0.82353        38\n",
      "\n",
      "    accuracy                        0.84000        75\n",
      "   macro avg    0.85556   0.84139   0.83859        75\n",
      "weighted avg    0.85659   0.84000   0.83839        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 164\n",
      "Average of depth: 2.3109756097560976\n",
      "Number of nodes: 1276\n",
      "20 128 1\n",
      "29 197 2\n",
      "33 233 3\n",
      "37 243 4\n",
      "42 284 5\n",
      "44 302 6\n",
      "50 364 7\n",
      "51 373 8\n",
      "55 393 9\n",
      "58 416 10\n",
      "59 417 11\n",
      "64 458 12\n",
      "66 478 13\n",
      "69 491 14\n",
      "71 497 15\n",
      "77 533 16\n",
      "78 532 17\n",
      "80 548 18\n",
      "84 576 19\n",
      "86 598 20\n",
      "87 605 21\n",
      "87 605 22\n",
      "90 632 23\n",
      "92 652 24\n",
      "94 662 25\n",
      "96 666 26\n",
      "98 674 27\n",
      "98 672 28\n",
      "98 672 29\n",
      "99 679 30\n",
      "99 679 31\n",
      "99 679 32\n",
      "99 679 33\n",
      "101 691 34\n",
      "104 722 35\n",
      "104 714 36\n",
      "104 708 37\n",
      "107 731 38\n",
      "108 744 39\n",
      "109 753 40\n",
      "109 753 41\n",
      "109 743 42\n",
      "113 781 43\n",
      "115 801 44\n",
      "115 797 45\n",
      "115 787 46\n",
      "116 790 47\n",
      "117 801 48\n",
      "117 801 49\n",
      "117 801 50\n",
      "117 801 51\n",
      "118 812 52\n",
      "118 812 53\n",
      "120 838 54\n",
      "120 834 55\n",
      "120 828 56\n",
      "120 828 57\n",
      "120 828 58\n",
      "122 850 59\n",
      "123 861 60\n",
      "123 855 61\n",
      "123 855 62\n",
      "126 870 63\n",
      "126 870 64\n",
      "128 894 65\n",
      "130 906 66\n",
      "130 904 67\n",
      "130 904 68\n",
      "130 900 69\n",
      "130 896 70\n",
      "130 896 71\n",
      "130 896 72\n",
      "131 899 73\n",
      "134 932 74\n",
      "134 930 75\n",
      "137 963 76\n",
      "137 961 77\n",
      "137 959 78\n",
      "137 959 79\n",
      "138 968 80\n",
      "139 983 81\n",
      "139 981 82\n",
      "140 996 83\n",
      "140 996 84\n",
      "140 990 85\n",
      "140 990 86\n",
      "140 990 87\n",
      "142 1000 88\n",
      "142 1000 89\n",
      "143 1015 90\n",
      "144 1014 91\n",
      "144 1012 92\n",
      "147 1053 93\n",
      "147 1051 94\n",
      "146 1042 95\n",
      "148 1062 96\n",
      "148 1062 97\n",
      "150 1078 98\n",
      "152 1104 99\n",
      "152 1100 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.25398243249812535 \ttest: 0.8933333333333333 9.601180856850057\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.06754959537717145 \ttest: 0.8933333333333333 9.818656313387987\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.0329219190559691 \ttest: 0.8933333333333333 9.95189363295895\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.01976861438085548 \ttest: 0.8933333333333333 10.046348409972323\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.013268414246166708 \ttest: 0.8933333333333333 10.121654723847726\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.009556570844611523 \ttest: 0.9066666666666666 10.185331871916855\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.007229426545338525 \ttest: 0.9066666666666666 10.240839846177302\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.005670840117329588 \ttest: 0.8933333333333333 10.290081876533112\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.00457413263023395 \ttest: 0.8933333333333333 10.334271742091152\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0037721763058371624 \ttest: 0.8933333333333333 10.374267508286664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.89189   0.89189   0.89189        37\n",
      "           1    0.89474   0.89474   0.89474        38\n",
      "\n",
      "    accuracy                        0.89333        75\n",
      "   macro avg    0.89331   0.89331   0.89331        75\n",
      "weighted avg    0.89333   0.89333   0.89333        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 152\n",
      "Average of depth: 2.1973684210526314\n",
      "Number of nodes: 1100\n",
      "17 105 1\n",
      "26 182 2\n",
      "31 227 3\n",
      "38 278 4\n",
      "49 365 5\n",
      "54 404 6\n",
      "60 452 7\n",
      "61 455 8\n",
      "67 505 9\n",
      "72 544 10\n",
      "75 571 11\n",
      "79 601 12\n",
      "81 615 13\n",
      "84 644 14\n",
      "86 660 15\n",
      "88 666 16\n",
      "91 691 17\n",
      "91 691 18\n",
      "91 691 19\n",
      "91 683 20\n",
      "91 677 21\n",
      "92 684 22\n",
      "92 680 23\n",
      "94 704 24\n",
      "96 724 25\n",
      "97 735 26\n",
      "97 735 27\n",
      "97 721 28\n",
      "98 724 29\n",
      "98 720 30\n",
      "99 729 31\n",
      "100 740 32\n",
      "101 741 33\n",
      "101 741 34\n",
      "102 750 35\n",
      "104 774 36\n",
      "105 773 37\n",
      "107 791 38\n",
      "109 819 39\n",
      "110 810 40\n",
      "111 809 41\n",
      "111 809 42\n",
      "111 809 43\n",
      "112 820 44\n",
      "114 838 45\n",
      "115 835 46\n",
      "115 829 47\n",
      "117 847 48\n",
      "118 862 49\n",
      "118 858 50\n",
      "121 889 51\n",
      "121 889 52\n",
      "121 889 53\n",
      "121 885 54\n",
      "120 876 55\n",
      "121 885 56\n",
      "123 905 57\n",
      "123 905 58\n",
      "123 905 59\n",
      "125 921 60\n",
      "125 921 61\n",
      "125 921 62\n",
      "126 922 63\n",
      "129 955 64\n",
      "130 970 65\n",
      "130 964 66\n",
      "132 986 67\n",
      "132 986 68\n",
      "131 975 69\n",
      "131 975 70\n",
      "132 990 71\n",
      "132 990 72\n",
      "132 990 73\n",
      "132 988 74\n",
      "132 988 75\n",
      "132 988 76\n",
      "134 1004 77\n",
      "135 1015 78\n",
      "136 1028 79\n",
      "136 1028 80\n",
      "136 1028 81\n",
      "137 1037 82\n",
      "137 1033 83\n",
      "138 1040 84\n",
      "138 1034 85\n",
      "138 1034 86\n",
      "139 1045 87\n",
      "139 1043 88\n",
      "140 1048 89\n",
      "140 1044 90\n",
      "141 1055 91\n",
      "142 1066 92\n",
      "142 1066 93\n",
      "142 1066 94\n",
      "142 1066 95\n",
      "143 1075 96\n",
      "143 1073 97\n",
      "143 1073 98\n",
      "143 1071 99\n",
      "143 1069 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.24569982989419065 \ttest: 0.88 16.25818655423646\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.08854651722472105 \ttest: 0.88 16.276938203485912\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.04728208048113812 \ttest: 0.88 16.315484282718202\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.029695548684390897 \ttest: 0.88 16.349380469068993\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.020456568873768954 \ttest: 0.88 16.377342719685416\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.01497390096910239 \ttest: 0.88 16.400491382980082\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.011445036987144618 \ttest: 0.88 16.419946089117232\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.009036807689730725 \ttest: 0.88 16.436558618368732\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.007318723022027243 \ttest: 0.88 16.450949642295136\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.006049417818363467 \ttest: 0.88 16.463572845162922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96667   0.78378   0.86567        37\n",
      "           1    0.82222   0.97368   0.89157        38\n",
      "\n",
      "    accuracy                        0.88000        75\n",
      "   macro avg    0.89444   0.87873   0.87862        75\n",
      "weighted avg    0.89348   0.88000   0.87879        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 143\n",
      "Average of depth: 2.2797202797202796\n",
      "Number of nodes: 1069\n",
      "15 105 1\n",
      "31 235 2\n",
      "39 281 3\n",
      "50 374 4\n",
      "54 408 5\n",
      "58 414 6\n",
      "61 437 7\n",
      "62 440 8\n",
      "68 506 9\n",
      "71 529 10\n",
      "74 562 11\n",
      "74 560 12\n",
      "75 571 13\n",
      "78 586 14\n",
      "77 567 15\n",
      "77 567 16\n",
      "78 572 17\n",
      "78 572 18\n",
      "83 621 19\n",
      "84 632 20\n",
      "85 631 21\n",
      "87 653 22\n",
      "88 664 23\n",
      "89 669 24\n",
      "92 696 25\n",
      "92 696 26\n",
      "96 738 27\n",
      "97 743 28\n",
      "99 759 29\n",
      "99 759 30\n",
      "99 759 31\n",
      "102 786 32\n",
      "102 786 33\n",
      "104 798 34\n",
      "105 803 35\n",
      "107 819 36\n",
      "109 827 37\n",
      "109 827 38\n",
      "109 823 39\n",
      "110 838 40\n",
      "111 841 41\n",
      "112 850 42\n",
      "113 855 43\n",
      "113 855 44\n",
      "113 851 45\n",
      "114 866 46\n",
      "114 864 47\n",
      "115 873 48\n",
      "116 878 49\n",
      "116 878 50\n",
      "117 885 51\n",
      "118 894 52\n",
      "119 903 53\n",
      "119 903 54\n",
      "121 919 55\n",
      "122 934 56\n",
      "122 924 57\n",
      "122 924 58\n",
      "122 924 59\n",
      "122 924 60\n",
      "123 933 61\n",
      "123 933 62\n",
      "125 963 63\n",
      "126 974 64\n",
      "126 966 65\n",
      "128 988 66\n",
      "128 978 67\n",
      "129 987 68\n",
      "130 998 69\n",
      "131 1013 70\n",
      "132 1020 71\n",
      "135 1045 72\n",
      "135 1043 73\n",
      "136 1058 74\n",
      "136 1058 75\n",
      "136 1054 76\n",
      "136 1052 77\n",
      "136 1052 78\n",
      "138 1074 79\n",
      "138 1066 80\n",
      "138 1066 81\n",
      "138 1066 82\n",
      "140 1080 83\n",
      "140 1078 84\n",
      "141 1089 85\n",
      "142 1104 86\n",
      "142 1098 87\n",
      "143 1109 88\n",
      "143 1107 89\n",
      "144 1118 90\n",
      "144 1116 91\n",
      "144 1112 92\n",
      "144 1112 93\n",
      "145 1117 94\n",
      "149 1157 95\n",
      "151 1173 96\n",
      "151 1173 97\n",
      "151 1173 98\n",
      "152 1182 99\n",
      "153 1185 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.11839851094649541 \ttest: 0.88 18.92463748484071\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.032521505596514004 \ttest: 0.84 19.567157850361895\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.015331073239955424 \ttest: 0.84 19.94088956964093\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.008990875848585272 \ttest: 0.84 20.199416271460734\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.005936704530121413 \ttest: 0.8266666666666667 20.39711209109223\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.004224529227593859 \ttest: 0.8266666666666667 20.55718830456101\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.0031654001584516165 \ttest: 0.8266666666666667 20.6916710504389\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0024632419506683736 \ttest: 0.8266666666666667 20.807606735836657\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.0019731345494206323 \ttest: 0.8266666666666667 20.909483801157684\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0016171290938292568 \ttest: 0.8266666666666667 21.00033899365598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.82051   0.84211   0.83117        38\n",
      "           1    0.83333   0.81081   0.82192        37\n",
      "\n",
      "    accuracy                        0.82667        75\n",
      "   macro avg    0.82692   0.82646   0.82654        75\n",
      "weighted avg    0.82684   0.82667   0.82660        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 153\n",
      "Average of depth: 2.287581699346405\n",
      "Number of nodes: 1185\n",
      "18 104 1\n",
      "24 170 2\n",
      "27 191 3\n",
      "32 234 4\n",
      "36 270 5\n",
      "38 276 6\n",
      "40 290 7\n",
      "45 335 8\n",
      "53 413 9\n",
      "57 441 10\n",
      "59 451 11\n",
      "66 514 12\n",
      "67 515 13\n",
      "68 508 14\n",
      "71 525 15\n",
      "73 551 16\n",
      "77 583 17\n",
      "80 596 18\n",
      "82 618 19\n",
      "83 625 20\n",
      "83 625 21\n",
      "85 643 22\n",
      "88 672 23\n",
      "89 667 24\n",
      "94 708 25\n",
      "94 708 26\n",
      "94 704 27\n",
      "94 702 28\n",
      "96 716 29\n",
      "97 721 30\n",
      "97 721 31\n",
      "98 728 32\n",
      "100 754 33\n",
      "100 754 34\n",
      "100 750 35\n",
      "103 775 36\n",
      "104 776 37\n",
      "104 776 38\n",
      "108 806 39\n",
      "107 793 40\n",
      "109 813 41\n",
      "110 814 42\n",
      "110 812 43\n",
      "113 841 44\n",
      "114 856 45\n",
      "115 867 46\n",
      "115 867 47\n",
      "117 883 48\n",
      "118 890 49\n",
      "118 890 50\n",
      "117 881 51\n",
      "117 877 52\n",
      "117 877 53\n",
      "118 884 54\n",
      "118 878 55\n",
      "121 903 56\n",
      "121 903 57\n",
      "124 930 58\n",
      "126 948 59\n",
      "126 946 60\n",
      "127 953 61\n",
      "128 958 62\n",
      "128 956 63\n",
      "130 978 64\n",
      "131 993 65\n",
      "133 1013 66\n",
      "134 1028 67\n",
      "136 1052 68\n",
      "136 1050 69\n",
      "137 1055 70\n",
      "139 1067 71\n",
      "139 1065 72\n",
      "140 1074 73\n",
      "141 1083 74\n",
      "141 1077 75\n",
      "143 1107 76\n",
      "143 1107 77\n",
      "143 1107 78\n",
      "143 1107 79\n",
      "143 1107 80\n",
      "143 1105 81\n",
      "143 1105 82\n",
      "143 1103 83\n",
      "143 1103 84\n",
      "143 1103 85\n",
      "144 1106 86\n",
      "144 1100 87\n",
      "144 1100 88\n",
      "144 1100 89\n",
      "144 1100 90\n",
      "144 1096 91\n",
      "145 1107 92\n",
      "145 1107 93\n",
      "146 1122 94\n",
      "146 1122 95\n",
      "146 1122 96\n",
      "146 1122 97\n",
      "146 1122 98\n",
      "147 1129 99\n",
      "147 1129 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.2340792396345897 \ttest: 0.8533333333333334 15.377104734097859\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.06680092485748318 \ttest: 0.8533333333333334 16.190487772181733\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.03234445442144863 \ttest: 0.8533333333333334 16.58888782684874\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.019216563485136447 \ttest: 0.8533333333333334 16.857194458442052\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.012779545135196196 \ttest: 0.8533333333333334 17.057220166415693\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.009133383714337839 \ttest: 0.8533333333333334 17.21551865908475\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.006863230426671086 \ttest: 0.8666666666666667 17.345944497469667\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.005351710390015085 \ttest: 0.8666666666666667 17.45655687750222\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.004293488236255314 \ttest: 0.8666666666666667 17.55241579516269\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0035231276529192673 \ttest: 0.8666666666666667 17.636887825272865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.85000   0.89474   0.87179        38\n",
      "           1    0.88571   0.83784   0.86111        37\n",
      "\n",
      "    accuracy                        0.86667        75\n",
      "   macro avg    0.86786   0.86629   0.86645        75\n",
      "weighted avg    0.86762   0.86667   0.86652        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 147\n",
      "Average of depth: 2.326530612244898\n",
      "Number of nodes: 1129\n",
      "20 154 1\n",
      "27 197 2\n",
      "32 236 3\n",
      "40 290 4\n",
      "48 366 5\n",
      "52 386 6\n",
      "54 400 7\n",
      "57 429 8\n",
      "64 484 9\n",
      "67 503 10\n",
      "69 517 11\n",
      "70 518 12\n",
      "75 567 13\n",
      "76 564 14\n",
      "76 564 15\n",
      "79 589 16\n",
      "80 600 17\n",
      "85 641 18\n",
      "86 654 19\n",
      "89 679 20\n",
      "92 696 21\n",
      "93 707 22\n",
      "95 725 23\n",
      "96 728 24\n",
      "99 757 25\n",
      "102 782 26\n",
      "103 787 27\n",
      "108 834 28\n",
      "109 843 29\n",
      "109 839 30\n",
      "110 844 31\n",
      "114 878 32\n",
      "115 887 33\n",
      "116 886 34\n",
      "116 886 35\n",
      "117 893 36\n",
      "118 904 37\n",
      "120 922 38\n",
      "120 922 39\n",
      "121 933 40\n",
      "121 933 41\n",
      "121 933 42\n",
      "121 933 43\n",
      "121 933 44\n",
      "123 957 45\n",
      "123 951 46\n",
      "123 947 47\n",
      "125 961 48\n",
      "126 972 49\n",
      "127 975 50\n",
      "127 973 51\n",
      "128 984 52\n",
      "129 995 53\n",
      "131 1015 54\n",
      "131 1015 55\n",
      "133 1043 56\n",
      "134 1054 57\n",
      "136 1076 58\n",
      "136 1074 59\n",
      "136 1074 60\n",
      "137 1085 61\n",
      "138 1096 62\n",
      "139 1101 63\n",
      "139 1101 64\n",
      "140 1112 65\n",
      "141 1119 66\n",
      "143 1137 67\n",
      "144 1148 68\n",
      "145 1153 69\n",
      "145 1153 70\n",
      "145 1151 71\n",
      "145 1151 72\n",
      "144 1136 73\n",
      "145 1141 74\n",
      "146 1150 75\n",
      "146 1150 76\n",
      "147 1159 77\n",
      "147 1159 78\n",
      "150 1192 79\n",
      "151 1207 80\n",
      "151 1207 81\n",
      "153 1231 82\n",
      "154 1242 83\n",
      "154 1238 84\n",
      "156 1262 85\n",
      "156 1262 86\n",
      "156 1258 87\n",
      "156 1250 88\n",
      "157 1259 89\n",
      "157 1259 90\n",
      "158 1272 91\n",
      "158 1272 92\n",
      "158 1270 93\n",
      "159 1279 94\n",
      "159 1275 95\n",
      "159 1269 96\n",
      "159 1269 97\n",
      "159 1261 98\n",
      "162 1290 99\n",
      "162 1290 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.14899953145008077 \ttest: 0.88 17.03366366583703\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.044092843846253296 \ttest: 0.88 17.097804423085556\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.021349698141443614 \ttest: 0.8666666666666667 17.16330292514009\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.012655538482637953 \ttest: 0.8666666666666667 17.242447170719608\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.008392259714488214 \ttest: 0.8666666666666667 17.322625150916632\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.005980029703963413 \ttest: 0.8533333333333334 17.398612194827166\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.004480631956754545 \ttest: 0.8533333333333334 17.46889310409974\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0034842148914977365 \ttest: 0.8533333333333334 17.533374085630427\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.002788024821006971 \ttest: 0.8533333333333334 17.592464566117076\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0022822404460871933 \ttest: 0.8533333333333334 17.646717370543072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.86486   0.84211   0.85333        38\n",
      "           1    0.84211   0.86486   0.85333        37\n",
      "\n",
      "    accuracy                        0.85333        75\n",
      "   macro avg    0.85349   0.85349   0.85333        75\n",
      "weighted avg    0.85364   0.85333   0.85333        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 162\n",
      "Average of depth: 2.3703703703703702\n",
      "Number of nodes: 1290\n",
      "12 56 1\n",
      "17 101 2\n",
      "20 128 3\n",
      "25 173 4\n",
      "25 169 5\n",
      "31 229 6\n",
      "40 320 7\n",
      "42 330 8\n",
      "49 381 9\n",
      "51 397 10\n",
      "55 421 11\n",
      "59 459 12\n",
      "62 480 13\n",
      "65 503 14\n",
      "73 581 15\n",
      "78 622 16\n",
      "83 675 17\n",
      "85 689 18\n",
      "91 755 19\n",
      "96 792 20\n",
      "97 799 21\n",
      "98 798 22\n",
      "99 813 23\n",
      "99 809 24\n",
      "100 816 25\n",
      "100 808 26\n",
      "101 809 27\n",
      "104 826 28\n",
      "106 830 29\n",
      "107 837 30\n",
      "108 848 31\n",
      "109 845 32\n",
      "111 867 33\n",
      "111 867 34\n",
      "111 867 35\n",
      "113 879 36\n",
      "113 863 37\n",
      "115 877 38\n",
      "115 867 39\n",
      "115 863 40\n",
      "115 863 41\n",
      "115 863 42\n",
      "116 872 43\n",
      "116 872 44\n",
      "116 872 45\n",
      "118 888 46\n",
      "119 903 47\n",
      "120 914 48\n",
      "121 919 49\n",
      "121 919 50\n",
      "122 926 51\n",
      "125 959 52\n",
      "125 959 53\n",
      "126 964 54\n",
      "126 960 55\n",
      "129 993 56\n",
      "129 993 57\n",
      "131 1015 58\n",
      "133 1045 59\n",
      "135 1065 60\n",
      "135 1065 61\n",
      "136 1074 62\n",
      "136 1064 63\n",
      "137 1069 64\n",
      "137 1069 65\n",
      "138 1082 66\n",
      "138 1082 67\n",
      "140 1096 68\n",
      "140 1096 69\n",
      "140 1094 70\n",
      "140 1094 71\n",
      "140 1090 72\n",
      "140 1090 73\n",
      "140 1088 74\n",
      "140 1088 75\n",
      "140 1088 76\n",
      "141 1103 77\n",
      "141 1095 78\n",
      "141 1095 79\n",
      "141 1095 80\n",
      "141 1095 81\n",
      "142 1106 82\n",
      "143 1115 83\n",
      "144 1128 84\n",
      "147 1153 85\n",
      "147 1153 86\n",
      "147 1153 87\n",
      "148 1164 88\n",
      "148 1164 89\n",
      "149 1167 90\n",
      "149 1165 91\n",
      "149 1159 92\n",
      "149 1155 93\n",
      "150 1166 94\n",
      "151 1177 95\n",
      "151 1177 96\n",
      "151 1177 97\n",
      "151 1177 98\n",
      "151 1177 99\n",
      "151 1177 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.5030260549648791 \ttest: 0.9066666666666666 11.004927032199912\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.2156687289536776 \ttest: 0.9066666666666666 10.850371339076524\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.12316764359589699 \ttest: 0.92 10.74106723447857\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.07987005593191271 \ttest: 0.92 10.666096097499425\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.05596678790789776 \ttest: 0.92 10.612825214934144\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.04137187504869093 \ttest: 0.92 10.574092007279006\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.03181285602079285 \ttest: 0.92 10.545553836053688\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.025215838305626338 \ttest: 0.92 10.524402158581289\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.020473810652640395 \ttest: 0.92 10.508738820492077\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.01695207160243862 \ttest: 0.92 10.497237800437983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96970   0.86486   0.91429        37\n",
      "           1    0.88095   0.97368   0.92500        38\n",
      "\n",
      "    accuracy                        0.92000        75\n",
      "   macro avg    0.92532   0.91927   0.91964        75\n",
      "weighted avg    0.92473   0.92000   0.91971        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 151\n",
      "Average of depth: 2.377483443708609\n",
      "Number of nodes: 1177\n",
      "20 124 1\n",
      "28 184 2\n",
      "33 223 3\n",
      "38 258 4\n",
      "40 272 5\n",
      "48 344 6\n",
      "56 408 7\n",
      "63 455 8\n",
      "63 453 9\n",
      "64 466 10\n",
      "65 471 11\n",
      "71 529 12\n",
      "71 529 13\n",
      "74 562 14\n",
      "79 609 15\n",
      "82 638 16\n",
      "86 682 17\n",
      "90 726 18\n",
      "92 728 19\n",
      "92 724 20\n",
      "94 736 21\n",
      "98 778 22\n",
      "101 811 23\n",
      "105 853 24\n",
      "105 849 25\n",
      "106 854 26\n",
      "108 866 27\n",
      "110 884 28\n",
      "110 874 29\n",
      "110 874 30\n",
      "110 874 31\n",
      "112 894 32\n",
      "113 905 33\n",
      "116 922 34\n",
      "118 926 35\n",
      "118 926 36\n",
      "120 938 37\n",
      "123 961 38\n",
      "125 977 39\n",
      "125 977 40\n",
      "126 992 41\n",
      "126 990 42\n",
      "128 1012 43\n",
      "130 1024 44\n",
      "132 1042 45\n",
      "135 1081 46\n",
      "135 1079 47\n",
      "135 1075 48\n",
      "135 1073 49\n",
      "135 1063 50\n",
      "135 1063 51\n",
      "137 1087 52\n",
      "137 1087 53\n",
      "140 1108 54\n",
      "140 1108 55\n",
      "140 1102 56\n",
      "141 1113 57\n",
      "143 1131 58\n",
      "143 1131 59\n",
      "144 1142 60\n",
      "144 1142 61\n",
      "144 1142 62\n",
      "145 1155 63\n",
      "145 1155 64\n",
      "146 1158 65\n",
      "147 1161 66\n",
      "147 1161 67\n",
      "148 1164 68\n",
      "149 1175 69\n",
      "149 1175 70\n",
      "149 1171 71\n",
      "149 1171 72\n",
      "149 1171 73\n",
      "149 1171 74\n",
      "151 1183 75\n",
      "151 1181 76\n",
      "151 1181 77\n",
      "152 1194 78\n",
      "155 1225 79\n",
      "156 1234 80\n",
      "156 1230 81\n",
      "156 1230 82\n",
      "156 1230 83\n",
      "157 1233 84\n",
      "157 1233 85\n",
      "156 1220 86\n",
      "156 1220 87\n",
      "160 1250 88\n",
      "162 1268 89\n",
      "162 1268 90\n",
      "162 1268 91\n",
      "162 1264 92\n",
      "162 1260 93\n",
      "162 1260 94\n",
      "162 1258 95\n",
      "163 1267 96\n",
      "163 1267 97\n",
      "163 1267 98\n",
      "163 1267 99\n",
      "163 1267 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.12979948473954497 \ttest: 0.8533333333333334 19.048546277988507\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.032603678611602314 \ttest: 0.8533333333333334 19.626730239099473\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.015298021789029791 \ttest: 0.8533333333333334 19.933585083064866\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.008977554101852808 \ttest: 0.8533333333333334 20.134706951942384\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.005934076118361284 \ttest: 0.8533333333333334 20.281152472291677\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.004226561850597882 \ttest: 0.8533333333333334 20.39466433585938\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.0031694615161770552 \ttest: 0.8533333333333334 20.486390370000144\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.0024681564988062346 \ttest: 0.8533333333333334 20.562747914720408\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.0019783442606362625 \ttest: 0.8533333333333334 20.627742562332514\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.0016223556449594598 \ttest: 0.8533333333333334 20.684029991491535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.79545   0.94595   0.86420        37\n",
      "           1    0.93548   0.76316   0.84058        38\n",
      "\n",
      "    accuracy                        0.85333        75\n",
      "   macro avg    0.86547   0.85455   0.85239        75\n",
      "weighted avg    0.86640   0.85333   0.85223        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 163\n",
      "Average of depth: 2.3067484662576687\n",
      "Number of nodes: 1267\n",
      "9 41 1\n",
      "18 100 2\n",
      "25 149 3\n",
      "31 195 4\n",
      "34 216 5\n",
      "42 286 6\n",
      "49 349 7\n",
      "56 406 8\n",
      "61 461 9\n",
      "62 468 10\n",
      "62 466 11\n",
      "63 473 12\n",
      "65 489 13\n",
      "65 489 14\n",
      "67 507 15\n",
      "69 521 16\n",
      "71 541 17\n",
      "73 553 18\n",
      "75 567 19\n",
      "76 576 20\n",
      "79 607 21\n",
      "82 640 22\n",
      "82 640 23\n",
      "84 652 24\n",
      "86 666 25\n",
      "87 673 26\n",
      "91 711 27\n",
      "94 738 28\n",
      "95 745 29\n",
      "97 761 30\n",
      "98 768 31\n",
      "98 766 32\n",
      "99 769 33\n",
      "98 754 34\n",
      "102 780 35\n",
      "103 791 36\n",
      "105 817 37\n",
      "106 826 38\n",
      "106 826 39\n",
      "109 863 40\n",
      "111 879 41\n",
      "114 918 42\n",
      "117 941 43\n",
      "117 941 44\n",
      "119 957 45\n",
      "120 966 46\n",
      "119 949 47\n",
      "119 949 48\n",
      "121 973 49\n",
      "121 971 50\n",
      "121 971 51\n",
      "122 976 52\n",
      "123 983 53\n",
      "124 990 54\n",
      "123 973 55\n",
      "123 973 56\n",
      "124 982 57\n",
      "126 1000 58\n",
      "126 1000 59\n",
      "127 1005 60\n",
      "128 1016 61\n",
      "129 1015 62\n",
      "130 1018 63\n",
      "130 1018 64\n",
      "131 1029 65\n",
      "133 1047 66\n",
      "133 1041 67\n",
      "133 1037 68\n",
      "133 1031 69\n",
      "133 1031 70\n",
      "133 1031 71\n",
      "133 1031 72\n",
      "133 1025 73\n",
      "134 1036 74\n",
      "134 1036 75\n",
      "136 1056 76\n",
      "137 1061 77\n",
      "139 1067 78\n",
      "139 1067 79\n",
      "140 1082 80\n",
      "141 1097 81\n",
      "142 1110 82\n",
      "142 1108 83\n",
      "142 1108 84\n",
      "142 1108 85\n",
      "142 1108 86\n",
      "142 1106 87\n",
      "143 1115 88\n",
      "143 1115 89\n",
      "143 1113 90\n",
      "144 1122 91\n",
      "144 1116 92\n",
      "144 1116 93\n",
      "147 1151 94\n",
      "147 1151 95\n",
      "148 1164 96\n",
      "151 1189 97\n",
      "152 1200 98\n",
      "154 1222 99\n",
      "154 1222 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.9693454714281875 \ttest: 0.8533333333333334 19.572633579922247\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.5746396272594982 \ttest: 0.8533333333333334 19.417095665547798\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.4168253419368257 \ttest: 0.8533333333333334 19.304872102637965\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.3221363448746074 \ttest: 0.8533333333333334 19.222869149718022\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.25748554567987847 \ttest: 0.8533333333333334 19.161935960062323\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.21059529491259926 \ttest: 0.8533333333333334 19.115496893872503\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.17531590388755758 \ttest: 0.8533333333333334 19.07924991564528\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.14806920206867502 \ttest: 0.8533333333333334 19.050366296049397\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.12659153161784528 \ttest: 0.8533333333333334 19.026937252705864\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.10937327141823998 \ttest: 0.8533333333333334 19.00763810654369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        87\n",
      "           1    1.00000   1.00000   1.00000        88\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.86486   0.84211   0.85333        38\n",
      "           1    0.84211   0.86486   0.85333        37\n",
      "\n",
      "    accuracy                        0.85333        75\n",
      "   macro avg    0.85349   0.85349   0.85333        75\n",
      "weighted avg    0.85364   0.85333   0.85333        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 154\n",
      "Average of depth: 2.4155844155844157\n",
      "Number of nodes: 1222\n",
      "13 73 1\n",
      "27 159 2\n",
      "32 192 3\n",
      "42 274 4\n",
      "44 288 5\n",
      "48 326 6\n",
      "48 324 7\n",
      "52 364 8\n",
      "54 378 9\n",
      "58 404 10\n",
      "62 440 11\n",
      "65 463 12\n",
      "68 488 13\n",
      "72 512 14\n",
      "75 525 15\n",
      "77 541 16\n",
      "78 552 17\n",
      "78 542 18\n",
      "80 556 19\n",
      "80 556 20\n",
      "82 582 21\n",
      "83 589 22\n",
      "84 600 23\n",
      "87 631 24\n",
      "89 647 25\n",
      "92 674 26\n",
      "94 688 27\n",
      "94 688 28\n",
      "94 686 29\n",
      "96 710 30\n",
      "98 728 31\n",
      "102 768 32\n",
      "103 777 33\n",
      "104 786 34\n",
      "104 786 35\n",
      "104 782 36\n",
      "105 791 37\n",
      "106 788 38\n",
      "106 788 39\n",
      "108 810 40\n",
      "108 810 41\n",
      "110 836 42\n",
      "111 841 43\n",
      "111 841 44\n",
      "113 859 45\n",
      "114 870 46\n",
      "115 877 47\n",
      "116 886 48\n",
      "116 882 49\n",
      "118 900 50\n",
      "119 909 51\n",
      "120 918 52\n",
      "120 918 53\n",
      "120 914 54\n",
      "120 912 55\n",
      "123 939 56\n",
      "125 957 57\n",
      "126 962 58\n",
      "126 962 59\n",
      "128 980 60\n",
      "128 980 61\n",
      "129 991 62\n",
      "130 994 63\n",
      "133 1029 64\n",
      "135 1055 65\n",
      "135 1055 66\n",
      "135 1051 67\n",
      "137 1073 68\n",
      "137 1073 69\n",
      "138 1080 70\n",
      "138 1080 71\n",
      "138 1080 72\n",
      "141 1113 73\n",
      "141 1111 74\n",
      "141 1103 75\n",
      "141 1103 76\n",
      "141 1103 77\n",
      "142 1114 78\n",
      "145 1145 79\n",
      "147 1169 80\n",
      "147 1169 81\n",
      "147 1169 82\n",
      "148 1180 83\n",
      "148 1176 84\n",
      "147 1157 85\n",
      "148 1168 86\n",
      "150 1194 87\n",
      "150 1194 88\n",
      "150 1194 89\n",
      "150 1194 90\n",
      "150 1194 91\n",
      "151 1205 92\n",
      "152 1218 93\n",
      "154 1228 94\n",
      "155 1239 95\n",
      "155 1239 96\n",
      "155 1235 97\n",
      "155 1235 98\n",
      "156 1248 99\n",
      "156 1242 100\n",
      "retrain  1 :\n",
      "\ttrain: 1.0 0.45520775807402925 \ttest: 0.88 16.05778196397625\n",
      "retrain  2 :\n",
      "\ttrain: 1.0 0.12421007808181739 \ttest: 0.88 16.08426822388706\n",
      "retrain  3 :\n",
      "\ttrain: 1.0 0.05688969531619645 \ttest: 0.88 16.12684910923501\n",
      "retrain  4 :\n",
      "\ttrain: 1.0 0.03247273415969075 \ttest: 0.88 16.1578585883209\n",
      "retrain  5 :\n",
      "\ttrain: 1.0 0.020973865747086463 \ttest: 0.88 16.1792798831872\n",
      "retrain  6 :\n",
      "\ttrain: 1.0 0.014663855821665213 \ttest: 0.88 16.193745505136885\n",
      "retrain  7 :\n",
      "\ttrain: 1.0 0.010832771539236492 \ttest: 0.88 16.203212091158065\n",
      "retrain  8 :\n",
      "\ttrain: 1.0 0.008333134928688962 \ttest: 0.88 16.20903981611815\n",
      "retrain  9 :\n",
      "\ttrain: 1.0 0.00661192531433664 \ttest: 0.88 16.212174737020234\n",
      "retrain  10 :\n",
      "\ttrain: 1.0 0.005376081764991759 \ttest: 0.88 16.213285979072406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        88\n",
      "           1    1.00000   1.00000   1.00000        87\n",
      "\n",
      "    accuracy                        1.00000       175\n",
      "   macro avg    1.00000   1.00000   1.00000       175\n",
      "weighted avg    1.00000   1.00000   1.00000       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91176   0.83784   0.87324        37\n",
      "           1    0.85366   0.92105   0.88608        38\n",
      "\n",
      "    accuracy                        0.88000        75\n",
      "   macro avg    0.88271   0.87945   0.87966        75\n",
      "weighted avg    0.88232   0.88000   0.87974        75\n",
      "\n",
      "----------------GTGP-------------\n",
      "Number of Trees: 156\n",
      "Average of depth: 2.3525641025641026\n",
      "Number of nodes: 1242\n"
     ]
    }
   ],
   "source": [
    "def fit_trees():\n",
    "    learning_rate=10\n",
    "    max_depth=3\n",
    "    bins=8\n",
    "    lam=10\n",
    "\n",
    "    gtgp = GTGP(learning_rate=learning_rate,max_depth=max_depth,bins=bins,lam=lam)\n",
    "\n",
    "    total_size=10\n",
    "    elite_size = 10\n",
    "    epoch= 100\n",
    "    gp_epoch= 3\n",
    "    verbose = 1\n",
    "    tolerance=0.01\n",
    "\n",
    "    gtgp.fit(X_train,y_train,total_size=total_size,elite_size = elite_size,epoch=epoch,gp_epoch=gp_epoch,tolerance=tolerance,verbose=verbose)\n",
    "\n",
    "    retrain_epoch= 10\n",
    "    alpha=0\n",
    "    beta=1\n",
    "    gammer=0\n",
    "\n",
    "    verbose=1\n",
    "    gtgp.lam = 10\n",
    "    gtgp.retrain_estimators(X_test,y_test,retrain_epoch=retrain_epoch,alpha=alpha,beta=beta,gammer=gammer,verbose=verbose)\n",
    "\n",
    "    return gtgp\n",
    "\n",
    "with open('./benchmark/'+dataset+'.csv','w') as f:\n",
    "        f.writelines(\"\")\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "    gtgp = fit_trees()\n",
    "\n",
    "    print(classification_report(y_train,np.argmax(gtgp.train_p,axis=1),digits=5))\n",
    "    print(classification_report(y_test,np.argmax(gtgp.test_p,axis=1),digits=5))\n",
    "\n",
    "    num_trees,depth,num_nodes = gtgp.print_model()\n",
    "\n",
    "    train_acc = accuracy_score(y_train,np.argmax(gtgp.train_p,axis=1))\n",
    "    test_acc = accuracy_score(y_test,np.argmax(gtgp.test_p,axis=1))\n",
    "\n",
    "    \n",
    "    # y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "    # train_f1 = mean_squared_error(y_train_one_hot.toarray(),gtgp.train_p)\n",
    "    # test_f1 = mean_squared_error(y_test_one_hot.toarray(),gtgp.test_p)\n",
    "\n",
    "\n",
    "    # train_f1 = roc_auc_score(gtgp.y_one_hot.toarray(),(gtgp.train_p.T/np.sum(gtgp.train_p,axis=1)).T)\n",
    "    # test_f1 = roc_auc_score(y_test_one_hot.toarray(),(gtgp.test_p.T/np.sum(gtgp.test_p,axis=1)).T)\n",
    "\n",
    "    y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "    train_f1 = roc_auc_score(gtgp.y_one_hot.toarray(),gtgp.train_p)\n",
    "    test_f1 = roc_auc_score(y_test_one_hot.toarray(),gtgp.test_p)\n",
    "\n",
    "\n",
    "    with open('./benchmark/'+dataset+'.csv','a') as f:\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e91a5",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ed15d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "661630ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "\n",
    "    num_trees = 1\n",
    "    depth = clf.tree_.max_depth\n",
    "    num_nodes = clf.tree_.node_count\n",
    "\n",
    "    train_acc = accuracy_score(y_train,clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test,clf.predict(X_test))\n",
    "    train_f1 = f1_score(y_train,clf.predict(X_train),average='macro')\n",
    "    test_f1 = f1_score(y_test,clf.predict(X_test),average='macro')\n",
    "\n",
    "    train_roc = roc_auc_score(y_train_one_hot.toarray(),clf.predict_proba(X_train))\n",
    "    test_roc = roc_auc_score(y_test_one_hot.toarray(),clf.predict_proba(X_test))\n",
    "\n",
    "    with open('./benchmark_DC/'+dataset+'.csv','a') as f:\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afbae1",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28fa8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acfdd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./benchmark_xgb/'+dataset+'.csv','w') as f:\n",
    "    for i in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "        y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        # xgb = xgboost.XGBClassifier(n_estimators=100,max_depth=6)\n",
    "        # xgb = xgboost.XGBClassifier(min_child_weight=0,max_depth=4)\n",
    "        xgb = xgboost.XGBClassifier(n_estimators=1000,max_depth=6)\n",
    "        xgb.fit(X_train,y_train)\n",
    "\n",
    "        import json\n",
    "\n",
    "        def item_generator(json_input, lookup_key):\n",
    "            if isinstance(json_input, dict):\n",
    "                for k, v in json_input.items():\n",
    "                    if k == lookup_key:\n",
    "                        yield v\n",
    "                    else:\n",
    "                        yield from item_generator(v, lookup_key)\n",
    "            elif isinstance(json_input, list):\n",
    "                for item in json_input:\n",
    "                    yield from item_generator(item, lookup_key)\n",
    "\n",
    "        def tree_depth(json_text):\n",
    "            json_input = json.loads(json_text)\n",
    "            depths = list(item_generator(json_input, 'depth'))\n",
    "            return max(depths) + 1 if len(depths) != 0 else 1\n",
    "\n",
    "        train_acc = accuracy_score(y_train,xgb.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test,xgb.predict(X_test))\n",
    "\n",
    "        # train_f1 = f1_score(y_train,xgb.predict(X_train),average='macro')\n",
    "        # test_f1 = f1_score(y_test,xgb.predict(X_test),average='macro')\n",
    "        train_f1 = roc_auc_score(y_train_one_hot.toarray(),xgb.predict_proba(X_train))\n",
    "        test_f1 = roc_auc_score(y_test_one_hot.toarray(),xgb.predict_proba(X_test))\n",
    "\n",
    "        booster = xgb.get_booster()\n",
    "\n",
    "        tree_df = booster.trees_to_dataframe()\n",
    "        depths = [tree_depth(x) for x in booster.get_dump(dump_format = \"json\")]\n",
    "        num_trees = len(depths)\n",
    "        depth = np.average(depths)\n",
    "        num_nodes = len(tree_df)\n",
    "\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc363f99",
   "metadata": {},
   "source": [
    "# GDBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e6d3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef69c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./benchmark_GBDT/'+dataset+'.csv','w') as f:\n",
    "    for i in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "        y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        clf = GradientBoostingClassifier(n_estimators=1000)\n",
    "        # clf = GradientBoostingClassifier()\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        train_acc = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test,clf.predict(X_test))\n",
    "        # train_f1 = f1_score(y_train,clf.predict(X_train),average='macro')\n",
    "        # test_f1 = f1_score(y_test,clf.predict(X_test),average='macro')\n",
    "        train_f1 = roc_auc_score(y_train_one_hot.toarray(),clf.predict_proba(X_train))\n",
    "        test_f1 = roc_auc_score(y_test_one_hot.toarray(),clf.predict_proba(X_test))\n",
    "\n",
    "\n",
    "        num_trees = len([ est for ests in clf.estimators_ for est in ests])\n",
    "        depth = np.average([ max(1,est.tree_.max_depth) for ests in clf.estimators_ for est in ests])\n",
    "        num_nodes = sum([ est.tree_.node_count for ests in clf.estimators_ for est in ests])\n",
    "\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ef54f",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8623bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3edfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./benchmark_RF/'+dataset+'.csv','w') as f:\n",
    "    for i in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size,stratify=y, random_state=seeds[i])\n",
    "        y_train,y_test,y_train_one_hot,y_test_one_hot = to_one_hot(y_train,y_test)\n",
    "        # rfc = RandomForestClassifier(n_estimators=100)\n",
    "        rfc = RandomForestClassifier(n_estimators=1000)\n",
    "        rfc.fit(X_train,y_train)\n",
    "\n",
    "        train_acc = accuracy_score(y_train,rfc.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test,rfc.predict(X_test))\n",
    "        # train_f1 = f1_score(y_train,rfc.predict(X_train),average='macro')\n",
    "        # test_f1 = f1_score(y_test,rfc.predict(X_test),average='macro')\n",
    "        \n",
    "        train_f1 = roc_auc_score(y_train_one_hot.toarray(),rfc.predict_proba(X_train))\n",
    "        test_f1 = roc_auc_score(y_test_one_hot.toarray(),rfc.predict_proba(X_test))\n",
    "\n",
    "        num_trees = len(rfc.estimators_)\n",
    "        depth = np.average([est.tree_.max_depth for est in rfc.estimators_])\n",
    "        num_nodes = sum([est.tree_.node_count for est in rfc.estimators_])\n",
    "\n",
    "        s = str(train_acc)+\",\"+str(test_acc)+\",\"+str(train_f1)+\",\"+str(test_f1)+\",\"+str(num_trees)+\",\"+str(depth)+\",\"+str(num_nodes)+\"\\n\"\n",
    "        f.writelines(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BStackGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "598cefc26d6e5a65b2978c65314d0610ea9dfe34c7d989c4b6d2528d500ccb7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
